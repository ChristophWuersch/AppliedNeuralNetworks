{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN10/10.1-Erweiterte_Nutzung_rekurrenter_neuronaler_Netze_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 10. Erweiterte Nutzung rekurrenter neuronaler Netze (RNNs)\n",
    "\n",
    "Dieses Notizbuch enthält die Codebeispiele aus Kapitel 6, Abschnitt 3 von [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). \n",
    "\n",
    "In diesem Abschnitt werden wir drei ausgeklügeltere Verfahren zur Verbesserung der Leistung und der Verallgemeinerungsfähigkeit von RNNs betrachten. \n",
    "- Nach der Lektüre dieses Abschnitts werden Sie das Wichtigste über die Verwendung von RNNs wissen. Wir werden die drei Konzepte anhand der Aufgabe erörtern, die Temperatur vorherzusagen. \n",
    "- Zu diesem Zweck stehen Zeitreihen mit Datenpunkten zur Verfügung, die von auf einem Gebäudedach installierten Sensoren erfasst wurden, wie z.B. Temperatur, Luftdruck und Luftfeuchtigkeit. Diese Daten verwenden wir, um die Temperatur 24 Stunden nach Erfassung des letzten Datenpunkts vorherzusagen. Das stellt sich als ziemlich anspruchsvolle Aufgabe heraus, die viele der typischen Schwierigkeiten veranschaulicht, die bei der Verwendung von Zeitreihen auftreten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir behandeln die folgenden Verfahren:\n",
    "- **_Rekurrentes Dropout-Verfahren_** – Hierbei handelt es sich um ein spezielles integriertes Verfahren, Dropout einzusetzen, um eine Überanpassung in rekurrenten Layern zu verhindern.\n",
    "- **_Hintereinanderschaltung rekurrenter Layer_** – Dieses Verfahren erhöht die Repräsentationsfähigkeit des NNs (auf Kosten höheren Rechenbedarfs).\n",
    "- **_Bidirektionale rekurrente Layer_** – Bei diesem Verfahren werden einem RNN dieselben Informationen auf unterschiedliche Weise bereitgestellt. Dadurch wird zum einen die Korrektklassifizierungsrate erhöht, zum anderen wiegen Probleme mit verloren gegangenen Informationen weniger schwer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temperaturvorhersage (Wetterdaten Jena)\n",
    "Bislang haben wir nur eine Art sequenzieller Daten betrachtet, nämlich Texte, wie\n",
    "die IMDb-Filmbewertungen oder die Reuters-Datensammlung. Sequenzielle Daten\n",
    "spielen jedoch keineswegs nur bei der Verarbeitung von Sprache eine Rolle. Die\n",
    "Beispiele in diesem Abschnitt verwenden eine Zeitreihe mit Wetterdaten, die von\n",
    "der Wetterstation am *Max-Planck-Institut für Biogeochemie in Jena* aufgezeichnet\n",
    "wurden. http://www.bgc-jena.mpg.de/wetter/.\n",
    "\n",
    "- Die Datenmenge enthält *14 verschiedene Messgrössen* (wie Lufttemperatur, Luftdruck, Luftfeuchtigkeit, Windrichtung usw.), die über mehrere Jahre hinweg im *Zehnminutentakt* aufgezeichnet wurden. \n",
    "- Die ursprünglichen Daten reichen zurückbis 2003, aber dieses Beispiel ist auf den Zeitraum von 2009 bis 2016 beschränkt.\n",
    "- Diese Datenmenge ist perfekt dafür geeignet, den Umgang mit numerischen Zeitreihen zu erlernen. \n",
    "- Wir werden ein Modell erstellen, das als Eingabe einige Daten aus der jüngsten Vergangenheit entgegennimmt (die Wetterdaten für ein paar Tage) und die Lufttemperatur für einen Zeitpunkt vorhersagt, der 24 Stunden in der Zukunft liegt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Um die Daten herunerladen zu können, müssen Sie auf einer Windows-Umgebung erst noch `wget` installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "url = \"https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "zip_path = \"data/jena_climate_2009_2016.csv.zip\"\n",
    "csv_path = \"data/jena_climate_2009_2016.csv\"\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    r = requests.get(url)\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Schauen wir uns die Daten an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"data/\"\n",
    "fname = os.path.join(data_dir, \"jena_climate_2009_2016.csv\")\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split(\"\\n\")\n",
    "header = lines[0].split(\",\")\n",
    "lines = lines[1:]\n",
    "print(len(lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die Ausgabe besagt, dass die Datei 420.551 Zeilen mit Daten enthält. Jede Zeile ist\n",
    "ein Zeitschritt: die Aufzeichnung von Uhrzeit und Datum sowie 14 wetterbezogenen\n",
    "Messwerten. Darüber hinaus enthält die Datei die folgenden Kopfzeilen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(header)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir müssen noch die \"Double Quotes\" entfernen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "new_header = []\n",
    "for item in header:\n",
    "    item = item.replace('\"', \"\")\n",
    "    print(item)\n",
    "    new_header.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "header = new_header\n",
    "new_header\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nun wandeln wir die 420'551 Zeilen in ein Numpy-Array um:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(\",\")[1:]]\n",
    "    float_data[i, :] = values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hier wird beispielhaft der zeitliche Verlauf der Temperatur im gesamten Erfassungszeitraum\n",
    "(in Grad Celsius) ausgegeben. In diesem Diagramm\n",
    "sind die jährlich periodisch schwankenden Temperaturen klar erkennbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualisierung der Temperaturdaten\n",
    "sns.set_theme(style=\"whitegrid\")  # Setze den Stil auf \"whitegrid\"\n",
    "temp = float_data[:, 1]  # temperature (in degrees Celsius)\n",
    "plt.figure(figsize=(12, 6))  # Größere Figur für bessere Lesbarkeit\n",
    "sns.lineplot(x=range(len(temp)), y=temp, color=\"blue\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.title(\"Temperature Over Time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Der folgende Plot zeigt den Temperaturverlauf eines kürzeren Zeitraums (der ersten zehn Tage). Die Daten wurden im Zehnminutentakt erfasst, also ergeben sich 144 Messwerte pro Tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")  # Setze den Stil auf \"whitegrid\"\n",
    "plt.figure(figsize=(12, 6))  # Größere Figur für bessere Lesbarkeit\n",
    "sns.lineplot(x=range(1440), y=temp[:1440], color=\"blue\")\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.title(\"Temperature Over First 10 Days\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In diesem Diagramm sind die täglich periodisch schwankenden Temperaturen gut erkennbar, insbesondere in den letzten vier Tagen. Beachten Sie auch, dass der Erfassungszeitraum in einem ziemlich kalten Wintermonat liegen muss.\n",
    "\n",
    "- Wenn Sie versuchen würden, die Durchschnittstemperatur des nächsten Monats anhand der Messwerte einiger vorangegangener Monate vorherzusagen, wäre die Aufgabe aufgrund der verlässlichen jährlichen Periodizität der Daten ziemlich einfach.\n",
    "- Betrachtet man jedoch die Daten nur einiger weniger Tage, sieht der Temperaturverlauf viel chaotischer aus. Lässt sich diese Zeitreihe anhand der Daten einiger weniger Tage vorhersagen? Dieser Frage gehen wir jetzt auf den Grund."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Daten vorbereiten\n",
    "\n",
    "Die genaue Aufgabenstellung lautet folgendermassen: \n",
    "\n",
    "- Wenn die Daten von `lookback` zurückliegenden Zeitschritten (ein Zeitschritt ist zehn Minuten lang) vorliegen und die Messwerte aller `steps` Zeitschritte verwendet werden, lässt sich dann die Temperatur in delay Zeitschritten vorhersagen? \n",
    "\n",
    "Wir verwenden die folgenden Werte für die Parameter:\n",
    "- `lookback = 720` – Wir nutzen die Beobachtungsdaten der letzten fünf Tage.\n",
    "-  `steps = 6` – Wir nutzen einen Messwert pro Stunde.\n",
    "-  `delay = 144` – Die Temperatur soll für einen Zeitpunkt vorhergesagt werden, der 24 Stunden in der Zukunft liegt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir müssen zunächst zwei Dinge erledigen:\n",
    "\n",
    "- Die Daten müssen in ein für ein NN geeignetes **numerisches Format** umgewandelt werden. Das ist nicht weiter schwer: Die Daten liegen bereits in *numerischer Form* vor, eine Vektorisierung ist also nicht erforderlich. \n",
    "- Die Werte der Zeitreihen der Datenmenge sind jedoch von *verschiedener Grössenordnung*. Die Temperatur liegt typischerweise zwischen –20 und +30 Grad, der Luftdruck hingegen besitzt (gemessen in Millibar) meist Werte, die bei ca. 1.000 liegen.  Die Zeitreihen müssen also **unabhängig voneinander normiert** werden, damit sie kleine Werte von vergleichbarer Grössenordnung enthalten.\n",
    "- Wir benötigen einen **Python-Generator**, der ein Array der aktuellen Fliesskommadaten entgegennimmt und einen Stapel (`batch`) mit Daten der jüngsten Vergangenheit sowie einen Zielwert für die zukünftige Temperatur zurückgibt. Da die Samples in der Datenmenge hochgradig redundant sind (Sample `N` und Sample `N + 1` werden grösstenteils denselben Wert besitzen), wäre es nicht gerade effektiv, für sämtliche Samples Speicherplatz zu reservieren. Stattdessen erzeugen wir die Samples anhand der ursprünglichen Daten in Echtzeit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die Daten werden folgendermaßen vorverarbeitet: Von den einzelnen Werten\n",
    "wird der Mittelwert der Zeitreihe subtrahiert. Anschließend wird das Ergebnis\n",
    "durch die Standardabweichung dividiert. Wir nutzen die ersten 200.000 Zeitschritte\n",
    "als Trainingsdaten, daher muss zur Berechnung des Mittelwerts und der\n",
    "Standardabweichung nur dieser Teil der Daten verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Das folgende Listing enthält den Code für den **Datengenerator**. Er liefert ein Tupel `(samples, targets)` zurück.\n",
    "\n",
    "`samples` ist ein Stapel von Eingabedaten, und `targets` ist das dazugehörige Array, das die Zielwerte für die Temperaturen enthält. Der\n",
    "Generator nimmt die folgenden Argumente entgegen:\n",
    "* `data` – Das ursprüngliche Array mit Fliesskommazahldaten, das bereits normiert wurde.\n",
    "* `lookback` – Die Anzahl der zurückliegenden Zeitschritte, die für die Eingabe berücksichtigt werden sollen.\n",
    "* `delay` – Die Anzahl der Zeitschritte bis zu dem Zeitpunkt, für den die Temperatur vorhergesagt werden soll.\n",
    "* `min_index` und `max_index` – Indizes des Datenarrays, die beschränken, welche Zeitschritte verwendet werden dürfen. Sie dienen dazu, Teile der Daten für die Validierung und das Testen zurückzuhalten.\n",
    "* `shuffle` – Gibt an, ob die Samples durchmischt oder in chronologischer Reihenfolge zurückgegeben werden sollen.\n",
    "* `batch_size` – Die Anzahl der Samples pro Stapel.\n",
    "* `step` – Der zeitliche Abstand (angegeben in Zeitschritten) der zu verwendenden Samples. Er wird auf 6 gesetzt, sodass der Datenmenge ein Datenpunkt pro Stunde entnommen wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(float_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def generator(\n",
    "    data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6\n",
    "):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nun verwenden wir die abstrakte generator-Funktion zur Instanziierung von drei Generatoren: einen für das Training, einen für die Validierung und einen für das Testen. \n",
    "- Diese Generatoren berücksichtigen jeweils unterschiedliche zeitliche Abschnitte der ursprünglichen Daten. \n",
    "- Der *Trainingsgenerator* verwendet die ersten 200.000 Zeitschritte, der *Validierungsgenerator* die nachfolgenden 100.000 und der *Testgenerator* die verbleibenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lookback = 1440  # 10 days\n",
    "step = 6  # 10 minutes\n",
    "delay = 144  # 24 hours\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(\n",
    "    float_data,\n",
    "    lookback=lookback,\n",
    "    delay=delay,\n",
    "    min_index=0,\n",
    "    max_index=200000,\n",
    "    shuffle=True,\n",
    "    step=step,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "val_gen = generator(\n",
    "    float_data,\n",
    "    lookback=lookback,\n",
    "    delay=delay,\n",
    "    min_index=200001,\n",
    "    max_index=300000,\n",
    "    step=step,\n",
    "    batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_gen = generator(\n",
    "    float_data,\n",
    "    lookback=lookback,\n",
    "    delay=delay,\n",
    "    min_index=300001,\n",
    "    max_index=None,\n",
    "    step=step,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# This is how many steps to draw from `val_gen`\n",
    "# in order to see the whole validation set:\n",
    "val_steps = (300000 - 200001 - lookback) // batch_size\n",
    "\n",
    "# This is how many steps to draw from `test_gen`\n",
    "# in order to see the whole test set:\n",
    "test_steps = (len(float_data) - 300001 - lookback) // batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eine vernünftige Abschätzung ohne Machine Learning\n",
    "\n",
    "Bevor wir zur Vorhersage der Temperaturen ein Deep-Learning-Modell einsetzen, das wie eine Blackbox arbeitet, sollten wir einen einfachen Ansatz ausprobieren, der auf dem gesunden Menschenverstand beruht. \n",
    "- Er dient der **Überprüfung, ob das Modell vernünftig arbeitet**, und liefert eine **Abschätzung der Leistung**, die es zu schlagen gilt, um den Nutzen komplexerer Machine-Learning-Modelle zu demonstrieren. \n",
    "- Solche auf dem *gesunden Menschenverstand* beruhende Abschätzungen erweisen sich als nützlich, wenn Sie eine neue Aufgabe in Angriff nehmen, für die es (noch) keine bekannte Lösung gibt. \n",
    "- Ein klassisches Beispiel hierfür sind *unausgewogene Klassifizierungsaufgaben* (inbalanced classification), bei denen bestimmte Klassen sehr viel häufiger sind als andere. Wenn eine Datenmenge zu 90% aus Instanzen der Klasse A und nur zu 10% aus Instanzen der Klasse B besteht, sagt einem der gesunde Menschenverstand, immer »A« zu wählen, wenn ein neues Sample klassifiziert wird. Ein solcher Klassifizierer erreicht insgesamt eine Korrektklassifizierungsrate von 90%. Dieses Ergebnis gilt es zu schlagen, um zu demonstrieren, dass auf Machine Learning beruhende Ansätze überhaupt von Nutzen sind. \n",
    "- In manchen Fällen erweist es sich als erstaunlich schwierig, so einfache  Abschätzungen zu übertreffen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Im vorliegenden Fall können wir davon ausgehen, dass die Temperaturzeitreihen stetige Werte enthalten (die morgige Temperatur wird ähnlich hoch wie die heutige sein) und dass diese Werte täglich periodisch schwanken. Ein auf dem gesunden Menschenverstand beruhender Ansatz würde also immer vorhersagen, dass die Temperatur in 24 Stunden der momentanen Temperatur entspricht.\n",
    "\n",
    "Wir bewerten diesen Ansatz jetzt anhand der `mae`-Metrik (**Mean Absolute Error, mittlerer absoluter Fehler**):\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "np.mean(np.abs(preds - targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Und hier ist eine Schleife zur Bewertung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_naive_method():\n",
    "    batch_maes = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(val_gen)\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_maes))\n",
    "\n",
    "\n",
    "evaluate_naive_method()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Der Code errechnet einen Wert von `0.29`. \n",
    "- Da die Temperaturdaten normiert sind (zentriert um 0 und Standardabweichung 1), ist dieser Wert nicht unmittelbar aussagekräftig.\n",
    "- Eine Umrechnung ergibt einen mittleren absoluten Fehler von 0.29 × `temperature_std`, also 2.57 Grad Celsius.\n",
    "- Dieser mittlere absolute Fehler ist realativ gross. Nun geht es darum, das Wissen über Deep Learning zu nutzen, um ein besseres Ergebnis zu erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ein elementarer Machine‑Learning‑Ansatz mit Lightning\n",
    "\n",
    "**Best Practice:** Wie beim gesundem Menschenverstand gilt auch hier: Erst einfache, nicht rechenaufwendige Modelle (z. B. kleine Fully‑connected NNs in PyTorch‑Lightning) ausprobieren, bevor man komplexe RNNs einsetzt – so kann man sicherstellen, dass zusätzliche Komplexität wirklich Vorteile bringt.\n",
    "\n",
    "- **TimeseriesDataset**:  \n",
    "  Ein Generator, der aus den Roh‑Zeitreihendaten für Training, Validierung und Test sequenzielle Paare `(Input, Ziel)` erstellt.  \n",
    "  - `lookback`, `step`, `delay` und `batch_size` legen dabei Fenstergröße, Schrittweite und Vorhersagehorizont fest.\n",
    "\n",
    "- **TimeseriesModel (LightningModule)**:  \n",
    "  - `Flatten()` → `Linear(input_shape, 32)` → `ReLU()` → `Linear(32, 1)`  \n",
    "  - Keine Aktivierung im letzten Dense‑Layer (typisch für Regression).  \n",
    "  - Verlustfunktion: mittlerer absoluter Fehler (`nn.L1Loss()`).  \n",
    "  - Optimierer: RMSprop.\n",
    "\n",
    "- **Training**:  \n",
    "  - Datensätze und DataLoader für Training/Validation/Test werden analog zum Originalansatz erzeugt.  \n",
    "  - Mit `trainer = L.Trainer(max_epochs=10, accelerator=\"auto\", devices=1)` und `trainer.fit(model, train_loader, val_loader)` trainieren wir 10 Epochen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ---- Generator als Dataset ----\n",
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        lookback,\n",
    "        delay,\n",
    "        min_index,\n",
    "        max_index,\n",
    "        step=6,\n",
    "        batch_size=128,\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.delay = delay\n",
    "        self.step = step\n",
    "        self.batch_size = batch_size\n",
    "        self.min_index = min_index + lookback\n",
    "        self.max_index = max_index if max_index is not None else len(data) - delay\n",
    "        self.indices = list(range(self.min_index, self.max_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_index - self.min_index\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        sample = self.data[i - self.lookback : i : self.step]\n",
    "        target = self.data[i + self.delay][1]  # Temperatur als Ziel (wie im Original)\n",
    "        return torch.tensor(sample, dtype=torch.float32), torch.tensor(\n",
    "            target, dtype=torch.float32\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Lightning-Modell ----\n",
    "class TimeseriesModel(L.LightningModule):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(input_shape, 64), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        # Listen zum Mitschreiben\n",
    "        self.train_loss_epoch = []\n",
    "        self.val_loss_epoch = []\n",
    "\n",
    "        self._train_losses = []\n",
    "        self._val_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self._train_losses.append(loss.detach())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._train_losses).mean()\n",
    "        self.train_loss_epoch.append(avg_loss.item())\n",
    "        self._train_losses.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self._val_losses.append(loss.detach())\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._val_losses).mean()\n",
    "        self.val_loss_epoch.append(avg_loss.item())\n",
    "        self._val_losses.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter wie im Original ----\n",
    "lookback = 1440 #  # 10 days\n",
    "step = 6 # 10 minutes\n",
    "delay = 144 # 24 hours\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Größe für Flatten Layer\n",
    "input_shape = (lookback // step) * float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(\n",
    "    float_data, lookback, delay, 0, 200000, step, batch_size\n",
    ")\n",
    "val_dataset = TimeseriesDataset(\n",
    "    float_data, lookback, delay, 200001, 300000, step, batch_size\n",
    ")\n",
    "test_dataset = TimeseriesDataset(\n",
    "    float_data, lookback, delay, 300001, None, step, batch_size\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first batch\n",
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training ----\n",
    "model = TimeseriesModel(input_shape=input_shape)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Schauen wir uns die Lernkurven für Test- und Validierungsdatenset an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotte die Trainings- und Validierungsverluste\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    np.arange(len(model.train_loss_epoch)),\n",
    "    model.train_loss_epoch, \"o-\",\n",
    "    label=\"Trainingsverlust\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(len(model.val_loss_epoch)),\n",
    "    model.val_loss_epoch,\"o-\",\n",
    "    label=\"Validierungsverlust\",\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Verlust\")\n",
    "plt.title(\"Trainings- und Validierungsverluste\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Abbildung: Verlauf von Trainings- und Validierungsverlust  \n",
    "- **Blaue Linie (Trainingsverlust):** Fällt von ~1.5 auf ~0.75, zeigt stetiges Lernen auf den Trainingsdaten.  \n",
    "- **Orange Linie (Validierungsverlust):** Starke Schwankungen zwischen 2.4 und 0.33 – nach anfänglichem Anstieg bis Epoche 1 fällt er bis Epoche 3, steigt dann wieder an und pendelt sich zuletzt um 0.4–0.7 ein.\n",
    "\n",
    "---\n",
    "\n",
    "## Verbesserte Interpretation\n",
    "\n",
    "1. **Teilerfolg gegenüber dem Menschenverstand-Ansatz**  \n",
    "   - In einigen Epochen (z. B. um Epoche 3 und am Ende) erreicht der Validierungsverlust Werte, die mit der einfachen, auf gesundem Menschenverstand beruhenden Schätzung vergleichbar sind.  \n",
    "   - Allerdings kommen diese Tiefpunkte unzuverlässig und nur kurzfristig zustande – insgesamt übertrifft das einfache Modell hier oft den NN-Ansatz.\n",
    "\n",
    "2. **Wert der Abschätzung**  \n",
    "   - Die stabile Performance des Baseline‑Ansatzes unterstreicht, wie viel implizites Wissen in heuristischen Methoden steckt.  \n",
    "   - Ohne diese Abschätzung hätte man leicht den Eindruck, das NN liefere “gute” Ergebnisse, obwohl es den einfachen Trick nicht konstant nutzt.\n",
    "\n",
    "3. **Hypothesenraum vs. einfache Lösung**  \n",
    "   - Der gewählte Hypothesenraum (zweischichtige Fully‑connected NNs) enthält zwar theoretisch auch die “Baseline‑Funktion” – praktisch findet der Optimierer diese jedoch nicht zuverlässig.  \n",
    "   - Komplexere Räume machen einfache Lösungen oft schwer zugänglich, wenn der Lernalgorithmus nicht speziell darauf ausgerichtet ist, sie zu erkunden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sie werden sich nun vielleicht fragen: \n",
    "- Wenn es offenbar ein einfaches und gut funktionierendes Modell gibt (der auf dem gesunden Menschenverstand beruhende\n",
    "Ansatz), warum findet das Modell ihn dann nicht und verbessert ihn? \n",
    "- Weil das Modell nicht dafür ausgelegt ist, beim Training nach dieser einfachen Lösung zu suchen. Der Raum der Modelle, in dem wir nach einer Lösung suchen, also der **Hypothesenraum**, enthält alle möglichen zweischichtigen NNs mit der definierten Konfiguration. \n",
    "- Diese NNs sind schon ziemlich kompliziert. Und wenn Sie in einem Raum komplizierter Modelle nach einer Lösung suchen, dann ist der einfache und gut funktionierende Ansatz womöglich gar nicht erlernbar, selbst wenn er rein technisch betrachtet Teil des Hypothesenraums ist. Hierbei handelt es sich um eine ziemlich bedeutende Einschränkung des Machine Learnings im Allgemeinen: \n",
    "\n",
    "Sofern in den Lernalgorithmus nicht fest einprogrammiert ist, nach einer bestimmten Art einfacher Modelle zu suchen, wird beim Erlernen der Parameter womöglich eine einfache Lösung für eine einfache Aufgabe nicht gefunden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ein erstes RNN\n",
    "\n",
    "Der erste vollständig verbundene Ansatz hat nicht besonders gut funktioniert, das soll aber nicht heissen, dass Machine Learning auf diese Aufgabe nicht anwendbar ist. Der letzte Ansatz hat anfangs die Dimensionalität der Zeitreihe verringert, und dadurch wurden die zeitlichen Informationen aus den Eingabedaten entfernt. \n",
    "\n",
    "Betrachten wir die Daten doch als das, was sie tatsächlich sind: eine Sequenz, in der *Kausalität und Reihenfolge von Bedeutung* sind.  \n",
    "- Wir werden nun ein Modell zur Verarbeitung rekurrenter Sequenzen ausprobieren, das für sequenzielle Daten dieser Art massgeschneidert sein sollte, eben weil es sich im Gegensatz zum ersten Ansatz die zeitliche Reihenfolge der Datenpunkte zunutze macht.\n",
    "- Anstelle des im letzten Abschnitt vorgestellten `LSTM`-Layers verwenden wir einen `GRU`-Layer (**Gated Recurrent Unit**, zu Deutsch etwa »geschlossene rekurrente Einheit «), die 2014 von **_Chung et al._** [1] entwickelt wurde.\n",
    "- `GRU`-Layer basieren auf den gleichen Prinzipien wie `LSTM`-Layer, sind jedoch etwas **besser optimiert und daher weniger rechenaufwendig**, wenngleich ihre Repräsentationsfähigkeit oft nicht an diejenige von `LSTM`-Layern heranreicht. \n",
    "- Einen solchen **Kompromiss** zwischen Rechenaufwand und Repräsentationsfähigkeit muss man beim Machine Learning ständig eingehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[1] [Junyoung Chung et al., Empirical Evaluation of Gated Recurrent Neural Networks on Sequence\n",
    "Modeling, Conference on Neural Information Processing Systems (2014)](https://arxiv.org/abs/1412.3555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ---- Dataset ----\n",
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback, delay, min_index, max_index, step=6):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.delay = delay\n",
    "        self.step = step\n",
    "        self.min_index = min_index + lookback\n",
    "        self.max_index = max_index if max_index is not None else len(data) - delay\n",
    "        self.indices = list(range(self.min_index, self.max_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        sample = self.data[i - self.lookback : i : self.step]\n",
    "        target = self.data[i + self.delay][1]  # Temperatur als Ziel\n",
    "        return torch.tensor(sample, dtype=torch.float32), torch.tensor(\n",
    "            target, dtype=torch.float32\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Lightning-Modell ----\n",
    "class GRUForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        # Batch-Verlustsammler pro Epoche\n",
    "        self.train_epoch_losses = []\n",
    "        self.val_epoch_losses = []\n",
    "\n",
    "        # Logging über Epochen hinweg\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        output = self.fc(output[:, -1, :])  # Letzter Zeitschritt\n",
    "        return output\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.train_epoch_losses.clear()\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.val_epoch_losses.clear()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self.train_epoch_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self.val_epoch_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_train_loss = sum(self.train_epoch_losses) / len(self.train_epoch_losses)\n",
    "        self.train_losses.append(avg_train_loss)\n",
    "        self.log(\"train_loss\", avg_train_loss, prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_val_loss = sum(self.val_epoch_losses) / len(self.val_epoch_losses)\n",
    "        self.val_losses.append(avg_val_loss)\n",
    "        self.log(\"val_loss\", avg_val_loss, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Größe für GRU\n",
    "input_size = float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Training ----\n",
    "model = GRUForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot der Verluste ----\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(model.train_losses,\"o-\", label=\"Train Loss\")\n",
    "plt.plot(model.val_losses,\"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 📊 Trainings- und Validierungsverlauf – Kurzfassung\n",
    "\n",
    "- 🔹 **Train Loss** sinkt zu Beginn, steigt aber ab Epoche 3–4 wieder an → **Overfitting** beginnt.\n",
    "- 🔸 **Validation Loss** bleibt relativ konstant oder steigt leicht → **keine echte Generalisierung**.\n",
    "\n",
    "### 📌 Fazit:\n",
    "- **Overfitting ab Epoche 3–4**\n",
    "- ✅ Einsatz von **Early Stopping** oder **Regularisierung** (z. B. Dropout, L2) empfohlen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rekurrentes Dropout-Verfahren zum Verhindern einer Überanpassung\n",
    "\n",
    "- Den Graphen für das Training und die Validierung ist zu entnehmen, dass es zu einer **Überanpassung (overfitting)** kommt: \n",
    "- Die Werte der Verlustfunktion für Trainings- und Validierungsdaten divergieren nach einigen wenigen Epochen beträchtlich.\n",
    "- Das **klassische Dropout-Verfahren** zum Verhindern einer Überanpassung ist Ihnen bereits vertraut: Zufällig ausgewählte Einheiten der Eingabe werden auf null gesetzt, um durch glückliche Umstände entstandene, aber irrelevante Korrelationen in den Trainingsdaten aufzulösen, die dem Layer übergeben werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die korrekte Anwendung des **Dropout-Verfahrens auf ein rekurrentes NN** ist allerdings *nicht trivial*. Die Anwendung des Dropout-Verfahrens vor einem rekurrenten Layer erschwert das Lernen, anstatt zur Regularisierung beizutragen. \n",
    "\n",
    "2015 hat *Yarin Gal* [2] in seiner Doktorarbeit über Bayes‘sches Deep Learning eine geeignete Methode für die Anwendung des Dropout-Verfahrens in RNNs beschrieben: \n",
    "- Bei allen Zeitschritten sollte die gleiche Dropout-Maske (das Muster der Einheiten, die auf null gesetzt werden) verwendet werden, anstatt die Dropout-Maske bei jedem Zeitschritt zufällig zu ändern. \n",
    "- Zur Regularisierung der Repräsentationen, die von rekurrenten `GRU`- oder `LSTM`-Layern gebildet werden, sollte zudem eine zeitlich konstante Dropout-Maske auf die inneren rekurrenten Aktivierungen der Layer angewendet werden (eine sogenannte rekurrente Dropout-Maske). \n",
    "- Die Anwendung der gleichen Dropout-Maske bei allen Zeitschritten ermöglicht es, dass die erlernten Fehler zeitlich im NN weitergegeben werden. Eine jeweils zufällige Dropout-Maske würde dieses Fehlersignal unterbrechen und damit dem Lernvorgang schaden.\n",
    "\n",
    "[2] [Yarin Gal, Uncertainty in Deep Learning (Doktorarbeit), 13. Oktober 2016](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡️ Verwendung von LSTM- und GRU-Schichten mit cuDNN in PyTorch Lightning\n",
    "\n",
    "- Wenn du in PyTorch oder PyTorch Lightning eine `nn.LSTM`- oder `nn.GRU`-Schicht auf der **GPU mit Standardparametern** verwendest, nutzt PyTorch automatisch die **cuDNN-beschleunigte Implementierung**.\n",
    "- Diese cuDNN-Kernel stammen von **NVIDIA** und bieten **sehr hohe Performance**, da sie speziell für die GPU optimiert wurden.\n",
    "- Allerdings sind sie **weniger flexibel**: Sobald du von den unterstützten Features abweichst, **fällt PyTorch automatisch auf eine langsamere (nicht-cuDNN) Implementierung zurück**.\n",
    "- 🔥 **Wichtiges Beispiel**:  \n",
    "  - **Recurrent Dropout** (d. h. Dropout zwischen Rechenschritten innerhalb der Sequenz) wird **nicht vom cuDNN-Kernel unterstützt**.\n",
    "  - Wenn du z. B. `dropout > 0` in einem LSTM oder GRU mit `num_layers=1` setzt, wird die **schnelle cuDNN-Implementierung deaktiviert**.\n",
    "  - Die Ausführung kann dann **2–5 mal langsamer** sein – besonders relevant bei größeren Sequenzen oder Batchgrößen.\n",
    "\n",
    "### ✅ Empfehlung:\n",
    "- Nutze `dropout` nur bei `num_layers > 1`, da PyTorch dann Dropout **zwischen den Schichten** einfügt, was von cuDNN unterstützt wird.\n",
    "- Vermeide rekurrentes Dropout **innerhalb** der LSTM-/GRU-Zellen, wenn dir Performance wichtig ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ---- Lightning-Modell ----\n",
    "class GRUForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=128,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "            num_layers=2,\n",
    "        )\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        self.train_losses = []  # Liste für Plot\n",
    "        self.val_losses = []\n",
    "\n",
    "        self._train_epoch_losses = []  # temporäre Sammler\n",
    "        self._val_epoch_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        return self.fc(output[:, -1, :])\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self._train_epoch_losses.clear()\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self._val_epoch_losses.clear()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._train_epoch_losses.append(loss.detach())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._val_epoch_losses.append(loss.detach())\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._train_epoch_losses).mean()\n",
    "        self.train_losses.append(avg_loss.item())\n",
    "        self.log(\"train_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._val_epoch_losses).mean()\n",
    "        self.val_losses.append(avg_loss.item())\n",
    "        self.log(\"val_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Größe für GRU\n",
    "input_size = float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Training ----\n",
    "model = GRUForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot der Verluste ----\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(model.train_losses, \"o-\", label=\"Training Loss\")\n",
    "plt.plot(model.val_losses, \"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hintereinanderschaltung rekurrenter Layer (stacking RNN layers)\n",
    "\n",
    "Es gibt zwar keine Überanpassung mehr, allerdings sind wir offenbar auf einen Leistungsengpass gestossen, daher sollten wir in Betracht ziehen, die **Kapazität des NNs zu erhöhen**. \n",
    "Rufen Sie sich die Beschreibung des allgemeinen Machine-Learning-Workflows ins Gedächtnis: \n",
    "- Üblicherweise ist es sinnvoll, die Kapazität des NNs zu erhöhen, bis eine Überanpassung einsetzt und zum grössten Hindernis wird. (Vorausgesetzt, Sie haben die elementaren Schritte zum Abschwächen der Überanpassung, wie z.B. den Einsatz des Dropout-Verfahrens, bereits unternommen.)\n",
    "- Solange es nicht zu einer allzu grossen Überanpassung kommt, können Sie die Kapazität wahrscheinlich noch erhöhen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Zwecks Erhöhung der Kapazität eines NNs erhöht man typischerweise die Anzahl\n",
    "der Einheiten in den Layern oder fügt zusätzliche Layer hinzu. \n",
    "- Die **Hintereinanderschaltung rekurrenter Layer ist ein klassisches Verfahren, leistungsfähigere\n",
    "rekurrente NNs zu erstellen**. \n",
    "- Googles Übersetzungsalgorithmus beispielsweise verwendet eine Hintereinanderschaltung von sieben LSTM-Layern – das ist schon\n",
    "enorm.\n",
    "- Bei der Hintereinanderschaltung rekurrenter Layer sollten alle zwischenliegenden Layer ihre vollständige Ausgabesequenz (einen 3-D-Tensor) zurückliefern, nicht nur die Ausgabe des letzten Zeitschritts. \n",
    "- Zu diesem Zweck verwendet man das Argument `return_sequence=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# ---- Lightning-Modell ----\n",
    "class GRUForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.gru1 = nn.GRU(\n",
    "            input_size=input_size, hidden_size=32, batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.gru2 = nn.GRU(input_size=32, hidden_size=64, batch_first=True, dropout=0.1)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        # Initialize attributes for tracking losses\n",
    "        self._train_epoch_losses = []\n",
    "        self._val_epoch_losses = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        output = self.fc(x[:, -1, :])  # Letzter Zeitschritt\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._train_epoch_losses.append(loss)  # Append loss to track it\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._val_epoch_losses.append(loss)  # Append loss to track it\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._train_epoch_losses).mean()\n",
    "        self.train_losses.append(avg_loss.item())\n",
    "        self.log(\"train_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._val_epoch_losses).mean()\n",
    "        self.val_losses.append(avg_loss.item())\n",
    "        self.log(\"val_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Größe für GRU\n",
    "input_size = float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Training ----\n",
    "model = GRUForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot der Verluste ----\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(model.train_losses, \"o-\", label=\"Training Loss\")\n",
    "plt.plot(model.val_losses, \"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ---- Lightning-Modell ----\n",
    "class LSTMForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=32, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=32, hidden_size=64, batch_first=True)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        # Listen zum Speichern der Verluste\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self._train_epoch_losses = []\n",
    "        self._val_epoch_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        output = self.fc(x[:, -1, :])  # Letzter Zeitschritt\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._train_epoch_losses.append(loss)  # Append batch loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._val_epoch_losses.append(loss)  # Append batch loss\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._train_epoch_losses).mean()\n",
    "        self.train_losses.append(avg_loss.item())\n",
    "        self.log(\"train_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._val_epoch_losses).mean()\n",
    "        self.val_losses.append(avg_loss.item())\n",
    "        self.log(\"val_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Größe für LSTM\n",
    "input_size = float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Training ----\n",
    "model = LSTMForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot der Verluste ----\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(model.train_losses,\"o-\", label=\"Train Loss\")\n",
    "plt.plot(model.val_losses,\"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bidirektionale RNNs\n",
    "\n",
    "Das letzte Verfahren, das in diesem Abschnitt vorgestellt wird, sind *bidirektionale RNNs*. Dabei handelt es sich um eine gebräuchliche Variante eines RNNs, mit der sich bei bestimmten Aufgaben eine bessere Leistung erzielen lässt als mit herkömmlichen RNNs. \n",
    "- Sie wird häufig zur Verarbeitung natürlicher Sprache eingesetzt (NLP) – man könnte sie als das »Schweizer Taschenmesser« des Deep Learnings für die Verarbeitung natürlicher Sprache bezeichnen.\n",
    "- RNNs sind *von der Reihenfolge der Eingabe abhängig*: Sie verarbeiten die Zeitschritte der Eingabesequenzen der Reihe nach, und eine Durchmischung oder Umkehr der Zeitschritte kann die Repräsentationen, die das RNN aus der Sequenz extrahiert, völlig verändern. Aus genau diesem Grund sind RNNs besonders gut für Aufgaben geeignet, bei denen die Reihenfolge von Bedeutung ist, wie etwa bei der Temperaturvorhersage. \n",
    "- Ein **bidirektionales RNN** macht sich die Tatsache zunutze, dass RNNs empfindlich auf die Reihenfolge reagieren:\n",
    "    * Es besteht aus *zwei herkömmlichen RNNs*, wie den bereits bekannten `GRU`- oder `LSTM`-Layern, die die Eingabesequenz in chronologischer bzw. umgekehrt chronologischer Reihenfolge verarbeiten und anschliessend ihre Repräsentationen verschmelzen.\n",
    "    * Dank der Verarbeitung der Sequenz in beide Richtungen kann ein bidirektionales RNN Muster erfassen, die einem unidirektionalen RNN womöglich entgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTMForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size, hidden_size=32, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=32 * 2, hidden_size=64, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 2, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        # Listen zum Speichern der Verluste\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self._train_epoch_losses = []\n",
    "        self._val_epoch_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        output = self.fc(x[:, -1, :])  # Letzter Zeitschritt\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._train_epoch_losses.append(loss)  # Append batch loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._val_epoch_losses.append(loss)  # Append batch loss\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._train_epoch_losses).mean()\n",
    "        self.train_losses.append(avg_loss.item())\n",
    "        self.log(\"train_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._val_epoch_losses).mean()\n",
    "        self.val_losses.append(avg_loss.item())\n",
    "        self.log(\"val_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Größe für LSTM\n",
    "input_size = float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Training ----\n",
    "model = BidirectionalLSTMForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot der Verluste ----\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(model.train_losses,\"o-\", label=\"Train Loss\")\n",
    "plt.plot(model.val_losses,\"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die Tatsache, dass die RNN-Layer in den letzten Abschnitten die Sequenzen in chronologischer Reihenfolge verarbeitet haben (die ältesten Zeitschritte zuerst), ist bemerkenswerterweise eine vollkommen willkürliche Entscheidung – oder zumindest\n",
    "eine Entscheidung, die wir bislang nicht infrage gestellt haben. \n",
    "- Wären die RNNs leistungsfähig genug gewesen, wenn sie die Eingabesequenzen beispielsweise **in umgekehrt chronologischer Reihenfolge (neuere Zeitschritte zuerst)** verarbeitet hätten? \n",
    "\n",
    "Das probieren wir nun aus und betrachten, was geschieht.  Wir benötigen lediglich eine Variante des Datengenerators, die die Eingabesequenzen entlang der Zeitdimension umkehrt. Ersetzen Sie die letzte Zeile einfach durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ---- Fully Torch-native Reversed Time Axis Dataset ----\n",
    "class ReversedTimeseriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback, delay, min_index, max_index, step=6):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.delay = delay\n",
    "        self.step = step\n",
    "        self.min_index = min_index + lookback\n",
    "        self.max_index = min(\n",
    "            max_index if max_index is not None else len(data), len(data) - delay\n",
    "        )\n",
    "        self.indices = list(range(self.min_index, self.max_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        sample_np = self.data[i - self.lookback : i : self.step]  # still NumPy here\n",
    "        sample = torch.from_numpy(sample_np).float()  # convert to torch tensor\n",
    "        sample = torch.flip(sample, dims=[0])  # flip along time axis (dim 0)\n",
    "\n",
    "        target = torch.tensor(self.data[i + self.delay][1], dtype=torch.float32)\n",
    "        return sample, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Lightning-Modell ----\n",
    "class GRUForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # loggt Hyperparameter automatisch\n",
    "        self.gru = nn.RNN(input_size=input_size, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        return self.fc(output[:, -1, :])  # Letzter Zeitschritt\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self.train_losses.append(loss.detach())\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        mean_loss = torch.stack(self.train_losses).mean()\n",
    "        self.log(\"train_loss\", mean_loss, prog_bar=True)\n",
    "        self.train_losses.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self.val_losses.append(loss.detach())\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        mean_loss = torch.stack(self.val_losses).mean()\n",
    "        self.log(\"val_loss\", mean_loss, prog_bar=True)\n",
    "        self.val_losses.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440  # 10 Tage bei Daten alle 10 Min\n",
    "step = 6  # jede Stunde ein Wert\n",
    "delay = 144  # Vorhersage 1 Tag in die Zukunft\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "# ---- Dataset / DataLoader ----\n",
    "input_size = float_data.shape[-1]\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Logger ----\n",
    "csv_logger = CSVLogger(\"logs\", name=\"gru_forecasting\")\n",
    "\n",
    "# ---- Training ----\n",
    "model = GRUForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    logger=csv_logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv logs/gru_forecasting/version_0/metrics.csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Pfad zur CSV-Datei\n",
    "csv_path = \"logs/gru_forecasting/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "# Filtere nur Zeilen mit train_loss bzw. val_loss\n",
    "train_df = df[[\"epoch\", \"train_loss\"]].dropna().drop_duplicates(subset=\"epoch\")\n",
    "val_df = df[[\"epoch\", \"val_loss\"]].dropna().drop_duplicates(subset=\"epoch\")\n",
    "\n",
    "# Füge sie auf Basis der Epoche zusammen\n",
    "merged_df = pd.merge(train_df, val_df, on=\"epoch\", how=\"outer\").sort_values(\"epoch\")\n",
    "\n",
    "# Optional: Index zurücksetzen\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(merged_df)\n",
    "\n",
    "# Plot der Verluste\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_df[\"epoch\"], merged_df[\"train_loss\"],\"o-\", label=\"Train Loss\")\n",
    "plt.plot(merged_df[\"epoch\"], merged_df[\"val_loss\"],\"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Schlussfolgerung**:\n",
    "\n",
    "- Das mit Sequenzen in umgekehrt chronologischer Reihenfolge trainierte GRUModell funktioniert sogar noch schlechter als der auf dem gesunden Menschenverstand beruhende Ansatz, was in diesem Fall darauf hinweist, dass die chronologische Verarbeitung für das Funktionieren des Modells wichtig ist.\n",
    "- Das ergibt auch durchaus Sinn: Der zugrunde liegende `GRU`-Layer wird sich typischerweise besser an die jüngere als an die weiter zurückliegende Vergangenheit »erinnern«, und dementsprechend besitzen jüngere Wetterdaten für die Aufgabe eine grössere Aussagekraft als ältere. Aus diesem Grund ist der auf dem gesunden Menschenverstand beruhende Ansatz ziemlich leistungsfähig.\n",
    "- Deshalb übertrifft die chronologische Version des Layers den mit Sequenzen in umgekehrt chronologischer Reihenfolge trainierten Layer bei Weitem. \n",
    "\n",
    "Beachten Sie jedoch, dass dies für viele andere Aufgaben, auch für die **Verarbeitung natürlicher Sprache, nicht zutrifft**:\n",
    "- Anschaulich gesagt, hängt die Bedeutung eines Worts für das Verständnis eines  Satzes für gewöhnlich nicht davon ab, an welcher Stelle im Satz sich das Wort befindet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wrapping up\n",
    "\n",
    "Nehmen Sie Folgendes aus diesem Abschnitt mit:\n",
    "1. Es ist sinnvoll, eine **auf dem gesunden Menschenverstand beruhende Abschätzung vorzunehmen**, mit der Sie Ihre Ergebnisse vergleichen können. Wenn Sie nicht wissen, welche Leistung es zu schlagen gilt, können Sie auch nicht feststellen, ob Sie Fortschritte machen.\n",
    "2. **Bottom-up**: Probieren Sie zunächst statt rechenaufwendiger Modelle einfache aus, um gegebenenfalls weiteren Aufwand zu rechtfertigen. Mitunter stellt sich heraus, dass ein einfaches Modell die beste Lösung ist.\n",
    "3. Wenn Sie Daten verwenden, bei denen die **zeitliche Reihenfolge von Bedeutung** ist, sind **RNNs** bestens geeignet und den Modellen deutlich überlegen, die zunächst die Dimensionalität der zeitlichen Daten verringern.\n",
    "4. Wenn Sie das **Dropout-Verfahren auf RNNs** anwenden, sollten Sie eine Dropout-Maske und eine rekurrente Dropout-Maske verwenden, die zeitlich konstant sind. Diese Masken sind in  rekurrenten Layern integriert, Sie brauchen also nur noch die Argumente `dropout` und `recurrent_dropout` anzugeben.\n",
    "5. **Hintereinandergeschaltete RNNs bieten eine höhere Repräsentationsfähigkeit** als einzelne RNN-Layer. Sie sind allerdings auch erheblich rechenaufwendiger, deshalb lohnt ihr Einsatz nicht immer. Sie bieten bei komplexen Aufgaben (z.B. der maschinellen Übersetzung von Fremdsprachen) zwar deutliche Vorteile, die bei kleineren, einfachen Aufgaben aber nicht ins Gewicht fallen.\n",
    "6. Bidirektionale RNNs, die Sequenzen in normaler und in umgekehrter Reihenfolge verarbeiten, sind gut für die Verarbeitung natürlicher Sprache geeignet, weisen jedoch bei sequenziellen Daten Schwächen auf, wenn die jüngste Vergangenheit erheblich informativer ist als der Anfang der Sequenz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weiterfürhende Konzepte: Attention und Sequence Masking\n",
    "\n",
    "Es gibt zwei bedeutende Konzepte, auf die wir an dieser Stelle nicht näher eingehen:\n",
    "- **Recurrent Attention: rekurrente Berücksichtigung** und die \n",
    "- **Sequence Masking: Maskierung von Sequenzen**. \n",
    "\n",
    "Die beiden Konzepte sind insbesondere für die Verarbeitung natürlicher Sprache von Bedeutung, für die Temperaturvorhersage\n",
    "jedoch kaum geeignet. Bei Interesse müssen Sie sich gegebenenfalls jenseits dieses Buchs über diese Konzepte informieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finanzmärkte und Machine Learning\n",
    "\n",
    "Manche Leser sind sehr darauf erpicht, die hier vorgestellten Verfahren auf die\n",
    "**Vorhersage von Wertpapierkursen (oder den Wechselkurs von Fremdwährungen usw.)** anzuwenden.\n",
    "\n",
    "- Finanzmärkte besitzen allerdings *völlig andere statistische Eigenschaften als natürliche Phänomene* wie z.B. der Verlauf des Wetters. \n",
    "- Zu versuchen, Finanzmärkte durch Machine Learning zu übertreffen, wenn nur öffentlich zugängliche Daten verfügbar sind, ist ein schwieriges Unterfangen. \n",
    "- Sie werden aller Wahrscheinlichkeit nach nur Zeit und Geld verschwenden und kein Ergebnis vorweisen können.\n",
    "\n",
    "Sie dürfen auch nicht vergessen, dass das vorangegangene Verhalten der Finanzmärkte kein guter Indikator für die zu erwartenden Renditen und Erträge ist – beim Autofahren ausschliesslich in den Rückspiegel zu sehen, ist ebenfalls keine\n",
    "vernünftige Fahrweise. Andererseits ist Machine Learning durchaus auf Datenmengen\n",
    "anwendbar, bei denen die Vergangenheit tatsächlich ein guter Indikator\n",
    "für die Zukunft ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
