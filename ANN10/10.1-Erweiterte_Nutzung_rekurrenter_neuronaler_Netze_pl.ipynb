{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> ¬© Christoph W√ºrsch, Fran√ßois Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN10/10.1-Erweiterte_Nutzung_rekurrenter_neuronaler_Netze_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f√ºr Ausf√ºhrung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 10. Erweiterte Nutzung rekurrenter neuronaler Netze (RNNs)\n",
    "\n",
    "Dieses Notizbuch enth√§lt die Codebeispiele aus Kapitel 6, Abschnitt 3 von [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). \n",
    "\n",
    "In diesem Abschnitt werden wir drei ausgekl√ºgeltere Verfahren zur Verbesserung der Leistung und der Verallgemeinerungsf√§higkeit von RNNs betrachten. \n",
    "- Nach der Lekt√ºre dieses Abschnitts werden Sie das Wichtigste √ºber die Verwendung von RNNs wissen. Wir werden die drei Konzepte anhand der Aufgabe er√∂rtern, die Temperatur vorherzusagen. \n",
    "- Zu diesem Zweck stehen Zeitreihen mit Datenpunkten zur Verf√ºgung, die von auf einem Geb√§udedach installierten Sensoren erfasst wurden, wie z.B. Temperatur, Luftdruck und Luftfeuchtigkeit. Diese Daten verwenden wir, um die Temperatur 24 Stunden nach Erfassung des letzten Datenpunkts vorherzusagen. Das stellt sich als ziemlich anspruchsvolle Aufgabe heraus, die viele der typischen Schwierigkeiten veranschaulicht, die bei der Verwendung von Zeitreihen auftreten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir behandeln die folgenden Verfahren:\n",
    "- **_Rekurrentes Dropout-Verfahren_** ‚Äì Hierbei handelt es sich um ein spezielles integriertes Verfahren, Dropout einzusetzen, um eine √úberanpassung in rekurrenten Layern zu verhindern.\n",
    "- **_Hintereinanderschaltung rekurrenter Layer_** ‚Äì Dieses Verfahren erh√∂ht die Repr√§sentationsf√§higkeit des NNs (auf Kosten h√∂heren Rechenbedarfs).\n",
    "- **_Bidirektionale rekurrente Layer_** ‚Äì Bei diesem Verfahren werden einem RNN dieselben Informationen auf unterschiedliche Weise bereitgestellt. Dadurch wird zum einen die Korrektklassifizierungsrate erh√∂ht, zum anderen wiegen Probleme mit verloren gegangenen Informationen weniger schwer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temperaturvorhersage (Wetterdaten Jena)\n",
    "Bislang haben wir nur eine Art sequenzieller Daten betrachtet, n√§mlich Texte, wie\n",
    "die IMDb-Filmbewertungen oder die Reuters-Datensammlung. Sequenzielle Daten\n",
    "spielen jedoch keineswegs nur bei der Verarbeitung von Sprache eine Rolle. Die\n",
    "Beispiele in diesem Abschnitt verwenden eine Zeitreihe mit Wetterdaten, die von\n",
    "der Wetterstation am *Max-Planck-Institut f√ºr Biogeochemie in Jena* aufgezeichnet\n",
    "wurden. http://www.bgc-jena.mpg.de/wetter/.\n",
    "\n",
    "- Die Datenmenge enth√§lt *14 verschiedene Messgr√∂ssen* (wie Lufttemperatur, Luftdruck, Luftfeuchtigkeit, Windrichtung usw.), die √ºber mehrere Jahre hinweg im *Zehnminutentakt* aufgezeichnet wurden. \n",
    "- Die urspr√ºnglichen Daten reichen zur√ºckbis 2003, aber dieses Beispiel ist auf den Zeitraum von 2009 bis 2016 beschr√§nkt.\n",
    "- Diese Datenmenge ist perfekt daf√ºr geeignet, den Umgang mit numerischen Zeitreihen zu erlernen. \n",
    "- Wir werden ein Modell erstellen, das als Eingabe einige Daten aus der j√ºngsten Vergangenheit entgegennimmt (die Wetterdaten f√ºr ein paar Tage) und die Lufttemperatur f√ºr einen Zeitpunkt vorhersagt, der 24 Stunden in der Zukunft liegt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Um die Daten herunerladen zu k√∂nnen, m√ºssen Sie auf einer Windows-Umgebung erst noch `wget` installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "url = \"https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "zip_path = \"data/jena_climate_2009_2016.csv.zip\"\n",
    "csv_path = \"data/jena_climate_2009_2016.csv\"\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    r = requests.get(url)\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Schauen wir uns die Daten an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"data/\"\n",
    "fname = os.path.join(data_dir, \"jena_climate_2009_2016.csv\")\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split(\"\\n\")\n",
    "header = lines[0].split(\",\")\n",
    "lines = lines[1:]\n",
    "print(len(lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die Ausgabe besagt, dass die Datei 420.551 Zeilen mit Daten enth√§lt. Jede Zeile ist\n",
    "ein Zeitschritt: die Aufzeichnung von Uhrzeit und Datum sowie 14 wetterbezogenen\n",
    "Messwerten. Dar√ºber hinaus enth√§lt die Datei die folgenden Kopfzeilen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(header)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir m√ºssen noch die \"Double Quotes\" entfernen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "new_header = []\n",
    "for item in header:\n",
    "    item = item.replace('\"', \"\")\n",
    "    print(item)\n",
    "    new_header.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "header = new_header\n",
    "new_header\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nun wandeln wir die 420'551 Zeilen in ein Numpy-Array um:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(\",\")[1:]]\n",
    "    float_data[i, :] = values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hier wird beispielhaft der zeitliche Verlauf der Temperatur im gesamten Erfassungszeitraum\n",
    "(in Grad Celsius) ausgegeben. In diesem Diagramm\n",
    "sind die j√§hrlich periodisch schwankenden Temperaturen klar erkennbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualisierung der Temperaturdaten\n",
    "sns.set_theme(style=\"whitegrid\")  # Setze den Stil auf \"whitegrid\"\n",
    "temp = float_data[:, 1]  # temperature (in degrees Celsius)\n",
    "plt.figure(figsize=(12, 6))  # Gr√∂√üere Figur f√ºr bessere Lesbarkeit\n",
    "sns.lineplot(x=range(len(temp)), y=temp, color=\"blue\")\n",
    "plt.ylabel(\"Temperature (¬∞C)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.title(\"Temperature Over Time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Der folgende Plot zeigt den Temperaturverlauf eines k√ºrzeren Zeitraums (der ersten zehn Tage). Die Daten wurden im Zehnminutentakt erfasst, also ergeben sich 144 Messwerte pro Tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")  # Setze den Stil auf \"whitegrid\"\n",
    "plt.figure(figsize=(12, 6))  # Gr√∂√üere Figur f√ºr bessere Lesbarkeit\n",
    "sns.lineplot(x=range(1440), y=temp[:1440], color=\"blue\")\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Temperature (¬∞C)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.title(\"Temperature Over First 10 Days\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In diesem Diagramm sind die t√§glich periodisch schwankenden Temperaturen gut erkennbar, insbesondere in den letzten vier Tagen. Beachten Sie auch, dass der Erfassungszeitraum in einem ziemlich kalten Wintermonat liegen muss.\n",
    "\n",
    "- Wenn Sie versuchen w√ºrden, die Durchschnittstemperatur des n√§chsten Monats anhand der Messwerte einiger vorangegangener Monate vorherzusagen, w√§re die Aufgabe aufgrund der verl√§sslichen j√§hrlichen Periodizit√§t der Daten ziemlich einfach.\n",
    "- Betrachtet man jedoch die Daten nur einiger weniger Tage, sieht der Temperaturverlauf viel chaotischer aus. L√§sst sich diese Zeitreihe anhand der Daten einiger weniger Tage vorhersagen? Dieser Frage gehen wir jetzt auf den Grund."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Daten vorbereiten\n",
    "\n",
    "Die genaue Aufgabenstellung lautet folgendermassen: \n",
    "\n",
    "- Wenn die Daten von `lookback` zur√ºckliegenden Zeitschritten (ein Zeitschritt ist zehn Minuten lang) vorliegen und die Messwerte aller `steps` Zeitschritte verwendet werden, l√§sst sich dann die Temperatur in delay Zeitschritten vorhersagen? \n",
    "\n",
    "Wir verwenden die folgenden Werte f√ºr die Parameter:\n",
    "- `lookback = 720` ‚Äì Wir nutzen die Beobachtungsdaten der letzten f√ºnf Tage.\n",
    "-  `steps = 6` ‚Äì Wir nutzen einen Messwert pro Stunde.\n",
    "-  `delay = 144` ‚Äì Die Temperatur soll f√ºr einen Zeitpunkt vorhergesagt werden, der 24 Stunden in der Zukunft liegt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir m√ºssen zun√§chst zwei Dinge erledigen:\n",
    "\n",
    "- Die Daten m√ºssen in ein f√ºr ein NN geeignetes **numerisches Format** umgewandelt werden. Das ist nicht weiter schwer: Die Daten liegen bereits in *numerischer Form* vor, eine Vektorisierung ist also nicht erforderlich. \n",
    "- Die Werte der Zeitreihen der Datenmenge sind jedoch von *verschiedener Gr√∂ssenordnung*. Die Temperatur liegt typischerweise zwischen ‚Äì20 und +30 Grad, der Luftdruck hingegen besitzt (gemessen in Millibar) meist Werte, die bei ca. 1.000 liegen.  Die Zeitreihen m√ºssen also **unabh√§ngig voneinander normiert** werden, damit sie kleine Werte von vergleichbarer Gr√∂ssenordnung enthalten.\n",
    "- Wir ben√∂tigen einen **Python-Generator**, der ein Array der aktuellen Fliesskommadaten entgegennimmt und einen Stapel (`batch`) mit Daten der j√ºngsten Vergangenheit sowie einen Zielwert f√ºr die zuk√ºnftige Temperatur zur√ºckgibt. Da die Samples in der Datenmenge hochgradig redundant sind (Sample `N` und Sample `N + 1` werden gr√∂sstenteils denselben Wert besitzen), w√§re es nicht gerade effektiv, f√ºr s√§mtliche Samples Speicherplatz zu reservieren. Stattdessen erzeugen wir die Samples anhand der urspr√ºnglichen Daten in Echtzeit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die Daten werden folgenderma√üen vorverarbeitet: Von den einzelnen Werten\n",
    "wird der Mittelwert der Zeitreihe subtrahiert. Anschlie√üend wird das Ergebnis\n",
    "durch die Standardabweichung dividiert. Wir nutzen die ersten 200.000 Zeitschritte\n",
    "als Trainingsdaten, daher muss zur Berechnung des Mittelwerts und der\n",
    "Standardabweichung nur dieser Teil der Daten verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Das folgende Listing enth√§lt den Code f√ºr den **Datengenerator**. Er liefert ein Tupel `(samples, targets)` zur√ºck.\n",
    "\n",
    "`samples` ist ein Stapel von Eingabedaten, und `targets` ist das dazugeh√∂rige Array, das die Zielwerte f√ºr die Temperaturen enth√§lt. Der\n",
    "Generator nimmt die folgenden Argumente entgegen:\n",
    "* `data` ‚Äì Das urspr√ºngliche Array mit Fliesskommazahldaten, das bereits normiert wurde.\n",
    "* `lookback` ‚Äì Die Anzahl der zur√ºckliegenden Zeitschritte, die f√ºr die Eingabe ber√ºcksichtigt werden sollen.\n",
    "* `delay` ‚Äì Die Anzahl der Zeitschritte bis zu dem Zeitpunkt, f√ºr den die Temperatur vorhergesagt werden soll.\n",
    "* `min_index` und `max_index` ‚Äì Indizes des Datenarrays, die beschr√§nken, welche Zeitschritte verwendet werden d√ºrfen. Sie dienen dazu, Teile der Daten f√ºr die Validierung und das Testen zur√ºckzuhalten.\n",
    "* `shuffle` ‚Äì Gibt an, ob die Samples durchmischt oder in chronologischer Reihenfolge zur√ºckgegeben werden sollen.\n",
    "* `batch_size` ‚Äì Die Anzahl der Samples pro Stapel.\n",
    "* `step` ‚Äì Der zeitliche Abstand (angegeben in Zeitschritten) der zu verwendenden Samples. Er wird auf 6 gesetzt, sodass der Datenmenge ein Datenpunkt pro Stunde entnommen wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(float_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def generator(\n",
    "    data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6\n",
    "):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nun verwenden wir die abstrakte generator-Funktion zur Instanziierung von drei Generatoren: einen f√ºr das Training, einen f√ºr die Validierung und einen f√ºr das Testen. \n",
    "- Diese Generatoren ber√ºcksichtigen jeweils unterschiedliche zeitliche Abschnitte der urspr√ºnglichen Daten. \n",
    "- Der *Trainingsgenerator* verwendet die ersten 200.000 Zeitschritte, der *Validierungsgenerator* die nachfolgenden 100.000 und der *Testgenerator* die verbleibenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lookback = 1440  # 10 days\n",
    "step = 6  # 10 minutes\n",
    "delay = 144  # 24 hours\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(\n",
    "    float_data,\n",
    "    lookback=lookback,\n",
    "    delay=delay,\n",
    "    min_index=0,\n",
    "    max_index=200000,\n",
    "    shuffle=True,\n",
    "    step=step,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "val_gen = generator(\n",
    "    float_data,\n",
    "    lookback=lookback,\n",
    "    delay=delay,\n",
    "    min_index=200001,\n",
    "    max_index=300000,\n",
    "    step=step,\n",
    "    batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_gen = generator(\n",
    "    float_data,\n",
    "    lookback=lookback,\n",
    "    delay=delay,\n",
    "    min_index=300001,\n",
    "    max_index=None,\n",
    "    step=step,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# This is how many steps to draw from `val_gen`\n",
    "# in order to see the whole validation set:\n",
    "val_steps = (300000 - 200001 - lookback) // batch_size\n",
    "\n",
    "# This is how many steps to draw from `test_gen`\n",
    "# in order to see the whole test set:\n",
    "test_steps = (len(float_data) - 300001 - lookback) // batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eine vern√ºnftige Absch√§tzung ohne Machine Learning\n",
    "\n",
    "Bevor wir zur Vorhersage der Temperaturen ein Deep-Learning-Modell einsetzen, das wie eine Blackbox arbeitet, sollten wir einen einfachen Ansatz ausprobieren, der auf dem gesunden Menschenverstand beruht. \n",
    "- Er dient der **√úberpr√ºfung, ob das Modell vern√ºnftig arbeitet**, und liefert eine **Absch√§tzung der Leistung**, die es zu schlagen gilt, um den Nutzen komplexerer Machine-Learning-Modelle zu demonstrieren. \n",
    "- Solche auf dem *gesunden Menschenverstand* beruhende Absch√§tzungen erweisen sich als n√ºtzlich, wenn Sie eine neue Aufgabe in Angriff nehmen, f√ºr die es (noch) keine bekannte L√∂sung gibt. \n",
    "- Ein klassisches Beispiel hierf√ºr sind *unausgewogene Klassifizierungsaufgaben* (inbalanced classification), bei denen bestimmte Klassen sehr viel h√§ufiger sind als andere. Wenn eine Datenmenge zu 90% aus Instanzen der Klasse A und nur zu 10% aus Instanzen der Klasse B besteht, sagt einem der gesunde Menschenverstand, immer ¬ªA¬´ zu w√§hlen, wenn ein neues Sample klassifiziert wird. Ein solcher Klassifizierer erreicht insgesamt eine Korrektklassifizierungsrate von 90%. Dieses Ergebnis gilt es zu schlagen, um zu demonstrieren, dass auf Machine Learning beruhende Ans√§tze √ºberhaupt von Nutzen sind. \n",
    "- In manchen F√§llen erweist es sich als erstaunlich schwierig, so einfache  Absch√§tzungen zu √ºbertreffen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Im vorliegenden Fall k√∂nnen wir davon ausgehen, dass die Temperaturzeitreihen stetige Werte enthalten (die morgige Temperatur wird √§hnlich hoch wie die heutige sein) und dass diese Werte t√§glich periodisch schwanken. Ein auf dem gesunden Menschenverstand beruhender Ansatz w√ºrde also immer vorhersagen, dass die Temperatur in 24 Stunden der momentanen Temperatur entspricht.\n",
    "\n",
    "Wir bewerten diesen Ansatz jetzt anhand der `mae`-Metrik (**Mean Absolute Error, mittlerer absoluter Fehler**):\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "np.mean(np.abs(preds - targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Und hier ist eine Schleife zur Bewertung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_naive_method():\n",
    "    batch_maes = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(val_gen)\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_maes))\n",
    "\n",
    "\n",
    "evaluate_naive_method()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Der Code errechnet einen Wert von `0.29`. \n",
    "- Da die Temperaturdaten normiert sind (zentriert um 0 und Standardabweichung 1), ist dieser Wert nicht unmittelbar aussagekr√§ftig.\n",
    "- Eine Umrechnung ergibt einen mittleren absoluten Fehler von 0.29 √ó `temperature_std`, also 2.57 Grad Celsius.\n",
    "- Dieser mittlere absolute Fehler ist realativ gross. Nun geht es darum, das Wissen √ºber Deep Learning zu nutzen, um ein besseres Ergebnis zu erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ein elementarer Machine‚ÄëLearning‚ÄëAnsatz mit Lightning\n",
    "\n",
    "**Best Practice:** Wie beim gesundem Menschenverstand gilt auch hier: Erst einfache, nicht rechenaufwendige Modelle (z.¬†B. kleine Fully‚Äëconnected NNs in PyTorch‚ÄëLightning) ausprobieren, bevor man komplexe RNNs einsetzt ‚Äì so kann man sicherstellen, dass zus√§tzliche Komplexit√§t wirklich Vorteile bringt.\n",
    "\n",
    "- **TimeseriesDataset**:  \n",
    "  Ein Generator, der aus den Roh‚ÄëZeitreihendaten f√ºr Training, Validierung und Test sequenzielle Paare `(Input, Ziel)` erstellt.  \n",
    "  - `lookback`, `step`, `delay` und `batch_size` legen dabei Fenstergr√∂√üe, Schrittweite und Vorhersagehorizont fest.\n",
    "\n",
    "- **TimeseriesModel (LightningModule)**:  \n",
    "  - `Flatten()` ‚Üí `Linear(input_shape, 32)` ‚Üí `ReLU()` ‚Üí `Linear(32, 1)`  \n",
    "  - Keine Aktivierung im letzten Dense‚ÄëLayer (typisch f√ºr Regression).  \n",
    "  - Verlustfunktion: mittlerer absoluter Fehler (`nn.L1Loss()`).  \n",
    "  - Optimierer: RMSprop.\n",
    "\n",
    "- **Training**:  \n",
    "  - Datens√§tze und DataLoader f√ºr Training/Validation/Test werden analog zum Originalansatz erzeugt.  \n",
    "  - Mit `trainer = L.Trainer(max_epochs=10, accelerator=\"auto\", devices=1)` und `trainer.fit(model, train_loader, val_loader)` trainieren wir 10 Epochen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ---- Generator als Dataset ----\n",
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        lookback,\n",
    "        delay,\n",
    "        min_index,\n",
    "        max_index,\n",
    "        step=6,\n",
    "        batch_size=128,\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.delay = delay\n",
    "        self.step = step\n",
    "        self.batch_size = batch_size\n",
    "        self.min_index = min_index + lookback\n",
    "        self.max_index = max_index if max_index is not None else len(data) - delay\n",
    "        self.indices = list(range(self.min_index, self.max_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_index - self.min_index\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        sample = self.data[i - self.lookback : i : self.step]\n",
    "        target = self.data[i + self.delay][1]  # Temperatur als Ziel (wie im Original)\n",
    "        return torch.tensor(sample, dtype=torch.float32), torch.tensor(\n",
    "            target, dtype=torch.float32\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Lightning-Modell ----\n",
    "class TimeseriesModel(L.LightningModule):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(input_shape, 64), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        # Listen zum Mitschreiben\n",
    "        self.train_loss_epoch = []\n",
    "        self.val_loss_epoch = []\n",
    "\n",
    "        self._train_losses = []\n",
    "        self._val_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self._train_losses.append(loss.detach())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._train_losses).mean()\n",
    "        self.train_loss_epoch.append(avg_loss.item())\n",
    "        self._train_losses.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self._val_losses.append(loss.detach())\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._val_losses).mean()\n",
    "        self.val_loss_epoch.append(avg_loss.item())\n",
    "        self._val_losses.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter wie im Original ----\n",
    "lookback = 1440 #  # 10 days\n",
    "step = 6 # 10 minutes\n",
    "delay = 144 # 24 hours\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Gr√∂√üe f√ºr Flatten Layer\n",
    "input_shape = (lookback // step) * float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(\n",
    "    float_data, lookback, delay, 0, 200000, step, batch_size\n",
    ")\n",
    "val_dataset = TimeseriesDataset(\n",
    "    float_data, lookback, delay, 200001, 300000, step, batch_size\n",
    ")\n",
    "test_dataset = TimeseriesDataset(\n",
    "    float_data, lookback, delay, 300001, None, step, batch_size\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first batch\n",
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training ----\n",
    "model = TimeseriesModel(input_shape=input_shape)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Schauen wir uns die Lernkurven f√ºr Test- und Validierungsdatenset an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotte die Trainings- und Validierungsverluste\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    np.arange(len(model.train_loss_epoch)),\n",
    "    model.train_loss_epoch, \"o-\",\n",
    "    label=\"Trainingsverlust\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(len(model.val_loss_epoch)),\n",
    "    model.val_loss_epoch,\"o-\",\n",
    "    label=\"Validierungsverlust\",\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Verlust\")\n",
    "plt.title(\"Trainings- und Validierungsverluste\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Abbildung: Verlauf von Trainings- und Validierungsverlust  \n",
    "- **Blaue Linie (Trainingsverlust):** F√§llt von ~1.5 auf ~0.75, zeigt stetiges Lernen auf den Trainingsdaten.  \n",
    "- **Orange Linie (Validierungsverlust):** Starke Schwankungen zwischen 2.4 und 0.33 ‚Äì nach anf√§nglichem Anstieg bis Epoche¬†1 f√§llt er bis Epoche¬†3, steigt dann wieder an und pendelt sich zuletzt um 0.4‚Äì0.7 ein.\n",
    "\n",
    "---\n",
    "\n",
    "## Verbesserte Interpretation\n",
    "\n",
    "1. **Teilerfolg gegen√ºber dem Menschenverstand-Ansatz**  \n",
    "   - In einigen Epochen (z.¬†B. um Epoche¬†3 und am Ende) erreicht der Validierungsverlust Werte, die mit der einfachen, auf gesundem Menschenverstand beruhenden Sch√§tzung vergleichbar sind.  \n",
    "   - Allerdings kommen diese Tiefpunkte unzuverl√§ssig und nur kurzfristig zustande ‚Äì insgesamt √ºbertrifft das einfache Modell hier oft den NN-Ansatz.\n",
    "\n",
    "2. **Wert der Absch√§tzung**  \n",
    "   - Die stabile Performance des Baseline‚ÄëAnsatzes unterstreicht, wie viel implizites Wissen in heuristischen Methoden steckt.  \n",
    "   - Ohne diese Absch√§tzung h√§tte man leicht den Eindruck, das NN liefere ‚Äúgute‚Äù Ergebnisse, obwohl es den einfachen Trick nicht konstant nutzt.\n",
    "\n",
    "3. **Hypothesenraum vs. einfache L√∂sung**  \n",
    "   - Der gew√§hlte Hypothesenraum (zweischichtige Fully‚Äëconnected NNs) enth√§lt zwar theoretisch auch die ‚ÄúBaseline‚ÄëFunktion‚Äù ‚Äì praktisch findet der Optimierer diese jedoch nicht zuverl√§ssig.  \n",
    "   - Komplexere R√§ume machen einfache L√∂sungen oft schwer zug√§nglich, wenn der Lernalgorithmus nicht speziell darauf ausgerichtet ist, sie zu erkunden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sie werden sich nun vielleicht fragen: \n",
    "- Wenn es offenbar ein einfaches und gut funktionierendes Modell gibt (der auf dem gesunden Menschenverstand beruhende\n",
    "Ansatz), warum findet das Modell ihn dann nicht und verbessert ihn? \n",
    "- Weil das Modell nicht daf√ºr ausgelegt ist, beim Training nach dieser einfachen L√∂sung zu suchen. Der Raum der Modelle, in dem wir nach einer L√∂sung suchen, also der **Hypothesenraum**, enth√§lt alle m√∂glichen zweischichtigen NNs mit der definierten Konfiguration. \n",
    "- Diese NNs sind schon ziemlich kompliziert. Und wenn Sie in einem Raum komplizierter Modelle nach einer L√∂sung suchen, dann ist der einfache und gut funktionierende Ansatz wom√∂glich gar nicht erlernbar, selbst wenn er rein technisch betrachtet Teil des Hypothesenraums ist. Hierbei handelt es sich um eine ziemlich bedeutende Einschr√§nkung des Machine Learnings im Allgemeinen: \n",
    "\n",
    "Sofern in den Lernalgorithmus nicht fest einprogrammiert ist, nach einer bestimmten Art einfacher Modelle zu suchen, wird beim Erlernen der Parameter wom√∂glich eine einfache L√∂sung f√ºr eine einfache Aufgabe nicht gefunden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ein erstes RNN\n",
    "\n",
    "Der erste vollst√§ndig verbundene Ansatz hat nicht besonders gut funktioniert, das soll aber nicht heissen, dass Machine Learning auf diese Aufgabe nicht anwendbar ist. Der letzte Ansatz hat anfangs die Dimensionalit√§t der Zeitreihe verringert, und dadurch wurden die zeitlichen Informationen aus den Eingabedaten entfernt. \n",
    "\n",
    "Betrachten wir die Daten doch als das, was sie tats√§chlich sind: eine Sequenz, in der *Kausalit√§t und Reihenfolge von Bedeutung* sind.  \n",
    "- Wir werden nun ein Modell zur Verarbeitung rekurrenter Sequenzen ausprobieren, das f√ºr sequenzielle Daten dieser Art massgeschneidert sein sollte, eben weil es sich im Gegensatz zum ersten Ansatz die zeitliche Reihenfolge der Datenpunkte zunutze macht.\n",
    "- Anstelle des im letzten Abschnitt vorgestellten `LSTM`-Layers verwenden wir einen `GRU`-Layer (**Gated Recurrent Unit**, zu Deutsch etwa ¬ªgeschlossene rekurrente Einheit ¬´), die 2014 von **_Chung et al._** [1] entwickelt wurde.\n",
    "- `GRU`-Layer basieren auf den gleichen Prinzipien wie `LSTM`-Layer, sind jedoch etwas **besser optimiert und daher weniger rechenaufwendig**, wenngleich ihre Repr√§sentationsf√§higkeit oft nicht an diejenige von `LSTM`-Layern heranreicht. \n",
    "- Einen solchen **Kompromiss** zwischen Rechenaufwand und Repr√§sentationsf√§higkeit muss man beim Machine Learning st√§ndig eingehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[1] [Junyoung Chung et al., Empirical Evaluation of Gated Recurrent Neural Networks on Sequence\n",
    "Modeling, Conference on Neural Information Processing Systems (2014)](https://arxiv.org/abs/1412.3555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ---- Dataset ----\n",
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback, delay, min_index, max_index, step=6):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.delay = delay\n",
    "        self.step = step\n",
    "        self.min_index = min_index + lookback\n",
    "        self.max_index = max_index if max_index is not None else len(data) - delay\n",
    "        self.indices = list(range(self.min_index, self.max_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        sample = self.data[i - self.lookback : i : self.step]\n",
    "        target = self.data[i + self.delay][1]  # Temperatur als Ziel\n",
    "        return torch.tensor(sample, dtype=torch.float32), torch.tensor(\n",
    "            target, dtype=torch.float32\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Lightning-Modell ----\n",
    "class GRUForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        # Batch-Verlustsammler pro Epoche\n",
    "        self.train_epoch_losses = []\n",
    "        self.val_epoch_losses = []\n",
    "\n",
    "        # Logging √ºber Epochen hinweg\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        output = self.fc(output[:, -1, :])  # Letzter Zeitschritt\n",
    "        return output\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.train_epoch_losses.clear()\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.val_epoch_losses.clear()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self.train_epoch_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self.val_epoch_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_train_loss = sum(self.train_epoch_losses) / len(self.train_epoch_losses)\n",
    "        self.train_losses.append(avg_train_loss)\n",
    "        self.log(\"train_loss\", avg_train_loss, prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_val_loss = sum(self.val_epoch_losses) / len(self.val_epoch_losses)\n",
    "        self.val_losses.append(avg_val_loss)\n",
    "        self.log(\"val_loss\", avg_val_loss, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Gr√∂√üe f√ºr GRU\n",
    "input_size = float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Training ----\n",
    "model = GRUForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot der Verluste ----\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(model.train_losses,\"o-\", label=\"Train Loss\")\n",
    "plt.plot(model.val_losses,\"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üìä Trainings- und Validierungsverlauf ‚Äì Kurzfassung\n",
    "\n",
    "- üîπ **Train Loss** sinkt zu Beginn, steigt aber ab Epoche 3‚Äì4 wieder an ‚Üí **Overfitting** beginnt.\n",
    "- üî∏ **Validation Loss** bleibt relativ konstant oder steigt leicht ‚Üí **keine echte Generalisierung**.\n",
    "\n",
    "### üìå Fazit:\n",
    "- **Overfitting ab Epoche 3‚Äì4**\n",
    "- ‚úÖ Einsatz von **Early Stopping** oder **Regularisierung** (z.‚ÄØB. Dropout, L2) empfohlen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rekurrentes Dropout-Verfahren zum Verhindern einer √úberanpassung\n",
    "\n",
    "- Den Graphen f√ºr das Training und die Validierung ist zu entnehmen, dass es zu einer **√úberanpassung (overfitting)** kommt: \n",
    "- Die Werte der Verlustfunktion f√ºr Trainings- und Validierungsdaten divergieren nach einigen wenigen Epochen betr√§chtlich.\n",
    "- Das **klassische Dropout-Verfahren** zum Verhindern einer √úberanpassung ist Ihnen bereits vertraut: Zuf√§llig ausgew√§hlte Einheiten der Eingabe werden auf null gesetzt, um durch gl√ºckliche Umst√§nde entstandene, aber irrelevante Korrelationen in den Trainingsdaten aufzul√∂sen, die dem Layer √ºbergeben werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die korrekte Anwendung des **Dropout-Verfahrens auf ein rekurrentes NN** ist allerdings *nicht trivial*. Die Anwendung des Dropout-Verfahrens vor einem rekurrenten Layer erschwert das Lernen, anstatt zur Regularisierung beizutragen. \n",
    "\n",
    "2015 hat *Yarin Gal* [2] in seiner Doktorarbeit √ºber Bayes‚Äòsches Deep Learning eine geeignete Methode f√ºr die Anwendung des Dropout-Verfahrens in RNNs beschrieben: \n",
    "- Bei allen Zeitschritten sollte die gleiche Dropout-Maske (das Muster der Einheiten, die auf null gesetzt werden) verwendet werden, anstatt die Dropout-Maske bei jedem Zeitschritt zuf√§llig zu √§ndern. \n",
    "- Zur Regularisierung der Repr√§sentationen, die von rekurrenten `GRU`- oder `LSTM`-Layern gebildet werden, sollte zudem eine zeitlich konstante Dropout-Maske auf die inneren rekurrenten Aktivierungen der Layer angewendet werden (eine sogenannte rekurrente Dropout-Maske). \n",
    "- Die Anwendung der gleichen Dropout-Maske bei allen Zeitschritten erm√∂glicht es, dass die erlernten Fehler zeitlich im NN weitergegeben werden. Eine jeweils zuf√§llige Dropout-Maske w√ºrde dieses Fehlersignal unterbrechen und damit dem Lernvorgang schaden.\n",
    "\n",
    "[2] [Yarin Gal, Uncertainty in Deep Learning (Doktorarbeit), 13. Oktober 2016](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö°Ô∏è Verwendung von LSTM- und GRU-Schichten mit cuDNN in PyTorch Lightning\n",
    "\n",
    "- Wenn du in PyTorch oder PyTorch Lightning eine `nn.LSTM`- oder `nn.GRU`-Schicht auf der **GPU mit Standardparametern** verwendest, nutzt PyTorch automatisch die **cuDNN-beschleunigte Implementierung**.\n",
    "- Diese cuDNN-Kernel stammen von **NVIDIA** und bieten **sehr hohe Performance**, da sie speziell f√ºr die GPU optimiert wurden.\n",
    "- Allerdings sind sie **weniger flexibel**: Sobald du von den unterst√ºtzten Features abweichst, **f√§llt PyTorch automatisch auf eine langsamere (nicht-cuDNN) Implementierung zur√ºck**.\n",
    "- üî• **Wichtiges Beispiel**:  \n",
    "  - **Recurrent Dropout** (d.‚ÄØh. Dropout zwischen Rechenschritten innerhalb der Sequenz) wird **nicht vom cuDNN-Kernel unterst√ºtzt**.\n",
    "  - Wenn du z.‚ÄØB. `dropout > 0` in einem LSTM oder GRU mit `num_layers=1` setzt, wird die **schnelle cuDNN-Implementierung deaktiviert**.\n",
    "  - Die Ausf√ºhrung kann dann **2‚Äì5 mal langsamer** sein ‚Äì besonders relevant bei gr√∂√üeren Sequenzen oder Batchgr√∂√üen.\n",
    "\n",
    "### ‚úÖ Empfehlung:\n",
    "- Nutze `dropout` nur bei `num_layers > 1`, da PyTorch dann Dropout **zwischen den Schichten** einf√ºgt, was von cuDNN unterst√ºtzt wird.\n",
    "- Vermeide rekurrentes Dropout **innerhalb** der LSTM-/GRU-Zellen, wenn dir Performance wichtig ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ---- Lightning-Modell ----\n",
    "class GRUForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=128,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "            num_layers=2,\n",
    "        )\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        self.train_losses = []  # Liste f√ºr Plot\n",
    "        self.val_losses = []\n",
    "\n",
    "        self._train_epoch_losses = []  # tempor√§re Sammler\n",
    "        self._val_epoch_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        return self.fc(output[:, -1, :])\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self._train_epoch_losses.clear()\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self._val_epoch_losses.clear()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._train_epoch_losses.append(loss.detach())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._val_epoch_losses.append(loss.detach())\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._train_epoch_losses).mean()\n",
    "        self.train_losses.append(avg_loss.item())\n",
    "        self.log(\"train_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._val_epoch_losses).mean()\n",
    "        self.val_losses.append(avg_loss.item())\n",
    "        self.log(\"val_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Gr√∂√üe f√ºr GRU\n",
    "input_size = float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Training ----\n",
    "model = GRUForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot der Verluste ----\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(model.train_losses, \"o-\", label=\"Training Loss\")\n",
    "plt.plot(model.val_losses, \"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hintereinanderschaltung rekurrenter Layer (stacking RNN layers)\n",
    "\n",
    "Es gibt zwar keine √úberanpassung mehr, allerdings sind wir offenbar auf einen Leistungsengpass gestossen, daher sollten wir in Betracht ziehen, die **Kapazit√§t des NNs zu erh√∂hen**. \n",
    "Rufen Sie sich die Beschreibung des allgemeinen Machine-Learning-Workflows ins Ged√§chtnis: \n",
    "- √úblicherweise ist es sinnvoll, die Kapazit√§t des NNs zu erh√∂hen, bis eine √úberanpassung einsetzt und zum gr√∂ssten Hindernis wird. (Vorausgesetzt, Sie haben die elementaren Schritte zum Abschw√§chen der √úberanpassung, wie z.B. den Einsatz des Dropout-Verfahrens, bereits unternommen.)\n",
    "- Solange es nicht zu einer allzu grossen √úberanpassung kommt, k√∂nnen Sie die Kapazit√§t wahrscheinlich noch erh√∂hen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Zwecks Erh√∂hung der Kapazit√§t eines NNs erh√∂ht man typischerweise die Anzahl\n",
    "der Einheiten in den Layern oder f√ºgt zus√§tzliche Layer hinzu. \n",
    "- Die **Hintereinanderschaltung rekurrenter Layer ist ein klassisches Verfahren, leistungsf√§higere\n",
    "rekurrente NNs zu erstellen**. \n",
    "- Googles √úbersetzungsalgorithmus beispielsweise verwendet eine Hintereinanderschaltung von sieben LSTM-Layern ‚Äì das ist schon\n",
    "enorm.\n",
    "- Bei der Hintereinanderschaltung rekurrenter Layer sollten alle zwischenliegenden Layer ihre vollst√§ndige Ausgabesequenz (einen 3-D-Tensor) zur√ºckliefern, nicht nur die Ausgabe des letzten Zeitschritts. \n",
    "- Zu diesem Zweck verwendet man das Argument `return_sequence=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# ---- Lightning-Modell ----\n",
    "class GRUForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.gru1 = nn.GRU(\n",
    "            input_size=input_size, hidden_size=32, batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.gru2 = nn.GRU(input_size=32, hidden_size=64, batch_first=True, dropout=0.1)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        # Initialize attributes for tracking losses\n",
    "        self._train_epoch_losses = []\n",
    "        self._val_epoch_losses = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        output = self.fc(x[:, -1, :])  # Letzter Zeitschritt\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._train_epoch_losses.append(loss)  # Append loss to track it\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._val_epoch_losses.append(loss)  # Append loss to track it\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._train_epoch_losses).mean()\n",
    "        self.train_losses.append(avg_loss.item())\n",
    "        self.log(\"train_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._val_epoch_losses).mean()\n",
    "        self.val_losses.append(avg_loss.item())\n",
    "        self.log(\"val_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Gr√∂√üe f√ºr GRU\n",
    "input_size = float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Training ----\n",
    "model = GRUForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot der Verluste ----\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(model.train_losses, \"o-\", label=\"Training Loss\")\n",
    "plt.plot(model.val_losses, \"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ---- Lightning-Modell ----\n",
    "class LSTMForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=32, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=32, hidden_size=64, batch_first=True)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        # Listen zum Speichern der Verluste\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self._train_epoch_losses = []\n",
    "        self._val_epoch_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        output = self.fc(x[:, -1, :])  # Letzter Zeitschritt\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._train_epoch_losses.append(loss)  # Append batch loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._val_epoch_losses.append(loss)  # Append batch loss\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._train_epoch_losses).mean()\n",
    "        self.train_losses.append(avg_loss.item())\n",
    "        self.log(\"train_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._val_epoch_losses).mean()\n",
    "        self.val_losses.append(avg_loss.item())\n",
    "        self.log(\"val_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Gr√∂√üe f√ºr LSTM\n",
    "input_size = float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Training ----\n",
    "model = LSTMForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot der Verluste ----\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(model.train_losses,\"o-\", label=\"Train Loss\")\n",
    "plt.plot(model.val_losses,\"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bidirektionale RNNs\n",
    "\n",
    "Das letzte Verfahren, das in diesem Abschnitt vorgestellt wird, sind *bidirektionale RNNs*. Dabei handelt es sich um eine gebr√§uchliche Variante eines RNNs, mit der sich bei bestimmten Aufgaben eine bessere Leistung erzielen l√§sst als mit herk√∂mmlichen RNNs. \n",
    "- Sie wird h√§ufig zur Verarbeitung nat√ºrlicher Sprache eingesetzt (NLP) ‚Äì man k√∂nnte sie als das ¬ªSchweizer Taschenmesser¬´ des Deep Learnings f√ºr die Verarbeitung nat√ºrlicher Sprache bezeichnen.\n",
    "- RNNs sind *von der Reihenfolge der Eingabe abh√§ngig*: Sie verarbeiten die Zeitschritte der Eingabesequenzen der Reihe nach, und eine Durchmischung oder Umkehr der Zeitschritte kann die Repr√§sentationen, die das RNN aus der Sequenz extrahiert, v√∂llig ver√§ndern. Aus genau diesem Grund sind RNNs besonders gut f√ºr Aufgaben geeignet, bei denen die Reihenfolge von Bedeutung ist, wie etwa bei der Temperaturvorhersage. \n",
    "- Ein **bidirektionales RNN** macht sich die Tatsache zunutze, dass RNNs empfindlich auf die Reihenfolge reagieren:\n",
    "    * Es besteht aus *zwei herk√∂mmlichen RNNs*, wie den bereits bekannten `GRU`- oder `LSTM`-Layern, die die Eingabesequenz in chronologischer bzw. umgekehrt chronologischer Reihenfolge verarbeiten und anschliessend ihre Repr√§sentationen verschmelzen.\n",
    "    * Dank der Verarbeitung der Sequenz in beide Richtungen kann ein bidirektionales RNN Muster erfassen, die einem unidirektionalen RNN wom√∂glich entgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTMForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size, hidden_size=32, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=32 * 2, hidden_size=64, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 2, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        # Listen zum Speichern der Verluste\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self._train_epoch_losses = []\n",
    "        self._val_epoch_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        output = self.fc(x[:, -1, :])  # Letzter Zeitschritt\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._train_epoch_losses.append(loss)  # Append batch loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self._val_epoch_losses.append(loss)  # Append batch loss\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._train_epoch_losses).mean()\n",
    "        self.train_losses.append(avg_loss.item())\n",
    "        self.log(\"train_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self._val_epoch_losses).mean()\n",
    "        self.val_losses.append(avg_loss.item())\n",
    "        self.log(\"val_loss_epoch\", avg_loss, prog_bar=True)  # Logging Epochenmittel\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "# Input-Gr√∂√üe f√ºr LSTM\n",
    "input_size = float_data.shape[-1]\n",
    "\n",
    "# ---- Datasets und Dataloaders ----\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Training ----\n",
    "model = BidirectionalLSTMForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot der Verluste ----\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(model.train_losses,\"o-\", label=\"Train Loss\")\n",
    "plt.plot(model.val_losses,\"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die Tatsache, dass die RNN-Layer in den letzten Abschnitten die Sequenzen in chronologischer Reihenfolge verarbeitet haben (die √§ltesten Zeitschritte zuerst), ist bemerkenswerterweise eine vollkommen willk√ºrliche Entscheidung ‚Äì oder zumindest\n",
    "eine Entscheidung, die wir bislang nicht infrage gestellt haben. \n",
    "- W√§ren die RNNs leistungsf√§hig genug gewesen, wenn sie die Eingabesequenzen beispielsweise **in umgekehrt chronologischer Reihenfolge (neuere Zeitschritte zuerst)** verarbeitet h√§tten? \n",
    "\n",
    "Das probieren wir nun aus und betrachten, was geschieht.  Wir ben√∂tigen lediglich eine Variante des Datengenerators, die die Eingabesequenzen entlang der Zeitdimension umkehrt. Ersetzen Sie die letzte Zeile einfach durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ---- Fully Torch-native Reversed Time Axis Dataset ----\n",
    "class ReversedTimeseriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback, delay, min_index, max_index, step=6):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.delay = delay\n",
    "        self.step = step\n",
    "        self.min_index = min_index + lookback\n",
    "        self.max_index = min(\n",
    "            max_index if max_index is not None else len(data), len(data) - delay\n",
    "        )\n",
    "        self.indices = list(range(self.min_index, self.max_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        sample_np = self.data[i - self.lookback : i : self.step]  # still NumPy here\n",
    "        sample = torch.from_numpy(sample_np).float()  # convert to torch tensor\n",
    "        sample = torch.flip(sample, dims=[0])  # flip along time axis (dim 0)\n",
    "\n",
    "        target = torch.tensor(self.data[i + self.delay][1], dtype=torch.float32)\n",
    "        return sample, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Lightning-Modell ----\n",
    "class GRUForecastingModel(L.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # loggt Hyperparameter automatisch\n",
    "        self.gru = nn.RNN(input_size=input_size, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        return self.fc(output[:, -1, :])  # Letzter Zeitschritt\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self.train_losses.append(loss.detach())\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        mean_loss = torch.stack(self.train_losses).mean()\n",
    "        self.log(\"train_loss\", mean_loss, prog_bar=True)\n",
    "        self.train_losses.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y.unsqueeze(1))\n",
    "        self.val_losses.append(loss.detach())\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        mean_loss = torch.stack(self.val_losses).mean()\n",
    "        self.log(\"val_loss\", mean_loss, prog_bar=True)\n",
    "        self.val_losses.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter ----\n",
    "lookback = 1440  # 10 Tage bei Daten alle 10 Min\n",
    "step = 6  # jede Stunde ein Wert\n",
    "delay = 144  # Vorhersage 1 Tag in die Zukunft\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "# ---- Dataset / DataLoader ----\n",
    "input_size = float_data.shape[-1]\n",
    "train_dataset = TimeseriesDataset(float_data, lookback, delay, 0, 200000, step)\n",
    "val_dataset = TimeseriesDataset(float_data, lookback, delay, 200001, 300000, step)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ---- Logger ----\n",
    "csv_logger = CSVLogger(\"logs\", name=\"gru_forecasting\")\n",
    "\n",
    "# ---- Training ----\n",
    "model = GRUForecastingModel(input_size=input_size)\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    logger=csv_logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv logs/gru_forecasting/version_0/metrics.csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Pfad zur CSV-Datei\n",
    "csv_path = \"logs/gru_forecasting/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "# Filtere nur Zeilen mit train_loss bzw. val_loss\n",
    "train_df = df[[\"epoch\", \"train_loss\"]].dropna().drop_duplicates(subset=\"epoch\")\n",
    "val_df = df[[\"epoch\", \"val_loss\"]].dropna().drop_duplicates(subset=\"epoch\")\n",
    "\n",
    "# F√ºge sie auf Basis der Epoche zusammen\n",
    "merged_df = pd.merge(train_df, val_df, on=\"epoch\", how=\"outer\").sort_values(\"epoch\")\n",
    "\n",
    "# Optional: Index zur√ºcksetzen\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(merged_df)\n",
    "\n",
    "# Plot der Verluste\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_df[\"epoch\"], merged_df[\"train_loss\"],\"o-\", label=\"Train Loss\")\n",
    "plt.plot(merged_df[\"epoch\"], merged_df[\"val_loss\"],\"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Schlussfolgerung**:\n",
    "\n",
    "- Das mit Sequenzen in umgekehrt chronologischer Reihenfolge trainierte GRUModell funktioniert sogar noch schlechter als der auf dem gesunden Menschenverstand beruhende Ansatz, was in diesem Fall darauf hinweist, dass die chronologische Verarbeitung f√ºr das Funktionieren des Modells wichtig ist.\n",
    "- Das ergibt auch durchaus Sinn: Der zugrunde liegende `GRU`-Layer wird sich typischerweise besser an die j√ºngere als an die weiter zur√ºckliegende Vergangenheit ¬ªerinnern¬´, und dementsprechend besitzen j√ºngere Wetterdaten f√ºr die Aufgabe eine gr√∂ssere Aussagekraft als √§ltere. Aus diesem Grund ist der auf dem gesunden Menschenverstand beruhende Ansatz ziemlich leistungsf√§hig.\n",
    "- Deshalb √ºbertrifft die chronologische Version des Layers den mit Sequenzen in umgekehrt chronologischer Reihenfolge trainierten Layer bei Weitem. \n",
    "\n",
    "Beachten Sie jedoch, dass dies f√ºr viele andere Aufgaben, auch f√ºr die **Verarbeitung nat√ºrlicher Sprache, nicht zutrifft**:\n",
    "- Anschaulich gesagt, h√§ngt die Bedeutung eines Worts f√ºr das Verst√§ndnis eines  Satzes f√ºr gew√∂hnlich nicht davon ab, an welcher Stelle im Satz sich das Wort befindet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wrapping up\n",
    "\n",
    "Nehmen Sie Folgendes aus diesem Abschnitt mit:\n",
    "1. Es ist sinnvoll, eine **auf dem gesunden Menschenverstand beruhende Absch√§tzung vorzunehmen**, mit der Sie Ihre Ergebnisse vergleichen k√∂nnen. Wenn Sie nicht wissen, welche Leistung es zu schlagen gilt, k√∂nnen Sie auch nicht feststellen, ob Sie Fortschritte machen.\n",
    "2. **Bottom-up**: Probieren Sie zun√§chst statt rechenaufwendiger Modelle einfache aus, um gegebenenfalls weiteren Aufwand zu rechtfertigen. Mitunter stellt sich heraus, dass ein einfaches Modell die beste L√∂sung ist.\n",
    "3. Wenn Sie Daten verwenden, bei denen die **zeitliche Reihenfolge von Bedeutung** ist, sind **RNNs** bestens geeignet und den Modellen deutlich √ºberlegen, die zun√§chst die Dimensionalit√§t der zeitlichen Daten verringern.\n",
    "4. Wenn Sie das **Dropout-Verfahren auf RNNs** anwenden, sollten Sie eine Dropout-Maske und eine rekurrente Dropout-Maske verwenden, die zeitlich konstant sind. Diese Masken sind in  rekurrenten Layern integriert, Sie brauchen also nur noch die Argumente `dropout` und `recurrent_dropout` anzugeben.\n",
    "5. **Hintereinandergeschaltete RNNs bieten eine h√∂here Repr√§sentationsf√§higkeit** als einzelne RNN-Layer. Sie sind allerdings auch erheblich rechenaufwendiger, deshalb lohnt ihr Einsatz nicht immer. Sie bieten bei komplexen Aufgaben (z.B. der maschinellen √úbersetzung von Fremdsprachen) zwar deutliche Vorteile, die bei kleineren, einfachen Aufgaben aber nicht ins Gewicht fallen.\n",
    "6. Bidirektionale RNNs, die Sequenzen in normaler und in umgekehrter Reihenfolge verarbeiten, sind gut f√ºr die Verarbeitung nat√ºrlicher Sprache geeignet, weisen jedoch bei sequenziellen Daten Schw√§chen auf, wenn die j√ºngste Vergangenheit erheblich informativer ist als der Anfang der Sequenz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weiterf√ºrhende Konzepte: Attention und Sequence Masking\n",
    "\n",
    "Es gibt zwei bedeutende Konzepte, auf die wir an dieser Stelle nicht n√§her eingehen:\n",
    "- **Recurrent Attention: rekurrente Ber√ºcksichtigung** und die \n",
    "- **Sequence Masking: Maskierung von Sequenzen**. \n",
    "\n",
    "Die beiden Konzepte sind insbesondere f√ºr die Verarbeitung nat√ºrlicher Sprache von Bedeutung, f√ºr die Temperaturvorhersage\n",
    "jedoch kaum geeignet. Bei Interesse m√ºssen Sie sich gegebenenfalls jenseits dieses Buchs √ºber diese Konzepte informieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finanzm√§rkte und Machine Learning\n",
    "\n",
    "Manche Leser sind sehr darauf erpicht, die hier vorgestellten Verfahren auf die\n",
    "**Vorhersage von Wertpapierkursen (oder den Wechselkurs von Fremdw√§hrungen usw.)** anzuwenden.\n",
    "\n",
    "- Finanzm√§rkte besitzen allerdings *v√∂llig andere statistische Eigenschaften als nat√ºrliche Ph√§nomene* wie z.B. der Verlauf des Wetters. \n",
    "- Zu versuchen, Finanzm√§rkte durch Machine Learning zu √ºbertreffen, wenn nur √∂ffentlich zug√§ngliche Daten verf√ºgbar sind, ist ein schwieriges Unterfangen. \n",
    "- Sie werden aller Wahrscheinlichkeit nach nur Zeit und Geld verschwenden und kein Ergebnis vorweisen k√∂nnen.\n",
    "\n",
    "Sie d√ºrfen auch nicht vergessen, dass das vorangegangene Verhalten der Finanzm√§rkte kein guter Indikator f√ºr die zu erwartenden Renditen und Ertr√§ge ist ‚Äì beim Autofahren ausschliesslich in den R√ºckspiegel zu sehen, ist ebenfalls keine\n",
    "vern√ºnftige Fahrweise. Andererseits ist Machine Learning durchaus auf Datenmengen\n",
    "anwendbar, bei denen die Vergangenheit tats√§chlich ein guter Indikator\n",
    "f√ºr die Zukunft ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
