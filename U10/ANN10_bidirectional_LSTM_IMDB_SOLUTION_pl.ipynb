{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> ¬© Christoph W√ºrsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN10/ANN10_bidirectional_LSTM_IMDB_SOLUTION_pl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Senstiment-Analyse mit bidirektionalem LSTM (IMDB-Datensatz)\n",
    "\n",
    "**Authoren:** [Francois Chollet](https://twitter.com/fchollet) und [Christoph W√ºrsch](christoph.wuersch@ost.ch)\n",
    "\n",
    "\n",
    "**Beschreibung:** Trainieren Sie ein bidirektionales 2-Schicht-LSTM auf dem IMDB-Filmkritik-Sentiment-Klassifizierungsdatensatz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.Inf = np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Konfiguration\n",
    "# ------------------------\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAXLEN = 200\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "DATA_DIR = \"data/imdb\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) Download der Daten und Datenset\n",
    "\n",
    "Dieses Skript dient zur Vorbereitung des IMDB-Datensatzes f√ºr ein Textklassifikationsprojekt. Es beinhaltet Funktionen zum Herunterladen, Entpacken, Vorverarbeiten und Kodieren der Textdaten sowie eine eigene `Dataset`-Klasse f√ºr PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ `download_and_extract_imdb(data_dir=DATA_DIR)`\n",
    "Diese Funktion l√§dt den IMDB-Datensatz von der Stanford-Webseite herunter und entpackt ihn lokal.\n",
    "\n",
    "- **`url`**: Die URL zum Download des `.tar.gz`-Archivs.\n",
    "- **`tar_path`**: Pfad, unter dem das Archiv gespeichert wird.\n",
    "- **`os.makedirs`**: Erstellt das Zielverzeichnis, falls es noch nicht existiert.\n",
    "- **`urllib.request.urlretrieve`**: L√§dt das Archiv herunter.\n",
    "- **`tarfile.open(...).extractall(...)`**: Entpackt das Archiv in das angegebene Verzeichnis.\n",
    "- Gibt Statusmeldungen √ºber den Fortschritt aus.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ `load_imdb_texts(data_dir, train=True)`\n",
    "L√§dt die Texte und Labels aus dem entpackten IMDB-Datensatz.\n",
    "\n",
    "- **`label_map`**: Weist `pos` den Wert 1 und `neg` den Wert 0 zu.\n",
    "- **`split`**: Bestimmt, ob Trainings- oder Testdaten geladen werden sollen.\n",
    "- Iteriert √ºber alle `.txt`-Dateien im jeweiligen Ordner (`pos` und `neg`).\n",
    "- **R√ºckgabe**: Zwei Listen ‚Äì eine mit Texten, eine mit Labels.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÇÔ∏è `tokenize(text)`\n",
    "Bereinigt und tokenisiert den Text:\n",
    "\n",
    "- Wandelt den Text in Kleinbuchstaben um.\n",
    "- Entfernt HTML-Tags mit einem Regex.\n",
    "- Zerlegt den Text in W√∂rter mithilfe eines Regex, der Wortgrenzen erkennt.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö `build_vocab(texts, max_size=MAX_VOCAB_SIZE)`\n",
    "Erstellt ein Vokabular aus den h√§ufigsten W√∂rtern in den Texten.\n",
    "\n",
    "- Nutzt einen `Counter`, um Wortfrequenzen zu z√§hlen.\n",
    "- Beh√§lt die `max_size - 2` h√§ufigsten W√∂rter bei (zwei Pl√§tze sind reserviert).\n",
    "- **`word2idx`** enth√§lt:\n",
    "  - `<PAD>` ‚Üí 0\n",
    "  - `<UNK>` ‚Üí 1\n",
    "  - und alle h√§ufigen W√∂rter ‚Üí fortlaufende Indizes ab 2.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ `encode(text, word2idx)`\n",
    "Kodiert einen Text als Liste von Integer-IDs entsprechend dem Vokabular.\n",
    "\n",
    "- Unbekannte W√∂rter werden mit dem Index f√ºr `<UNK>` ersetzt.\n",
    "\n",
    "---\n",
    "\n",
    "## üìè `pad_sequence(seq, maxlen=MAXLEN)`\n",
    "K√ºrzt oder erweitert eine Sequenz auf eine feste L√§nge:\n",
    "\n",
    "- Schneidet zu lange Sequenzen ab.\n",
    "- F√ºllt k√ºrzere Sequenzen mit Nullen (`<PAD>`) auf.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ `IMDBDataset` (PyTorch `Dataset`)\n",
    "Eine angepasste Dataset-Klasse f√ºr IMDB-Texte zur Verwendung mit DataLoadern.\n",
    "\n",
    "- **`__init__`**:\n",
    "  - Kodiert und paddet alle Texte bei der Initialisierung.\n",
    "  - Speichert auch die Labels als Float32-Tensoren.\n",
    "\n",
    "- **`__len__`**: Gibt die Anzahl der Beispiele zur√ºck.\n",
    "\n",
    "- **`__getitem__`**: Gibt ein (Text-Tensor, Label)-Paar f√ºr einen Index zur√ºck.\n",
    "\n",
    "---\n",
    "\n",
    "Diese Vorbereitung erm√∂glicht die einfache Nutzung des IMDB-Datensatzes in einem PyTorch-Trainingsprozess mit `DataLoader`, Modell, Training-Loop usw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Datendownload & Vorbereitung\n",
    "# ------------------------\n",
    "def download_and_extract_imdb(data_dir=DATA_DIR):\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    tar_path = os.path.join(data_dir, \"aclImdb_v1.tar.gz\")\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    if not os.path.exists(os.path.join(data_dir, \"aclImdb\")):\n",
    "        print(\"üîΩ Lade IMDB-Daten herunter...\")\n",
    "        urllib.request.urlretrieve(url, tar_path)\n",
    "        print(\"üì¶ Entpacke...\")\n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=data_dir)\n",
    "        print(\"‚úÖ IMDB-Daten bereit.\")\n",
    "    else:\n",
    "        print(\"‚úÖ IMDB-Daten bereits vorhanden.\")\n",
    "\n",
    "\n",
    "def load_imdb_texts(data_dir, train=True):\n",
    "    label_map = {\"pos\": 1, \"neg\": 0}\n",
    "    split = \"train\" if train else \"test\"\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        folder = os.path.join(data_dir, \"aclImdb\", split, label)\n",
    "        for fname in os.listdir(folder):\n",
    "            with open(os.path.join(folder, fname), encoding=\"utf-8\") as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(label_map[label])\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    return re.findall(r\"\\b\\w+\\b\", text)\n",
    "\n",
    "\n",
    "def build_vocab(texts, max_size=MAX_VOCAB_SIZE):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenize(text))\n",
    "    most_common = counter.most_common(max_size - 2)\n",
    "    word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    word2idx.update({word: i + 2 for i, (word, _) in enumerate(most_common)})\n",
    "    return word2idx\n",
    "\n",
    "\n",
    "def encode(text, word2idx):\n",
    "    return [word2idx.get(token, word2idx[\"<UNK>\"]) for token in tokenize(text)]\n",
    "\n",
    "\n",
    "def pad_sequence(seq, maxlen=MAXLEN):\n",
    "    return seq[:maxlen] + [0] * (maxlen - len(seq))\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word2idx):\n",
    "        self.data = [\n",
    "            torch.tensor(pad_sequence(encode(text, word2idx)), dtype=torch.long)\n",
    "            for text in texts\n",
    "        ]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) Modellvarianten\n",
    "\n",
    "Dieses Modell basiert auf einem zweischichtigen **bidirektionalen LSTM** f√ºr bin√§re Textklassifikation (z.‚ÄØB. IMDB Sentiment).\n",
    "\n",
    "## üîß Architektur\n",
    "- **Embedding-Schicht**: Wandelt Wortindizes in Vektoren um.\n",
    "- **LSTM-Schicht 1**: Bidirektionales LSTM, verarbeitet Sequenzen in beide Richtungen.\n",
    "- **Dropout (optional)**: Nur aktiv, wenn `dropout=True` √ºbergeben wird.\n",
    "- **LSTM-Schicht 2**: Nochmals bidirektional, verarbeitet die Ausgaben der ersten Schicht.\n",
    "- **Fully Connected Layer**: Liefert finale Klassifikationslogits (f√ºr 1 Output-Neuron).\n",
    "- **Loss-Funktion**: `BCEWithLogitsLoss` f√ºr bin√§re Klassifikation.\n",
    "\n",
    "## üîÅ Trainings- und Validierungsschritte\n",
    "- **`training_step` & `validation_step`**:\n",
    "  - Berechnen Loss und Accuracy pro Batch.\n",
    "  - Speichern die Metriken in Listen zur sp√§teren Aggregation.\n",
    "\n",
    "- **`on_train_epoch_end` & `on_validation_epoch_end`**:\n",
    "  - Aggregieren und loggen Mittelwerte von Loss und Accuracy.\n",
    "  - Leeren die Speicherlisten.\n",
    "\n",
    "## ‚öôÔ∏è Optimierung\n",
    "- **`configure_optimizers`**: Verwendet den Adam-Optimizer mit Lernrate 1e-3.\n",
    "\n",
    "## ‚úÖ Besonderheiten\n",
    "- Klare Trennung von Training und Validierung.\n",
    "- Explizite Speicherung und Aggregation der Metriken pro Epoche.\n",
    "- Nutzung von `torch.sigmoid` zur Schwellenwertentscheidung bei der Accuracy.\n",
    "\n",
    "Dieses Modell ist bereit f√ºr das Training mit einem `Trainer` in PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Modellvarianten\n",
    "# ------------------------\n",
    "class BiLSTMModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            embed_dim, hidden_dim, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.use_dropout = dropout\n",
    "        self.dropout = nn.Dropout(0.2) if dropout else nn.Identity()\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            2 * hidden_dim, hidden_dim, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(2 * hidden_dim, 1)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # ‚úÖ Speicher f√ºr Aggregation\n",
    "        self.train_losses = []\n",
    "        self.train_accs = []\n",
    "        self.val_losses = []\n",
    "        self.val_accs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x).squeeze(1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        acc = ((torch.sigmoid(logits) > 0.5) == y.bool()).float().mean()\n",
    "        # ‚úÖ in Liste speichern\n",
    "        self.train_losses.append(loss.detach())\n",
    "        self.train_accs.append(acc.detach())\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # ‚úÖ Mittelwert berechnen und loggen\n",
    "        avg_loss = torch.stack(self.train_losses).mean()\n",
    "        avg_acc = torch.stack(self.train_accs).mean()\n",
    "        self.log(\"train_loss\", avg_loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", avg_acc, prog_bar=True)\n",
    "        # Speicher leeren\n",
    "        self.train_losses.clear()\n",
    "        self.train_accs.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        acc = ((torch.sigmoid(logits) > 0.5) == y.bool()).float().mean()\n",
    "        # ‚úÖ in Liste speichern\n",
    "        self.val_losses.append(loss.detach())\n",
    "        self.val_accs.append(acc.detach())\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # ‚úÖ Mittelwert berechnen und loggen\n",
    "        avg_loss = torch.stack(self.val_losses).mean()\n",
    "        avg_acc = torch.stack(self.val_accs).mean()\n",
    "        self.log(\"val_loss\", avg_loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", avg_acc, prog_bar=True)\n",
    "        # Speicher leeren\n",
    "        self.val_losses.clear()\n",
    "        self.val_accs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) Modelltraining\n",
    "\n",
    "Die Funktion `train_model(name, use_dropout)` f√ºhrt das Training eines BiLSTM-Modells mit PyTorch Lightning durch.\n",
    "\n",
    "## üîß Parameter\n",
    "- **`name`**: Bezeichner f√ºr Checkpoint- und Log-Verzeichnisse.\n",
    "- **`use_dropout`**: Steuert, ob Dropout im Modell aktiviert wird.\n",
    "\n",
    "## üß† Modellinstanz\n",
    "- Erstellt ein `BiLSTMModel` mit den globalen Parametern `vocab_size`, `EMBED_DIM`, `HIDDEN_DIM` und dem √ºbergebenen `use_dropout`.\n",
    "\n",
    "## üíæ Checkpointing & Logging\n",
    "- **`ModelCheckpoint`**:\n",
    "  - Speichert das beste Modell basierend auf niedrigstem `val_loss`.\n",
    "  - Nur die Gewichte werden gespeichert (`save_weights_only=True`).\n",
    "  - Speicherort: `checkpoints/{name}/best.ckpt`.\n",
    "\n",
    "- **`EarlyStopping`**:\n",
    "  - Stoppt das Training fr√ºhzeitig, wenn sich `val_loss` f√ºr 5 Epochen nicht verbessert.\n",
    "\n",
    "- **`CSVLogger`**:\n",
    "  - Schreibt Metriken und Logs als CSV-Dateien in den Ordner `logs/{name}/`.\n",
    "\n",
    "## üöÇ Training mit `Trainer`\n",
    "- **`max_epochs`**: Maximale Anzahl an Trainings-Epochen.\n",
    "- **`accelerator`**: Automatische Nutzung von GPU oder CPU je nach `DEVICE`.\n",
    "- **`devices=1`**: Nutzt genau ein Ger√§t.\n",
    "- **`callbacks`**: √úbergibt Checkpointing und EarlyStopping.\n",
    "- **`logger`**: Nutzt CSVLogger zur Protokollierung.\n",
    "- **`enable_progress_bar=True`**: Fortschrittsanzeige w√§hrend des Trainings.\n",
    "\n",
    "## üìà Training starten\n",
    "- `trainer.fit(model, train_loader, val_loader)` startet das Training mit den vorbereiteten DataLoadern.\n",
    "\n",
    "Diese Funktion automatisiert den gesamten Trainingsprozess inklusive √úberwachung, Logging und Modell-Speicherung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Training\n",
    "# ------------------------\n",
    "def train_model(name, use_dropout):\n",
    "    model = BiLSTMModel(vocab_size, EMBED_DIM, HIDDEN_DIM, dropout=use_dropout)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "        dirpath=f\"checkpoints/{name}\",\n",
    "        filename=\"best\",\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    logger = CSVLogger(save_dir=\"logs\", name=name)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        accelerator=\"gpu\" if DEVICE == \"cuda\" else \"cpu\",\n",
    "        devices=1,\n",
    "        callbacks=[checkpoint_callback, early_stop],\n",
    "        logger=logger,\n",
    "        enable_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Logs und Checkpoints l√∂schen\n",
    "# ------------------------\n",
    "import shutil\n",
    "\n",
    "\n",
    "def clear_logs_and_checkpoints():\n",
    "    for folder in [\"logs\", \"checkpoints\"]:\n",
    "        if os.path.exists(folder):\n",
    "            print(f\"üßπ L√∂sche alten Ordner: {folder}\")\n",
    "            shutil.rmtree(folder)\n",
    "        os.makedirs(folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d) Hauptfunktion\n",
    "Die `main()`-Funktion steuert den gesamten Workflow vom Daten-Download bis zum Modelltraining f√ºr zwei Varianten des BiLSTM-Modells (mit und ohne Dropout).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Main\n",
    "# ------------------------\n",
    "def main():\n",
    "    clear_logs_and_checkpoints()\n",
    "    print(\"üîÑ Starte Skript...\")\n",
    "    download_and_extract_imdb(DATA_DIR)\n",
    "    train_texts, train_labels = load_imdb_texts(DATA_DIR, train=True)\n",
    "    test_texts, test_labels = load_imdb_texts(DATA_DIR, train=False)\n",
    "\n",
    "    global word2idx, vocab_size, train_loader, val_loader\n",
    "    word2idx = build_vocab(train_texts, MAX_VOCAB_SIZE)\n",
    "    vocab_size = len(word2idx)\n",
    "\n",
    "    train_dataset = IMDBDataset(train_texts, train_labels, word2idx)\n",
    "    val_dataset = IMDBDataset(test_texts, test_labels, word2idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    train_model(\"BiLSTM_NoDropout\", use_dropout=False)\n",
    "    train_model(\"BiLSTM_WithDropout\", use_dropout=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e)Trainings- und Validierungskurven plotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"logs/BiLSTM_NoDropout/version_0/metrics.csv\")\n",
    "# Zwei DataFrames: einer f√ºr val, einer f√ºr train\n",
    "val_df = df[df[\"val_loss\"].notna()].reset_index(drop=True)\n",
    "train_df = df[df[\"train_loss\"].notna()].reset_index(drop=True)\n",
    "\n",
    "# Join auf 'epoch' (beide DataFrames haben den Wert in der 'epoch'-Spalte)\n",
    "merged_df = pd.merge(\n",
    "    train_df[[\"epoch\", \"train_loss\", \"train_acc\"]],\n",
    "    val_df[[\"epoch\", \"val_loss\", \"val_acc\"]],\n",
    "    on=\"epoch\",\n",
    ")\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotte train und val loss mit seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_loss(df, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(data=df, x=\"epoch\", y=\"train_loss\", label=\"Train Loss\")\n",
    "    sns.lineplot(data=df, x=\"epoch\", y=\"val_loss\", label=\"Validation Loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_loss(merged_df, \"Loss Plot for BiLSTM without Dropout\")\n",
    "\n",
    "\n",
    "def plot_acc(df, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(data=df, x=\"epoch\", y=\"train_acc\", label=\"Train Accuracy\")\n",
    "    sns.lineplot(data=df, x=\"epoch\", y=\"val_acc\", label=\"Validation Accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_acc(merged_df, \"Accuracy Plot for BiLSTM without Dropout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"logs/BiLSTM_WithDropout/version_0/metrics.csv\")\n",
    "# Zwei DataFrames: einer f√ºr val, einer f√ºr train\n",
    "val_df = df[df[\"val_loss\"].notna()].reset_index(drop=True)\n",
    "train_df = df[df[\"train_loss\"].notna()].reset_index(drop=True)\n",
    "\n",
    "# Join auf 'epoch' (beide DataFrames haben den Wert in der 'epoch'-Spalte)\n",
    "merged_df = pd.merge(\n",
    "    train_df[[\"epoch\", \"train_loss\", \"train_acc\"]],\n",
    "    val_df[[\"epoch\", \"val_loss\", \"val_acc\"]],\n",
    "    on=\"epoch\",\n",
    ")\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(merged_df, \"Loss Plot for BiLSTM with Dropout\")\n",
    "plot_acc(merged_df, \"Accuracy Plot for BiLSTM with Dropout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bidirectional_lstm_imdb",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
