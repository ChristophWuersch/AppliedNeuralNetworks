{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> ¬© Christoph W√ºrsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN10/ANN10_WetterJena_SOLUTION_pl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Wettervorhersage mit Hilfe eines LSTM-Modells\n",
    "\n",
    "**Authors:** [Prabhanshu Attri](https://prabhanshu.com/github), [Yashika Sharma](https://github.com/yashika51), [Kristi Takach](https://github.com/ktakattack), [Falak Shah](https://github.com/falaktheoptimist), [Christoph W√ºrsch](christoph.wuersch@ost.ch)\n",
    "\n",
    "\n",
    "**Beschreibung:** Dieses Notebook demonstriert, wie man Zeitreihenvorhersagen mit Hilfe eines LSTM-Modells durchf√ºhrt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Jenaer Klima-Datensatz\n",
    "\n",
    "Wir werden den Jenaer Klimadatensatz verwenden, der vom [Max-Planck-Institut f√ºr Biogeochemie](https://www.bgc-jena.mpg.de/wetter/) zur Verf√ºgung gestellt wird. Der Datensatz besteht aus 14 Merkmalen wie Temperatur, Druck, Feuchtigkeit usw., die einmal pro 10 Minuten aufgezeichnet werden.\n",
    "\n",
    "- [Hier](https://www.bgc-jena.mpg.de/wetter/Wetterstation.pdf) finden Sie eine Beschreibung der Wetterstation und der damit aufgezeichneten Gr√∂ssen.\n",
    "- **Standort**: Wetterstation, Max-Planck-Institut f√ºr Biogeochemie in Jena, Deutschland\n",
    "- **Betrachteter Zeitrahmen**: 10. Januar 2009 - 31. Dezember 2016\n",
    "\n",
    "\n",
    "Die folgende Tabelle zeigt die Spaltennamen, ihre Werteformate und ihre Beschreibung.\n",
    "\n",
    "\n",
    "Index| Features      |Format             |Description              |\n",
    "----:|:--------------|:-----------------:|:------------------------|\n",
    "1    |Date Time      |01.01.2009 00:10:00|Date-time referenz       |\n",
    "2    |p (mbar)       |996.52             |Die von Pascal abgeleitete SI-Einheit des Drucks, die zur Quantifizierung des Innendrucks verwendet wird. In meteorologischen Berichten wird der atmosph√§rische Druck in der Regel in Millibar angegeben.|\n",
    "3    |T (degC)       |-8.02              |Temperatur in Celsius |\n",
    "4    |Tpot (K)       |265.4              |Temperatur in Kelvin |\n",
    "5    |Tdew (degC)    |-8.9               |Temperatur in Celsius relativ zur Luftfeuchtigkeit. Der Taupunkt ist ein Mass f√ºr die absolute Menge an Wasser in der Luft. Der DP ist die Temperatur, bei der die Luft nicht mehr die gesamte Feuchtigkeit halten kann und Wasser kondensiert. |\n",
    "6    |rh (%)         |93.3               |Relative Luftfeuchtigkeit ist ein Mass daf√ºr, wie ges√§ttigt die Luft mit Wasserdampf ist; der %RH-Wert bestimmt die in Sammelobjekten enthaltene Wassermenge. |\n",
    "7    |VPmax (mbar)   |3.33               |S√§ttigungsdampfdruck         |\n",
    "8    |VPact (mbar)   |3.11               |Dampfdruck                   |\n",
    "9    |VPdef (mbar)   |0.22               |Dampfdruckdefizit            |\n",
    "10   |sh (g/kg)      |1.94               |Spezifische Feuchtigkeit     |\n",
    "11   |H2OC (mmol/mol)|3.12               |Wasserdampfkonzentration     |\n",
    "12   |rho (g/m ** 3) |1307.75            |Luftdichte                   |\n",
    "13   |wv (m/s)       |1.03               |Windgeschwindigkeit          |\n",
    "14   |max. wv (m/s)  |1.75               |Maximale Windgeschwindigkeit |\n",
    "15   |wd (deg)       |152.3              |Windrichtung in Grad         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.Inf = np.inf  # Nur als Hotfix, nicht langfristig gedacht\n",
    "\n",
    "# Zielpfade\n",
    "DATA_DIR = \"data/jena\"\n",
    "ZIP_FILENAME = \"jena_climate_2009_2016.csv.zip\"\n",
    "CSV_FILENAME = \"jena_climate_2009_2016.csv\"\n",
    "ZIP_PATH = os.path.join(DATA_DIR, ZIP_FILENAME)\n",
    "CSV_PATH = os.path.join(DATA_DIR, CSV_FILENAME)\n",
    "\n",
    "# URL\n",
    "URL = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "\n",
    "# Daten herunterladen, falls nicht vorhanden\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    print(\"üîΩ Lade Jena Climate ZIP herunter...\")\n",
    "    response = requests.get(URL)\n",
    "    with open(ZIP_PATH, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(\"üì¶ Entpacke ZIP...\")\n",
    "    with ZipFile(ZIP_PATH, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "\n",
    "    print(\"‚úÖ Datei bereit:\", CSV_PATH)\n",
    "else:\n",
    "    print(\"‚úÖ Datei bereits vorhanden:\", CSV_PATH)\n",
    "\n",
    "# CSV laden\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualisierung der Rohdaten\n",
    "\n",
    "Um uns ein Gef√ºhl f√ºr die Daten zu vermitteln, mit denen wir arbeiten, wurde jedes Merkmal unten grafisch dargestellt.\n",
    "Dies zeigt das eindeutige Muster jedes Merkmals √ºber den Zeitraum von 2009 bis 2016.\n",
    "Sie zeigt auch, wo Anomalien vorhanden sind, die w√§hrend der Normalisierung behandelt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "titles = [\n",
    "    \"Pressure\",\n",
    "    \"Temperature\",\n",
    "    \"Temperature in Kelvin\",\n",
    "    \"Temperature (dew point)\",\n",
    "    \"Relative Humidity\",\n",
    "    \"Saturation vapor pressure\",\n",
    "    \"Vapor pressure\",\n",
    "    \"Vapor pressure deficit\",\n",
    "    \"Specific humidity\",\n",
    "    \"Water vapor concentration\",\n",
    "    \"Airtight\",\n",
    "    \"Wind speed\",\n",
    "    \"Maximum wind speed\",\n",
    "    \"Wind direction in degrees\",\n",
    "]\n",
    "\n",
    "feature_keys = [\n",
    "    \"p (mbar)\",\n",
    "    \"T (degC)\",\n",
    "    \"Tpot (K)\",\n",
    "    \"Tdew (degC)\",\n",
    "    \"rh (%)\",\n",
    "    \"VPmax (mbar)\",\n",
    "    \"VPact (mbar)\",\n",
    "    \"VPdef (mbar)\",\n",
    "    \"sh (g/kg)\",\n",
    "    \"H2OC (mmol/mol)\",\n",
    "    \"rho (g/m**3)\",\n",
    "    \"wv (m/s)\",\n",
    "    \"max. wv (m/s)\",\n",
    "    \"wd (deg)\",\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    \"blue\",\n",
    "    \"orange\",\n",
    "    \"green\",\n",
    "    \"red\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"pink\",\n",
    "    \"gray\",\n",
    "    \"olive\",\n",
    "    \"cyan\",\n",
    "]\n",
    "\n",
    "date_time_key = \"Date Time\"\n",
    "\n",
    "\n",
    "def show_raw_visualization(data):\n",
    "    time_data = data[date_time_key]\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    for i in range(len(feature_keys)):\n",
    "        key = feature_keys[i]\n",
    "        c = colors[i % (len(colors))]\n",
    "        t_data = data[key]\n",
    "        t_data.index = time_data\n",
    "        t_data.head()\n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i // 2, i % 2],\n",
    "            color=c,\n",
    "            title=\"{} - {}\".format(titles[i], key),\n",
    "            rot=25,\n",
    "        )\n",
    "        ax.legend([titles[i]])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "show_raw_visualization(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Diese Heatmap zeigt die Korrelation zwischen verschiedenen Merkmalen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_heatmap(data, figsize=(12, 10), fontsize=12, cmap=\"coolwarm\"):\n",
    "    corr = data.corr()\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        annot=False,\n",
    "        fmt=\".2f\",\n",
    "        cmap=cmap,\n",
    "        xticklabels=True,\n",
    "        yticklabels=True,\n",
    "        cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation\"},\n",
    "    )\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.title(\"Feature Correlation Heatmap\", fontsize=fontsize + 2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 'Date Time' entfernen und Heatmap anzeigen\n",
    "numeric_df = df.drop(columns=[\"Date Time\"])\n",
    "show_heatmap(numeric_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Vorverarbeitung der Daten\n",
    "\n",
    "- Hier w√§hlen wir ~300.000 Datenpunkte f√ºr das Training aus. \n",
    "- Die Beobachtung wird alle 10 Minuten aufgezeichnet, d.h. 6 Mal pro Stunde. Wir werden einen Punkt pro Stunde neu ausw√§hlen, da keine drastische Ver√§nderung innerhalb von 60 Minuten erwartet wird. Wir tun dies √ºber das Argument `sampling_rate` Argument im Dienstprogramm `timeseries_dataset_from_array`.\n",
    "\n",
    "- Wir verfolgen die Daten der letzten 720 Zeitstempel (720/6=120 Stunden). Diese Daten werden werden zur Vorhersage der Temperatur nach 72 Zeitstempeln (72/6=12 Stunden) verwendet.\n",
    "\n",
    "- Da jedes Merkmal √ºber Werte mit unterschiedlichen Bereichen hat, f√ºhren wir eine Normalisierung durch, um die Merkmalswerte auf einen Bereich von `[0, 1]` zu beschr√§nken, bevor ein neuronales Netz zu trainieren. Dazu subtrahieren wir den Mittelwert und dividieren ihn durch die Standardabweichung jedes Merkmals.\n",
    "\n",
    "- 71.5 % der Daten werden zum Trainieren des Modells verwendet, d. h. 300.693 Zeilen. `Split_fraction` kann ge√§ndert werden, um diesen Prozentsatz zu √§ndern.\n",
    "\n",
    "Dem Modell werden die Daten der ersten 5 Tage als Input gezeigt, d. h. 720 Beobachtungen, die st√ºndlich abgetastet werden.\n",
    "Die Temperatur nach 72 (12 Stunden $\\times$ 6 Beobachtungen pro Stunde) wird als Label verwendet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "split_fraction = 0.715\n",
    "train_split = int(split_fraction * int(df.shape[0]))\n",
    "step = 6\n",
    "\n",
    "past = 720\n",
    "future = 72\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "def normalize(data, train_split):\n",
    "    data_mean = data[:train_split].mean(axis=0)\n",
    "    data_std = data[:train_split].std(axis=0)\n",
    "    return (data - data_mean) / data_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Aus der Korrelationskarte ist ersichtlich, dass einige Parameter wie die relative Luftfeuchtigkeit und die\n",
    "Spezifische Luftfeuchtigkeit *redundant* sind. Daher werden wir nur ausgew√§hlte Merkmale verwenden, nicht alle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The selected parameters are:\",\n",
    "    \", \".join([titles[i] for i in [0, 1, 5, 7, 8, 10, 11]]),\n",
    ")\n",
    "selected_features = [feature_keys[i] for i in [0, 1, 5, 7, 8, 10, 11]]\n",
    "features = df[selected_features]\n",
    "features.index = df[date_time_key]\n",
    "features.head()\n",
    "\n",
    "features = normalize(features.values, train_split)\n",
    "features = pd.DataFrame(features)\n",
    "features.head()\n",
    "\n",
    "train_data = features.loc[0 : train_split - 1]\n",
    "val_data = features.loc[train_split:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Trainingsdatensatz\n",
    "\n",
    "Die Label des Trainingsdatensatzes beginnen mit der 792. Beobachtung (720 + 72)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "start = past + future\n",
    "end = start + train_split\n",
    "\n",
    "x_train = train_data[[i for i in range(7)]].values\n",
    "y_train = features.iloc[start:end][[1]]\n",
    "\n",
    "sequence_length = int(past / step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Die Funktion `Timeseries_dataset_from_array` nimmt eine Folge von Datenpunkten auf, die in\n",
    "gleichen Intervallen gesammelten Datenpunkte, zusammen mit Zeitreihenparametern wie L√§nge der\n",
    "Sequenzen/Fenster, Abst√§nde zwischen zwei Sequenzen/Fenstern usw., um Stapel von\n",
    "Teilzeitreihen-Eing√§nge und -Ziele, die aus der Hauptzeitreihe entnommen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, sequence_length, sampling_rate):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "        # Anzahl der verwendbaren Startpositionen\n",
    "        self.indices = list(range(0, len(x_data) - sequence_length * sampling_rate + 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        x_seq = self.x_data[\n",
    "            i : i + self.sequence_length * self.sampling_rate : self.sampling_rate\n",
    "        ]\n",
    "        y = self.y_data[i + self.sequence_length * self.sampling_rate - 1]\n",
    "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(\n",
    "            y, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "\n",
    "# Dataset initialisieren\n",
    "train_dataset = TimeseriesDataset(\n",
    "    x_data=x_train,\n",
    "    y_data=y_train.values,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    ")\n",
    "\n",
    "# DataLoader erzeugen\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Validierungsdatensatz\n",
    "\n",
    "Der Validierungsdatensatz darf die letzten 792 Zeilen nicht enthalten, da wir f√ºr diese Datens√§tze keine Beschriftungsdaten haben. Daher muss 792 vom Ende der Daten subtrahiert werden.\n",
    "\n",
    "Der Validierungsdatensatz muss mit 792 Zeilen nach `train_split beginnen`, daher m√ºssen wir Folgendes hinzuf√ºgen\n",
    "`past+future` (792) zu `label_start` addieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "x_end = len(val_data) - past - future\n",
    "\n",
    "label_start = train_split + past + future\n",
    "\n",
    "x_val = val_data.iloc[:x_end][[i for i in range(7)]].values\n",
    "y_val = features.iloc[label_start:][[1]]\n",
    "\n",
    "# Dataset f√ºr Validation\n",
    "val_dataset = TimeseriesDataset(\n",
    "    x_data=x_val,\n",
    "    y_data=y_val.values,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    ")\n",
    "\n",
    "# DataLoader f√ºr Validation\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Einen Batch aus dem Training holen\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch\n",
    "    break  # nur den ersten Batch\n",
    "\n",
    "print(\"Input shape:\", inputs.shape)  # z.B. (256, 120, 7)\n",
    "print(\"Target shape:\", targets.shape)  # z.B. (256, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (a) Modell-Architektur erstellen und kompilieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LSTMRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size=128, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, hidden_size=hidden_size, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]  # nur der letzte Zeitschritt\n",
    "        return self.fc(last_output)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x).squeeze()\n",
    "        loss = F.mse_loss(preds, y.squeeze())\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x).squeeze()\n",
    "        loss = F.mse_loss(preds, y.squeeze())\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMRegressor(\n",
    "    input_size=x_train.shape[1],  # z.B. 7\n",
    "    hidden_size=32,\n",
    "    learning_rate=learning_rate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# Logger erstellen\n",
    "csv_logger = CSVLogger(\"logs\", name=\"lstm_weather_forecast\")\n",
    "\n",
    "# EarlyStopping Callback erstellen\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Die Metrik, die √ºberwacht werden soll (Validierungsverlust)\n",
    "    patience=4,  # Anzahl der Epochen ohne Verbesserung, nach denen das Training gestoppt wird\n",
    "    mode=\"min\",  # Wir wollen den Validierungsverlust minimieren\n",
    ")\n",
    "\n",
    "# Trainer mit CSVLogger und EarlyStopping\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=csv_logger,\n",
    "    callbacks=[early_stopping_callback],  # F√ºge den Callback hier hinzu\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Lernkurven visualisieren\n",
    "\n",
    "Wir k√∂nnen den Verlust mit der nachstehenden Funktion veranschaulichen. Nach einem Punkt h√∂rt der Verlust auf abnehmend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def find_latest_metrics_path(base_log_dir):\n",
    "    version_dirs = [d for d in os.listdir(base_log_dir) if d.startswith(\"version_\")]\n",
    "    if not version_dirs:\n",
    "        raise FileNotFoundError(\"Keine version_x Ordner im Log-Verzeichnis gefunden.\")\n",
    "\n",
    "    # Nach Versionsnummer sortieren\n",
    "    version_dirs.sort(key=lambda x: int(x.split(\"_\")[1]))\n",
    "    latest_version = version_dirs[-1]\n",
    "\n",
    "    return os.path.join(base_log_dir, latest_version, \"metrics.csv\")\n",
    "\n",
    "\n",
    "# Hauptverzeichnis f√ºr Logs\n",
    "base_log_dir = os.path.join(\"logs\", \"lstm_weather_forecast\")\n",
    "metrics_path = find_latest_metrics_path(base_log_dir)\n",
    "\n",
    "print(\"Neueste metrics.csv gefunden unter:\", metrics_path)\n",
    "\n",
    "# DataFrame vorbereiten\n",
    "df = pd.read_csv(metrics_path)\n",
    "df_train = df[[\"epoch\", \"train_loss\"]].dropna().copy()\n",
    "df_val = df[[\"epoch\", \"val_loss\"]].dropna().copy()\n",
    "\n",
    "# Merge auf 'epoch'\n",
    "df_merged = pd.merge(df_train, df_val, on=\"epoch\", suffixes=(\"_train\", \"_val\"))\n",
    "\n",
    "df_merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotte df_merged mit seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setze den Stil f√ºr die Plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Erstelle eine Figur\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot f√ºr den Verlust\n",
    "sns.lineplot(\n",
    "    data=df_merged, x=\"epoch\", y=\"train_loss\", label=\"Trainingsverlust\"\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_merged, x=\"epoch\", y=\"val_loss\", label=\"Validierungsverlust\"\n",
    ")\n",
    "plt.title(\"Trainings- und Validierungsverlust\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Verlust\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (d) Wettervorhersage\n",
    "\n",
    "Das oben trainierte Modell ist nun in der Lage, Vorhersagen f√ºr 5 Wertes√§tze aus Validierungssatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, val, marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, val.flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Vorhersagen f√ºr 5 Beispiele aus dem Validierungs-Dataloader\n",
    "model.eval()  # Modell in den Evaluierungsmodus setzen\n",
    "with torch.no_grad():  # Keine Gradientenberechnung\n",
    "    for i, (x, y) in enumerate(val_loader):\n",
    "        if i >= 5:  # Nur 5 Beispiele anzeigen\n",
    "            break\n",
    "        predictions = model(x).squeeze().numpy()  # Vorhersagen des Modells\n",
    "        show_plot(\n",
    "            [x[0][:, 1].numpy(), y[0].numpy(), predictions[0]],\n",
    "            delta=12,\n",
    "            title=\"Single Step Prediction\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "timeseries_weather_forecasting",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
