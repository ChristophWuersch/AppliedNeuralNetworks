{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN03/3.2-Vorhersage_Hauspreise-PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "%pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beispiel für eine Regression: Vorhersage der Kaufpreise von Häusern\n",
    "\n",
    "Dieses Notebook enthält die Codebeispiele aus Kapitel 3, Abschnitt 5 von [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python).\n",
    "\n",
    "- Bei den beiden letzten Beispielen handelte es sich um **Klassifizierungsaufgaben**, die zum Ziel hatten, eine einzige Klassenbezeichnung eines Eingabedatenpunkts vorherzusagen. \n",
    "- Eine weitere häufige Machine-Learning-Aufgabe ist die **Regression**, bei der es darum geht, statt einer bestimmten Klassenbezeichnung stetige Werte vorherzusagen, z.B. die Prognose der morgigen Temperatur anhand von meteorologischen Daten oder eine Vorhersage des Zeitraums, der zum Abschluss eines Softwareprojekts erforderlich ist, anhand der Spezifikationen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Hinweis**\n",
    "Verwechseln Sie die *Regression* nicht mit der *logistischen Regression*. Tatsächlich handelt es sich bei der logistischen Regression verwirrenderweise nicht um einen Regressionsalgorithmus, sondern um einen Klassifizierungsalgorithmus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Die Boston-Housing-Price-Datensammlung\n",
    "\n",
    "- Wir werden versuchen, den durchschnittlichen Preis von in den Vorstädten von Boston gelegenen Immobilien anhand von Mitte der 1970er-Jahre erhobenen Daten über die Vorstädte vorherzusagen. \n",
    "- Dazu gehören beispielsweise die Kriminalitätsrate, die Höhe der Grundsteuer usw. \n",
    "\n",
    "Diese Datensammlung weist gegenüber den beiden vorangegangenen Beispielen einen interessanten Unterschied auf. Sie besteht aus relativ wenig Datenpunkten, nämlich aus nur 506, die in 404 Trainingsdatensätze und 102 Testdatensätze unterteilt sind. Die Merkmale der Eingabedaten (wie etwa die Kriminalitätsrate) sind von völlig verschiedenen Grössenordnungen.\n",
    "Bei einigen dieser Merkmale handelt es sich um Verhältnisse oder Quoten, die Werte zwischen 0 und 1 annehmen können, andere liegen zwischen 1 und 12, wieder andere zwischen 0 und 100 usw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O boston_housing.pkl \"https://github.com/ChristophWuersch/AppliedNeuralNetworks/raw/refs/heads/main/ANN02/Daten/boston_housing.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries since execution state was reset\n",
    "with open(\"boston_housing.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "train_data = data[\"train_data\"]\n",
    "train_targets = data[\"train_targets\"]\n",
    "test_data = data[\"test_data\"]\n",
    "test_targets = data[\"test_targets\"]\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wie Sie sehen, stehen 404 Trainingssamples und 102 Testsamples zur Verfügung,\n",
    "die jeweils 13 numerische Merkmale besitzen, wie die Pro-Kopf-Kriminalitätsrate,\n",
    "die durchschnittliche Anzahl der Zimmer pro Wohnung/Gebäude, ein Index, der\n",
    "die Zugänglichkeit von Schnellstrassen erfasst, usw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Die 13 Merkmale (features) in den Input-Daten sind wie folgt:\n",
    "\n",
    "| feature no. | Beschreibung |\n",
    "|:------------|:-------------|\n",
    "| 1. | Per capita crime rate |\n",
    "| 2. | Proportion of residential land zoned for lots over 25,000 square feet. |\n",
    "| 3. | Proportion of non-retail business acres per town. |\n",
    "| 4. | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). |\n",
    "| 5. | Nitric oxides concentration (parts per 10 million). |\n",
    "| 6. | Average number of rooms per dwelling. |\n",
    "| 7. | Proportion of owner-occupied units built prior to 1940. |\n",
    "| 8. | Weighted distances to five Boston employment centres. |\n",
    "| 9. | Index of accessibility to radial highways. |\n",
    "| 10. | Full-value property-tax rate per $10,000. |\n",
    "| 11. | Pupil-teacher ratio by town. |\n",
    "| 12. | 1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town. |\n",
    "| 13. | % lower status of the population.|\n",
    "\n",
    "Die Zielvariablen sind die Medianwerte der Preise der von den Eigentümern selbst\n",
    "genutzten Wohneinheiten in Einheiten von 1.000 Dollar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Die Preise liegen typischerweise zwischen 10.000 und 50.000 Dollar. Sie meinen, dass sei wenig? \n",
    "- Denken Sie daran, dass die Zahlen aus der Mitte der 1970er-Jahre stammen und nicht inflationsbereinigt sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Daten vorbereiten\n",
    "\n",
    "- Es würde zu Problemen führen, wenn man ein NN mit Werten füttert, die von völlig unterschiedlicher Grössenordnung sind. \n",
    "- as NN könnte sich möglicherweise automatisch an so ungleichartige Daten anpassen, aber das Lernen würde dadurch definitiv erschwert. \n",
    "- Zur Handhabung solcher Daten führt man üblicherweise eine **Normierung oder Standardisierung** (engl. Standardization, dt. auch als Normalisierung bezeichnet) der Merkmale durch: \n",
    "- Bei jedem Merkmal der Eingabedaten (eine Spalte in der Matrix der Eingabedaten) subtrahiert man den Mittelwert der gesamten Spalte und dividiert durch die Standardabweichung, sodass die Merkmalswerte um 0 zentriert sind und die Standardabweichung 1 besitzen. \n",
    "\n",
    "$$ \\tilde{X}=\\frac{X-\\mu_X}{s_X}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter des Transformer werden auf den Trainingsdaten bestimmt (gefittet, .fit in sklearn)\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "# und diese Parameter werden verwendet, um die Testdaten zu skalieren.\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_targets = torch.tensor(train_targets, dtype=torch.float32)\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_targets = torch.tensor(test_targets, dtype=torch.float32)\n",
    "\n",
    "print(train_data.shape, train_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Beachten Sie hier, dass die zur Normierung der Testdaten verwendeten Werte mit\n",
    "den Trainingsdaten berechnet werden.** Sie sollten in Ihrem Workflow niemals einen Wert verwenden, der anhand der Testdaten berechnet wird, auch nicht für etwas so Einfaches wie die Normierung der Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NN erzeugen\n",
    "\n",
    "Weil nur wenige Samples verfügbar sind, erzeugen wir ein sehr kleines NN aus\n",
    "zwei verdeckten Layern mit jeweils 64 Einheiten. \n",
    "\n",
    "- Im Allgemeinen gilt: Je weniger Trainingsdaten vorhanden sind, desto schlimmer wird sich die Überanpassung auswirken. Ein kleines NN ist eine Möglichkeit, die Überanpassung abzuschwächen.\n",
    "\n",
    "Das gleiche Modell muss mehrmals instanziiert werden, daher wird zum\n",
    "Erzeugen eine Funktion benutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonHousingModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BostonHousingModel, self).__init__()\n",
    "\n",
    "        # Define layers\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "        # Loss function\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Metric for Mean Absolute Error (MAE)\n",
    "        self.mae = torchmetrics.MeanAbsoluteError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y.view(-1, 1))\n",
    "        mae = self.mae(y_pred, y.view(-1, 1))\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_mae\", mae, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y.view(-1, 1))\n",
    "        mae = self.mae(y_pred, y.view(-1, 1))\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_mae\", mae, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.RMSprop(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mean Squared Error (`mse`) und Mean Absolute Error (`mae`)\n",
    "\n",
    "- Beachten Sie hier, dass bei der Kompilierung der **mittlere quadratische Fehler\n",
    "(`mse`, Mean Squared Error)**, also das Quadrat der Differenz zwischen Vorhersage und Zielvariable, als Verlustfunktion angegeben wird. \n",
    "- Diese Verlustfunktion ist bei Regressionsaufgaben sehr gebräuchlich.\n",
    "\n",
    "$$\\mathrm{mse} =\\frac{1}{N} \\sum_{i=1}^N \\big\\Vert y_i-\\hat{y}_i \\big\\Vert^2_2  = \\frac{1}{N}\\sum_{i=1}^N \\big\\Vert y_i-\\hat{y}(x_i)\\big\\Vert^2_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- Das NN endet mit einer einzelnen Einheit und ohne Aktivierung (einem linearen Layer). \n",
    "- Dieser Aufbau ist typisch für skalare Regressionen (also Regressionen, die einen einzelnen stetigen Wert vorhersagen). Eine Aktivierungsfunktion würde den Bereich beschränken, den die Werte der Ausgabe annehmen können. \n",
    "\n",
    "Wenn Sie beispielsweise eine sigmoid-Aktivierungsfunktion auf den letzten Layer anwenden, könnte das Modell nur erlernen, Werte zwischen 0 und 1 vorherzusagen. Da der letzte Layer im vorliegenden Fall rein linear ist, kann das NN erlernen, Werte\n",
    "aus einem beliebigen Bereich vorherzusagen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Ausserdem wird während des Trainings eine neue Kennzahl überwacht: der **mittlere absolute Fehler** (`mae`, Mean Absolute Error). \n",
    "- Hierbei handelt es sich um den Betrag der Differenz von Vorhersagen und Zielwerten. Ein Wert von 0.5 würde bei dieser Aufgabe bedeuten, dass die Vorhersagen durchschnittlich um 500 Dollar von den tatsächlichen Werten abweichen.\n",
    "\n",
    "$$\\mathrm{mae} =\\frac{1}{N} \\sum_{i=1}^N \\vert y_i-\\hat{y}_i \\vert =\\frac{1}{N} \\sum_{i=1}^N \\vert y_i-\\hat{y}(x_i)\\vert$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(train_data, train_targets)\n",
    "test_dataset = TensorDataset(test_data, test_targets)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "input_dim = train_data.shape[1]\n",
    "model = BostonHousingModel(input_dim=input_dim)\n",
    "\n",
    "# Train the model using PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",\n",
    "    enable_progress_bar=False,\n",
    "    logger=pl.loggers.CSVLogger(\"logs\"),  # Save logs for plotting\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logs from CSV for plotting\n",
    "metrics = pd.read_csv(\"./logs/lightning_logs/version_0/metrics.csv\")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss and MAE per epoch\n",
    "train_loss = metrics[\"train_loss_epoch\"].values\n",
    "val_loss = metrics[\"val_loss\"].values\n",
    "train_mae = metrics[\"train_mae_epoch\"].values\n",
    "val_mae = metrics[\"val_mae\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_loss, label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(epochs, val_loss, label=\"Validation Loss\", marker=\"s\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MAE Curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_mae, label=\"Train MAE\", marker=\"o\", linestyle=\"dashed\")\n",
    "plt.plot(epochs, val_mae, label=\"Validation MAE\", marker=\"s\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Absolute Error (MAE)\")\n",
    "plt.title(\"Training vs Validation MAE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-fache Kreuzvalidierungen des Ansatzes\n",
    "\n",
    "- Um die Leistung des NN zu beurteilen, während weiterhin eine Abstimmung der Parameter (wie etwa die Anzahl der Epochen beim Training) erfolgt, können Sie die Daten, wie beim letzten Beispiel, in eine Trainings- und eine Validierungsdatenmenge\n",
    "aufteilen. \n",
    "- Da es jedoch so wenige Datenpunkte gibt, wäre die Validierungsmenge sehr klein (vielleicht nur rund 100 Samples). Dementsprechend könnten die Validierungsscores heftig schwanken, je nachdem, welche Datenpunkte Sie für die Validierung und für das Training auswählen. Die Validierungsscores würden bezüglich der Aufteilung eine hohe Varianz aufweisen, und das würde eine zuverlässige Beurteilung des Modells verhindern.\n",
    "\n",
    "- Unter diesen Umständen hat sich die **K-fache Kreuzvalidierung** bewährt). Die verfügbaren Daten werden in $K$ Teilmengen aufgeteilt (typischerweise ist $K = 4$ oder $5$). Dann werden $K$ identische Modelle instanziiert, die mit jeweils $K – 1$ Teilmengen trainiert und anhand der verbleibenden Teilmenge beurteilt werden. \n",
    "- Der Validierungsscore des Modells ergibt sich dann als Durchschnittswert der $K$ erreichten Validierungsscores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/crossvalidation_3fold.png\" width=\"840\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Als Code formuliert, ist das \"straight forward\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(train_data, train_targets)\n",
    "\n",
    "\n",
    "# Define k-fold cross-validation\n",
    "k = 4\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "all_scores = []\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Processing fold #{i + 1}\")\n",
    "\n",
    "    # Create DataLoaders for training and validation sets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model\n",
    "    model = BostonHousingModel(input_dim=train_data.shape[1])\n",
    "\n",
    "    # Train using PyTorch Lightning Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        logger=False,\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # Evaluate model on validation set\n",
    "    val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    val_mae = trainer.callback_metrics[\"val_mae\"].item()\n",
    "    all_scores.append(val_mae)\n",
    "\n",
    "print(\"Cross-validation MAE scores:\", all_scores)\n",
    "print(\"Mean MAE:\", sum(all_scores) / len(all_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Die verschiedenen Durchläufe liefern tatsächlich ziemlich unterschiedliche Validierungsscores, die von 2.6 bis 3.2 reichen. Der Durchschnittswert von 3.5 ist eine sehr viel verlässlichere Kennzahl als jeder einzelne Wert – und das ist ja letzten Endes auch der eigentliche Sinn der K-fachen Kreuzvalidierung. \n",
    "\n",
    "- In diesem Fall weichen die Vorhersagen durchschnittlich um 3.000 Dollar von den tatsächlichen Werten ab. In Anbetracht der Tatsache, dass die Preise zwischen 10.000 und 50.000 Dollar liegen, ist das kein unerheblicher Betrag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nun soll das NN etwas länger trainiert werden, nämlich 500 Epochen lang. Um aufzuzeichnen, wie gut das Modell in den verschiedenen Epochen funktioniert, können Sie die Trainingsschleife wie folgt modifizieren, sodass die Validierungsscores der Epochen protokolliert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create PyTorch dataset\n",
    "dataset = TensorDataset(train_data, train_targets)\n",
    "\n",
    "# Instantiate model\n",
    "input_dim = train_data.shape[1]\n",
    "model = BostonHousingModel(input_dim=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "k = 2\n",
    "num_epochs = 500\n",
    "batch_size = 8\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "train_mae_per_epoch = []\n",
    "val_mae_per_epoch = []\n",
    "\n",
    "fold_train_mae = []\n",
    "fold_val_mae = []\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Processing fold #{i + 1}\")\n",
    "\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = BostonHousingModel(input_dim=train_data.shape[1])\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        enable_progress_bar=False,\n",
    "        logger=pl.loggers.CSVLogger(\"logs\"),  # Save logs for plotting\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logs from CSV for plotting\n",
    "metrics = pd.read_csv(\"./logs/lightning_logs/version_1/metrics.csv\") #TODO: \n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss and MAE per epoch\n",
    "train_loss = metrics[\"train_loss_epoch\"].values\n",
    "val_loss = metrics[\"val_loss\"].values\n",
    "train_mae = metrics[\"train_mae_epoch\"].values\n",
    "val_mae = metrics[\"val_mae\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_loss, label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(epochs, val_loss, label=\"Validation Loss\", marker=\"s\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Nehmen Sie Folgendes aus diesem Abschnitt mit:\n",
    "- Bei Regressionen kommen andere **Verlustfunktionen** als bei Klassifizierungen zum Einsatz. Der mittlere quadratische Fehler (Mean Squared Error, MSE) ist eine für Regressionen gebräuchliche Verlustfunktion.\n",
    "- Auch die zur Beurteilung einer Regression gebräuchlichen Kennzahlen unterscheiden sich von denen, die für Klassifizierungen üblich sind. Das Konzept der Korrektklassifizierungsrate ist auf eine Regression nicht anwendbar. Stattdessen ist der mittlere absolute Fehler (Mean Absolute Error, MAE) eine gängige Kennzahl.\n",
    "- Wenn die Merkmale der Eingabedaten von sehr unterschiedlicher Grössenordnung sind, sollten die Merkmale im Rahmen der Datenvorverarbeitung **voneinander unabhängig skaliert** (mittels der Trainingsdaten) werden.\n",
    "- Falls nur wenige Daten vorliegen, können Sie zur verlässlicheren Beurteilung des Modells eine **$K$-fache Kreuzvalidierung** verwenden.\n",
    "- Sollten nur wenig Trainingsdaten verfügbar sein, sollten kleine NN mit nur einigen wenigen verdeckten Layern (typischerweise ein oder zwei) verwendet werden, um eine **Überanpassung zu vermeiden**.\n",
    "\n",
    "Sie sind nun in der Lage, die für Vektordaten gebräuchlichsten Machine-Learning-Aufgaben zu handhaben: \n",
    "1. **Binärklassifizierung**\n",
    "2. **Mehrfachklassifizierung** und\n",
    "3. **skalare Regression**. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where's the intelligence?\n",
    "\n",
    "- Für gewöhnlich ist es erforderlich, Rohdaten aufzubereiten (preprocessing, scaling, transforing,...), bevor man ein NN mit ihnen füttern kann.\n",
    "- Wenn die Daten Merkmale von sehr unterschiedlicher Grössenordnung enthalten, sollten sie im Rahmen der Datenvorverarbeitung voneinander unabhängig skaliert werden.\n",
    "- Bei fortgesetztem Training eines NNs kommt und mit zunehmender Modellkomplexität und abnehmender Datenmenge kommt es früher oder später zu einer **Überanpassung**, die zu schlechteren Ergebnissen bei noch unbekannten Daten führt.\n",
    "- Falls nur wenige Trainingsdaten zur Verfügung stehen, sollten Sie ein kleines NN mit nur ein oder zwei verdeckten Layern verwenden, um eine Überanpassung zu vermeiden.\n",
    "- Wenn Ihre Daten in sehr viele Kategorien unterteilt sind, verursachen Sie durch zu kleine zwischenliegende Layer womöglich Informationslecks.\n",
    "- Bei Regressionen werden andere Verlustfunktionen und Leistungskennzahlen verwendet als bei Klassifizierungen.\n",
    "- Bei kleinen Datenmengen kann die **$K$-fache Kreuzvalidierung** dabei helfen, ein Modell verlässlicher zu beurteilen. \n",
    "- Der Extremfall liegt vor bei $K=N$. In diesem Fall spricht man von der **Leave-One-Out-Kreuzvalidierung** (LOO=Leave One Out)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "ger",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ger",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
