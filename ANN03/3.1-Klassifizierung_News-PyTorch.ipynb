{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN03/3.1-Klassifizierung_News-PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ein Beispiel für eine Mehrfachklassifizierung:\n",
    "\n",
    "Dieses Notebook enthält die Codebeispiele aus Kapitel 3, Abschnitt 5 von [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python).\n",
    "\n",
    "\n",
    "## Klassifizierung von Nachrichtenmeldungen\n",
    "\n",
    "Im letzten Abschnitt haben Sie erfahren, wie man mit vollständig verbundenen NNs Vektoreingaben klassifiziert und sie zwei sich gegenseitig ausschliessenden Klassen zuordnet. Aber was ist, wenn es mehr als zwei Klassen gibt?\n",
    "\n",
    "- In dieser Lektion werden wir ein NN entwickeln, das Nachrichtenmeldungen der Agentur Reuters 46 sich gegenseitig ausschliessenden Themenbereichen zuordnet. Da es hier mehrere Klassen gibt, spricht man von einer *Mehrfachklassifizierung*.\n",
    "- Und weil jeder Datenpunkt nur einer einzigen Kategorie zugeordnet werden darf, handelt es sich genau genommen um eine *Single-Label-Mehrfachklassifizierung.*\n",
    "- Wenn die Datenpunkte mehreren Kategorien (in diesem Fall Themengebieten) angehören dürfen, haben Sie es hingegen mit einer Multi-Label-Mehrfachklassifizierung zu tun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Die Reuters-Datensammlung\n",
    "\n",
    "Wir werden die Reuters-Datensammlung verwenden, eine aus kurzen Nachrichtenmeldungen und ihren Themengebieten bestehende Datenmenge, die 1986 von Reuters veröffentlicht wurde. Diese einfache Datenmenge wird häufig für Textklassifizierungsexperimente genutzt. Es gibt 46 verschiedene Themengebiete. Einige davon enthalten mehr Meldungen als andere, aber in der Trainingsmenge liegen zu jedem Themengebiet mindestens zehn Meldungen vor.\n",
    "\n",
    "\n",
    "Ebenso wie die IMDb- und die MNIST-Datensammlung ist auch die Reuters-Datensammlung Bestandteil von Keras. Sehen wir uns das genauer an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "%pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dateien laden\n",
    "!wget -O reuters_dataset_with_word_index.pkl \"https://github.com/ChristophWuersch/AppliedNeuralNetworks/raw/refs/heads/main/ANN02/Daten/reuters_dataset_with_word_index.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Save the dataset and word index into a pickle file\n",
    "pickle_file_path = \"reuters_dataset_with_word_index.pkl\"\n",
    "\n",
    "# Reload the data from the pickle file\n",
    "with open(pickle_file_path, \"rb\") as file:\n",
    "    ibdm_data = pickle.load(file)\n",
    "\n",
    "# Verify reloaded word index matches the original\n",
    "word_index = ibdm_data[\"word_index\"]\n",
    "list(word_index.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ibdm_data[\"train_data\"]\n",
    "test_data = ibdm_data[\"test_data\"]\n",
    "\n",
    "train_labels = ibdm_data[\"train_labels\"]\n",
    "test_labels = ibdm_data[\"test_labels\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Wie bei der IMDb-Datensammlung beschränkt das Argument `num=10000` die Daten auf die 10.000 am häufigsten vorkommenden Wörter.\n",
    "- Es gibt 8.982 Trainings- und 2.246 Testdatensätze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Und wie die Bewertungen der IMDb-Datensammlung bestehen die Datensätze aus einer Liste von Integern (Wortindizes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Falls Sie möchten, können Sie die Daten wieder in Wörter umwandeln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# Note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_newswire = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Den Datensätzen ist ein Integer zwischen 0 und 45 als Themenindex zugeordnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Daten vorbereiten (data preprocessing)\n",
    "Die *Vektorisierung der Daten* (tidy dataset) können Sie mit genau demselben Code wie beim letzten Beispiel vornehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.0\n",
    "    return results\n",
    "\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "Für die Vektorisierung der Klassenbezeichnungen (Labels) gibt es zwei Möglichkeiten:\n",
    "\n",
    "1. Sie können die Liste entweder in einen **Integertensor** umwandeln oder eine **One-hot-Codierung** verwenden. \n",
    "- Die **One-hot-Codierung** ist ein für kategoriale Daten gebräuchliches Format und wird mitunter auch als kategoriale Codierung bezeichnet. \n",
    "- Im vorliegenden Fall wird für die One-hot-Codierung der Klassenbezeichnungen ein aus Nullen bestehender Vektor verwendet, der an der Indexposition der Klassenbezeichnung eine 1 enthält. Hier ein Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.0\n",
    "    return results\n",
    "\n",
    "\n",
    "# Our vectorized training labels\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# Our vectorized test labels\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_train_labels[:, 11].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Beachten Sie, dass Keras hierfür eine integrierte Möglichkeit bietet, die Sie beim MNIST-Beispiel auch schon in Aktion gesehen haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print versions in a compact form\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for the Reuters dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (numpy.ndarray): The feature data (one-hot encoded).\n",
    "            labels (numpy.ndarray): The labels (one-hot encoded).\n",
    "        \"\"\"\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = ReutersDataset(x_train, one_hot_train_labels)\n",
    "test_dataset = ReutersDataset(x_test, one_hot_test_labels)\n",
    "\n",
    "# Create DataLoader instances for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Example usage\n",
    "for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bauen wir unser Neuronales Netz auf\n",
    "\n",
    "\n",
    "- Auf den ersten Blick ähnelt die Klassifizierung nach Themengebieten der Klassifizierung der Filmbewertungen, denn in beiden Fällen versuchen wir, kurze Textabschnitte zu klassifizieren. \n",
    "- Der Unterschied besteht hier jedoch darin, dass die Anzahl der möglichen Klassenbezeichnungen von 2 auf 46 gestiegen ist. **Die Dimensionalität des Ausgaberaums ist sehr viel grösser**.\n",
    "- Bei einem Stapel von Dense-Layern, wie wir ihn bislang verwendet haben, kann jeder Layer nur auf die in der Ausgabe des vorhergehenden Layers enthaltenen Informationen zugreifen. Wenn in einem Layer für die Lösung der Klassifizierungsaufgabe relevante Informationen verloren gehen, können diese in den nachfolgenden Layern nicht wiederhergestellt werden: Jeder Layer kann potenziell zu einem Informationsleck werden. \n",
    "\n",
    "Im letzten Beispiel haben Sie 16-dimensionale zwischenliegende Layer verwendet, aber ein 16-dimensionaler Raum ist womöglich\n",
    "nicht ausreichend, um die Unterscheidung von 46 verschiedenen Klassen zu erlernen: Die möglicherweise zu kleinen Layer könnten zu einem **Informationsleck** werden und relevante Informationen dauerhaft entfernen.\n",
    "Aus diesem Grund sollten Sie grössere Layer verwenden. \n",
    "\n",
    "Versuchen wir es mit 64 Einheiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersLightningModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A PyTorch Lightning implementation of the Keras Sequential model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReutersLightningModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(10000, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 46),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        predictions = self(data)\n",
    "        loss = nn.CrossEntropyLoss()(predictions, labels.argmax(dim=1))\n",
    "        accuracy = (predictions.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(accuracy.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        predictions = self(data)\n",
    "        loss = nn.CrossEntropyLoss()(predictions, labels.argmax(dim=1))\n",
    "        accuracy = (predictions.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_accuracy\", accuracy, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_accuracies.append(accuracy.item())\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader setup\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model instantiation\n",
    "model = ReutersLightningModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example input size: (batch_size, input_features)\n",
    "summary(model, input_size=(batch_size, 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input tensor\n",
    "example_input = torch.randn(1, 10000).to(device)\n",
    "\n",
    "# Get the model graph\n",
    "graph = make_dot(model(example_input), params=dict(model.named_parameters()))\n",
    "\n",
    "# Save the graph to a file\n",
    "graph.render(\"model_visualization\", format=\"png\", cleanup=True)\n",
    "\n",
    "# Display the visualization in Jupyter Notebook\n",
    "display(Image(\"model_visualization.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer setup\n",
    "max_epochs = 10\n",
    "trainer = pl.Trainer(max_epochs=10, log_every_n_steps=1)\n",
    "trainer.fit(model, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Bei dieser Architektur sind zwei weitere Dinge zu beachten:\n",
    "\n",
    "- Das NN endet mit einem **Dense-Layer der Größe 46**. Das heisst, dass das NN für jede Eingabe einen 46-dimensionalen Vektor ausgibt. Jedes Element (jede Dimension) dieses Vektors codiert eine andere Klassenbezeichnung.\n",
    "- Der letzte Layer verwendet eine **softmax-Aktivierungsfunktion**. Sie kennen diese Vorgehensweise bereits vom MNIST-Beispiel. - Die Ausgabe des NNs ist eine **Wahrscheinlichkeitsverteilung** der 46 verschiedenen Klassenbezeichnungen – das NN erzeugt für jede Eingabe einen 46-dimensionalen Ausgabevektor, wobei `output[i]` die Wahrscheinlichkeit dafür angibt, dass das Sample zur Klasse `i` gehört. Die Summe der 46 Scores beträgt 1.\n",
    "\n",
    "In diesem Fall ist die **kategoriale Kreuzentropie (categorial_crossentropy)** die am besten geeignete Verlustfunktion. Sie *misst die Differenz zwischen zwei Wahrscheinlichkeitsverteilungen* – hier die Differenz zwischen der vom NN ausgegebenen\n",
    "Wahrscheinlichkeitsverteilung und der tatsächlichen Verteilung der Klassenbezeichnungen. Durch die Minimierung der Differenz zwischen diesen beiden Verteilungen wird das NN darauf trainiert, eine Ausgabe zu erzeugen, die der tatsächlichen Verteilung der Klassen so nahe wie möglich kommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.callback_metrics\n",
    "metrics.keys()\n",
    "train_loss = metrics[\"train_loss_epoch\"].cpu().numpy()\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function to plot learning curves\n",
    "def plot_learning_curves(model):\n",
    "    epochs_train = (\n",
    "        np.array(range(1, len(model.train_losses) + 1))\n",
    "        / len(model.train_losses)\n",
    "        * max_epochs\n",
    "    )\n",
    "    epochs_val = (\n",
    "        np.array(range(1, len(model.val_losses) + 1))\n",
    "        / len(model.val_losses)\n",
    "        * max_epochs\n",
    "    )\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_train, model.train_losses, \"b.-\", label=\"Training Loss\")\n",
    "    plt.plot(epochs_val, model.val_losses, \"r.-\", label=\"Validation Loss\")\n",
    "    plt.title(\"Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_train, model.train_accuracies, \"b.-\", label=\"Training Accuracy\")\n",
    "    plt.plot(epochs_val, model.val_accuracies, \"r.-\", label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy Over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nach neun Epochen kommt es zu einer **Überanpassung**. Jetzt trainieren wir wieder\n",
    "ein völlig neues NN neun Epochen lang und beurteilen es anschliessend anhand\n",
    "der Testdaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Dieser Ansatz erzielt eine Korrektklassifizierungsrate von knapp 80%. Eine vollkommen auf Zufall beruhende Binärklassifizierung einer Datenmenge mit ausgewogener Verteilung würde eine Korrektklassifizierungsrate von 50% erreichen.\n",
    "\n",
    "Im vorliegenden Fall einer nicht ausgewogen verteilten Datenmenge läge der Wert bei etwa 19%. So gesehen ist das Ergebnis ziemlich gut, zumindest im Vergleich mit einer rein zufälligen Klassifizierung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion, um Vorhersagen für zufällige Samples auszugeben und zu visualisieren\n",
    "def plot_random_predictions(model, test_data, num_samples=5):\n",
    "    model.eval()  # Setze das Modell in den Evaluierungsmodus\n",
    "    random_indices = np.random.choice(len(test_data), size=num_samples, replace=False)\n",
    "    samples = [test_data[i] for i in random_indices]\n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        input_data = torch.tensor(sample, dtype=torch.float32).unsqueeze(\n",
    "            0\n",
    "        )  # Batch-Dimension hinzufügen\n",
    "        with torch.no_grad():\n",
    "            predictions_batch = model(input_data)  # Vorhersage berechnen\n",
    "            predictions = (\n",
    "                predictions_batch.squeeze().numpy()\n",
    "            )  # Batch-Dimension entfernen\n",
    "\n",
    "        # Barplot der Softmax-Wahrscheinlichkeiten\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.bar(range(len(predictions)), predictions)\n",
    "        plt.title(f\"Sample {random_indices[i]}\")\n",
    "        plt.xlabel(\"Klassen\")\n",
    "        plt.grid(True)\n",
    "        plt.ylabel(\"Wahrscheinlichkeit\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "predictions = plot_random_predictions(model, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Das Element mit dem größten Wert gibt die vorhersagte Klasse an – die Klasse mit\n",
    "der höchsten Wahrscheinlichkeit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "yhat = np.argmax(predictions)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eine weitere Möglichkeit zur Handhabung der Klassenbezeichnungen und der Verlustfunktion\n",
    "\n",
    "Wie bereits kurz erwähnt, gibt es auch die Möglichkeit, die Klassenbezeichnungen\n",
    "wie folgt in einen Integertensor umzuwandeln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Das Einzige, was sich bei diesem Ansatz ändert, ist die Wahl der Verlustfunktion.\n",
    "- Die oben eingesetzte Verlustfunktion (`categorial_crossentropy`) erwartet, dass die Klassenbezeichnungen *kategorial codiert* sind. \n",
    "- Wenn die Klassenbezeichnungen Integer sind, sollten Sie `sparse_categorical_crossentropy`verwenden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematisch betrachtet, liefert diese Verlustfunktion das gleiche Ergebnis wie\n",
    "`categorial_crossentropy`, sie besitzt lediglich eine andere Schnittstelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hinreichend große zwischenliegende Layer sind wichtig\n",
    "\n",
    "Weil die endgültige Ausgabe, wie bereits kurz erwähnt, 46-dimensional ist, sollten Sie vermeiden, zwischenliegende Layer mit deutlich weniger als 46 Einheiten zu verwenden. Sehen wir uns doch einmal an, was geschieht, wenn es ein Informationsleck\n",
    "in Form eines zwischenliegenden Layers mit beträchtlich weniger als 46 Dimensionen gibt, beispielsweise eine vierdimensionale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ReutersLightningModel_tight(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A PyTorch Lightning implementation of the Keras Sequential model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReutersLightningModel_tight, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(10000, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 46),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        predictions = self(data)\n",
    "        loss = nn.CrossEntropyLoss()(predictions, labels.argmax(dim=1))\n",
    "        accuracy = (predictions.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(accuracy.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        predictions = self(data)\n",
    "        loss = nn.CrossEntropyLoss()(predictions, labels.argmax(dim=1))\n",
    "        accuracy = (predictions.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_accuracy\", accuracy, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_accuracies.append(accuracy.item())\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "model2 = ReutersLightningModel_tight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer setup\n",
    "max_epochs = 10\n",
    "trainer = pl.Trainer(max_epochs=10, log_every_n_steps=1)\n",
    "trainer.fit(model2, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Das NN erreicht bei der Validierung eine Korrektklassifizierungsrate von rund 71%, das ist eine Verschlechterung von 8%. Diese Abnahme ist hauptsächlich der Tatsache geschuldet, dass Sie hier versuchen, eine Menge Informationen (die ausreichen,\n",
    "um die trennenden Hyperebenen von 46 Klassen zu berechnen) in einen zwischenliegenden Raum einzupferchen, der nicht genügend Dimensionen besitzt. Das NN kann zwar die meisten erforderlichen Informationen in diese achtdimensionalen Repräsentationen hineinstopfen, aber eben nicht alle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisierung mit Dropout-Layern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersLightningModel_dropout(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A PyTorch Lightning ReutersLightningModel_dropout of the Keras Sequential model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReutersLightningModel_dropout, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(10000, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 46),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        predictions = self(data)\n",
    "        loss = nn.CrossEntropyLoss()(predictions, labels.argmax(dim=1))\n",
    "        accuracy = (predictions.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(accuracy.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        predictions = self(data)\n",
    "        loss = nn.CrossEntropyLoss()(predictions, labels.argmax(dim=1))\n",
    "        accuracy = (predictions.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_accuracy\", accuracy, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_accuracies.append(accuracy.item())\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "model3 = ReutersLightningModel_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer setup\n",
    "max_epochs = 10\n",
    "trainer = pl.Trainer(max_epochs=10, log_every_n_steps=1)\n",
    "trainer.fit(model3, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gewichtregulierung (Weight Regularization) und die vorgenommenen Änderungen\n",
    "\n",
    "Die Gewichtregulierung (Weight Regularization) ist eine Technik in neuronalen Netzwerken, die verwendet wird, um das Modell vor Überanpassung (Overfitting) zu schützen. Dabei wird ein Strafterm zu der Verlustfunktion hinzugefügt, der die Größe der Gewichte begrenzt. Ziel ist es, die Komplexität des Modells zu kontrollieren und die Generalisierungsfähigkeit auf unbekannte Daten zu verbessern.\n",
    "\n",
    "Die **L2-Regularisierung**, auch als **Ridge Regularization** bekannt, minimiert die Summe der quadrierten Werte der Gewichte. Der Strafterm wird wie folgt definiert:\n",
    "\n",
    "$$\n",
    "R_{\\text{L2}} = \\lambda \\sum_{i} w_i^2\n",
    "$$\n",
    "\n",
    "Hierbei ist:\n",
    "- $ w_i $: Das Gewicht eines Parameters im Modell\n",
    "- $ \\lambda $: Der Regularisierungsparameter, der das Gewicht des Strafterms bestimmt.\n",
    "\n",
    "Die Gesamtverlustfunktion wird durch die L2-Regularisierung wie folgt erweitert:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{gesamt}} = \\mathcal{L}_{\\text{original}} + \\lambda \\sum_{i} w_i^2\n",
    "$$\n",
    "\n",
    "Dabei ist $ \\mathcal{L}_{\\text{original}} $ die ursprüngliche Verlustfunktion. Bei Klassifikationsproblemen mit Kreuzentropieverlust sieht die erweiterte Verlustfunktion folgendermaßen aus:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{gesamt}} = -\\frac{1}{N} \\sum_{j=1}^{N} \\sum_{k=1}^{C} y_{j,k} \\log(\\hat{y}_{j,k}) + \\lambda \\sum_{i} w_i^2\n",
    "$$\n",
    "\n",
    "Hierbei ist:\n",
    "- $ N $: Die Anzahl der Beispiele im Batch.\n",
    "- $ C $: Die Anzahl der Klassen.\n",
    "- $ y_{j,k} $: Das echte Label (one-hot codiert).\n",
    "- $ \\hat{y}_{j,k} $: Die vorhergesagte Wahrscheinlichkeit.\n",
    "- $ \\lambda $: Der Regularisierungsparameter.\n",
    "- $ w_i $: Die Gewichte des Modells.\n",
    "\n",
    "In der Methode `training_step` wurde ein L2-Strafterm hinzugefügt. Der Term berechnet die Summe der L2-Normen aller trainierbaren Parameter des Modells. Diese Änderung wurde wie folgt implementiert:\n",
    "\n",
    "```python\n",
    "# L2-Regularisierung (Gewichtstrafe)\n",
    "l2_lambda = 0.01\n",
    "l2_reg = sum(torch.norm(param, 2) for param in self.model.parameters() if param.requires_grad)\n",
    "loss = nn.CrossEntropyLoss()(predictions, labels.argmax(dim=1)) + l2_lambda * l2_reg\n",
    "```\n",
    "\n",
    "### Bedeutung der Parameter:\n",
    "- `l2_lambda = 0.01`: Dieser Parameter bestimmt die Stärke der Regularisierung. Ein höherer Wert führt zu einer stärkeren Bestrafung großer Gewichte.\n",
    "- `torch.norm(param, 2)`: Berechnet die L2-Norm (euklidische Norm) eines Parameters $ param $.\n",
    "- `sum(...)`: Summiert die L2-Normen aller trainierbaren Parameter.\n",
    "\n",
    "Die Regularisierung wird zur Kreuzentropieverlustfunktion hinzugefügt, um die Gesamtverluste zu berechnen. Durch die Gewichtregulierung werden die Gewichte des Modells klein gehalten, was folgende Vorteile mit sich bringt:\n",
    "- **Verbesserte Generalisierung**: Das Modell wird weniger anfällig für Überanpassung.\n",
    "- **Numerische Stabilität**: Kleinere Gewichte führen zu stabileren Berechnungen.\n",
    "\n",
    "Die eingeführte L2-Gewichtregulierung ist eine effektive Methode, um die Leistung des Modells auf Testdaten zu verbessern. Sie sorgt dafür, dass die Gewichte klein bleiben und die Generalisierungsfähigkeit erhöht wird, ohne die Modellarchitektur zu verkomplizieren.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersLightningModel_l2(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A PyTorch Lightning implementation of the Keras Sequential model with dropout and weight regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReutersLightningModel_l2, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(10000, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 46),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        predictions = self(data)\n",
    "        # Apply L2 regularization (weight decay)\n",
    "        l2_lambda = 0.01\n",
    "        l2_reg = sum(\n",
    "            torch.norm(param, 2)\n",
    "            for param in self.model.parameters()\n",
    "            if param.requires_grad\n",
    "        )\n",
    "        loss = (\n",
    "            nn.CrossEntropyLoss()(predictions, labels.argmax(dim=1))\n",
    "            + l2_lambda * l2_reg\n",
    "        )\n",
    "\n",
    "        accuracy = (predictions.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(accuracy.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        predictions = self(data)\n",
    "        loss = nn.CrossEntropyLoss()(predictions, labels.argmax(dim=1))\n",
    "        accuracy = (predictions.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_accuracy\", accuracy, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(accuracy.item())\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.trainer.callback_metrics\n",
    "        avg_loss = outputs.get(\"val_loss_epoch\", torch.tensor(0.0)).item()\n",
    "        avg_accuracy = outputs.get(\"val_accuracy_epoch\", torch.tensor(0.0)).item()\n",
    "        self.val_losses.append(avg_loss)\n",
    "        self.val_accuracies.append(avg_accuracy)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "model4 = ReutersLightningModel_l2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer setup\n",
    "trainer = pl.Trainer(max_epochs=10, log_every_n_steps=1)\n",
    "trainer.fit(model4, train_loader, validation_loader)\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curves(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weitere Experimente (Hausaufgaben)\n",
    "\n",
    "- Probieren Sie größere oder kleinere Layer aus: 32 Einheiten, 128 Einheiten usw.\n",
    "- Sie haben bislang zwei verdeckte Layer verwendet. Probieren Sie nun aus, nur einen einzigen oder drei verdeckte Layer zu verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Nehmen Sie Folgendes aus diesem Abschnitt mit:\n",
    "- Falls Sie versuchen, Datenpunkte N Kategorien zuzuordnen, sollte Ihr NN mit einem Dense-Layer der Größe N enden.\n",
    "- Bei einer Single-Label-Mehrfachklassifizierungsaufgabe sollte Ihr NN mit einer `softmax`-Aktivierung enden, damit es eine Wahrscheinlichkeitsverteilung der N Klassen ausgibt.\n",
    "- Bei Aufgaben dieser Art sollten Sie als Verlustfunktion fast immer die kategoriale Kreuzentropie (`categorical_crossentropy`) verwenden. Sie minimiert die Differenz zwischen den Wahrscheinlichkeitsverteilungen der Ausgabe des NNs und der tatsächlichen Verteilung der Zielwerte.\n",
    "- Zur Handhabung der Klassenbezeichnungen einer Mehrfachklassifizierung stehen zwei Möglichkeiten zur Verfügung:\n",
    "    - die **kategoriale Codierung** (die auch als **One-hot-Codierung** bezeichnet wird) mit der Verwendung der Verlustfunktion `categorial_crossentropy` \n",
    "    - und die die **Codierung der Klassenbezeichnungen als Integer** und die Verwendung der Verlustfunktion `sparse_categorial_crossentropy`\n",
    "- Wenn Sie die Daten sehr vielen Kategorien zuordnen müssen, sollten Sie Informationslecks durch zu kleine zwischenliegende Layer im NN vermeiden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "ger",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ger",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
