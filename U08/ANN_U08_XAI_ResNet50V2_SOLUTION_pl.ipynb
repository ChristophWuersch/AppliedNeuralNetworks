{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U08/ANN_08_XAI_ResNet50V2_SOLUTION_pl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Explainable AI: Visualizing what convnets learn\n",
    "\n",
    "\n",
    "Oft heisst es, Deep-Learning-Modelle seien »Blackboxes«, die Repräsentationen in einer Form erlernen, die schwer zu extrahieren und für Menschen kaum verständlich sind. \n",
    "- Für einige Arten von Deep-Learning-Modellen mag das teilweise stimmen, auf CNNs trifft es jedoch definitiv nicht zu. \n",
    "- Die von CNNs erlernten Repräsentationen sind in hohem Mass für Visualisierungen geeignet, und zwar vor allem deshalb, weil es sich um Repräsentationen visueller Konzepte handelt. Seit 2013 wurde eine Vielzahl von Verfahren zur Visualisierung und Interpretation dieser Repräsentationen entwickelt. Wir werden nicht alle diese Verfahren betrachten, aber die drei verständlichsten und nützlichsten erörtern:\n",
    "\n",
    "Abgesehen von der Befürchtung, dass böse künstliche Intelligenzen die Welt übernehmen werden, kann das Gebiet der künstlichen Intelligenz für Außenstehende entmutigend sein. Facebooks Direktor für künstliche Intelligenz, **Yann LeCun**, verwendet die Analogie, dass KI eine Blackbox mit einer Million Knöpfen ist; das Innenleben ist den meisten ein Rätsel. Aber jetzt haben wir einen Blick hinein geworfen.\n",
    "\n",
    "**Adam Harley**, Masterstudent an der Ryerson University, hat eine interaktive Visualisierung erstellt, die erklärt, wie ein neuronales Faltungsnetz, eine Art Programm für künstliche Intelligenz, das zur Analyse von Bildern verwendet wird, intern funktioniert.\n",
    "\n",
    "[Visualization of a CNN](http://www.cs.cmu.edu/~aharley/nn_vis/cnn/3d.html)\n",
    "\n",
    "Visualisierung der zwischenliegenden Ausgaben (zwischenliegende Aktivierungen) – Dieses Verfahren ermöglicht es, zu verstehen, wie aufeinanderfolgende Layer ihre Eingaben transformieren, und vermittelt eine Vorstellung von der Bedeutung\n",
    "der einzelnen CNN-Filter. \n",
    "\n",
    "* **Visualisierung der CNN-Filter** – Dieses Verfahren ermöglicht es, genau zu verstehen, für welche visuellen Muster oder Konzepte die CNN-Filter empfänglich sind.\n",
    "* **Visualisierung der Heatmaps der Klassenaktivierung als Bild** – Dieses Verfahren ermöglicht es, zu verstehen, welche Teile eines Bilds als zu einer bestimmten Klasse zugehörig erkannt wurden. Auf diese Weise können die Positionen von Objekten in Bildern ermittelt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Einführung\n",
    "\n",
    "In diesem Beispiel untersuchen wir, welche Art von visuellen Mustern Bildklassifikationsmodelle\n",
    "lernen. Wir werden das Modell \"ResNet50V2\" verwenden, das auf dem ImageNet-Datensatz trainiert wurde.\n",
    "\n",
    "Unser Verfahren ist einfach: Wir werden Eingabebilder erstellen, die die Aktivierung bestimmter Filter in einer\n",
    "bestimmten Filtern in einer Zielschicht (die irgendwo in der Mitte des Modells ausgewählt wird: Schicht\n",
    "`conv3_block4_out`). Solche Bilder stellen eine Visualisierung des\n",
    "Muster, auf das der Filter reagiert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup\n",
    "\n",
    "- Ziel des Codes: Visualisierung der Filter einer bestimmten Schicht des vortrainierten ResNet50-Modells.\n",
    "Hauptschritte:\n",
    "    - Laden des ResNet50-Modells mit vortrainierten Gewichten.\n",
    "    - Ausgabe der Modellstruktur zur Identifikation der Zielschicht.\n",
    "    - Definition einer Lightning-Klasse zur Kapselung des Modells.\n",
    "    - Extraktion und Visualisierung der Filter der Zielschicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from PIL import Image as PILImage\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Laden des ResNet50-Modells mit vortrainierten ImageNet-Gewichten\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Ausgabe der Namen aller Schichten im Modell\n",
    "print(\"Layer names in the model:\")\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n",
    "\n",
    "# Die Dimensionen des Eingabebildes\n",
    "img_width = 180\n",
    "img_height = 180\n",
    "\n",
    "# Zielschicht: Wir werden die Filter aus dieser Schicht visualisieren.\n",
    "# Sie können den Namen der Schicht ändern, indem Sie die Ausgabe von `model.named_modules()` überprüfen.\n",
    "layer_name = \"layer4.2.conv3\"\n",
    "\n",
    "\n",
    "# Definition einer Lightning-Modul-Klasse für ResNet50\n",
    "class ResNet50V2Lightning(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(ResNet50V2Lightning, self).__init__()\n",
    "        # Laden des vortrainierten ResNet50-Modells\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        # Setzen des Modells in den Evaluierungsmodus\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Vorwärtsdurchlauf durch das Modell\n",
    "        return self.model(x)\n",
    "\n",
    "    def visualize_filters(self):\n",
    "        # Funktion zur Visualisierung der Filter aus der Zielschicht\n",
    "        # Abrufen der Zielschicht aus dem Modell\n",
    "        target_layer = dict([*self.model.named_modules()])[layer_name]\n",
    "        # Abrufen der Gewichtsdaten der Zielschicht\n",
    "        filters = target_layer.weight.data.cpu().numpy()\n",
    "        return filters\n",
    "\n",
    "\n",
    "# Beispielverwendung\n",
    "# Instanziieren des Lightning-Modells\n",
    "model = ResNet50V2Lightning()\n",
    "\n",
    "# Visualisieren der Filter aus der Zielschicht\n",
    "filters = model.visualize_filters()\n",
    "\n",
    "# Ausgabe der Form der Filter (z. B. Anzahl der Filter, Dimensionen)\n",
    "print(filters.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (a) Erstellen eines Modells zur Merkmalsextraktion\n",
    "\n",
    "- Ziel: Anpassung eines vortrainierten ResNet50-Modells für Trainingszwecke.\n",
    "Hauptschritte:\n",
    "    - Laden des vortrainierten ResNet50-Modells mit ImageNet-Gewichten.\n",
    "    - Entfernen der letzten Schicht (fc), um das Modell als Feature-Extractor zu verwenden.\n",
    "    - Definition eines Trainingsschritts mit Cross-Entropy-Verlust.\n",
    "    - Konfiguration des Adam-Optimierers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition einer Lightning-Modul-Klasse für ResNet50\n",
    "class ResNet50V2Lightning(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(ResNet50V2Lightning, self).__init__()\n",
    "        # Laden des vortrainierten ResNet50-Modells mit ImageNet-Gewichten\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        # Entfernen der voll verbundenen Schicht (fully connected layer), um das Modell als Feature-Extractor zu verwenden\n",
    "        self.model.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Vorwärtsdurchlauf durch das Modell\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Trainingsschritt: Berechnung des Verlusts (Cross-Entropy) zwischen den Vorhersagen und den Labels\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Konfiguration des Optimierers (Adam-Optimierer mit Lernrate 0.001)\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Instanziieren des Lightning-Modells\n",
    "model = ResNet50V2Lightning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Überprüfung der GPU-Verfügbarkeit: Es wird geprüft, ob eine GPU verfügbar ist. Falls ja, wird das Gerät auf cuda gesetzt, andernfalls auf cpu.\n",
    "\n",
    "Modell auf das Gerät verschieben: Das Modell wird auf das ausgewählte Gerät (GPU oder CPU) verschoben, damit die Berechnungen entsprechend dort ausgeführt werden können.\n",
    "\n",
    "Modellzusammenfassung ausgeben: Mit der Funktion summary wird eine Übersicht des Modells erstellt. Diese zeigt die Architektur des Modells, die Anzahl der Parameter und die Dimensionen der Zwischenausgaben. Die Eingabegrösse wird dabei als (3, 224, 224) angegeben, was einem Bild mit 3 Farbkanälen (RGB) und einer Auflösung von 224x224 Pixeln entspricht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfen, ob eine GPU verfügbar ist, und das Gerät entsprechend festlegen\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Verschieben des Modells auf das ausgewählte Gerät (GPU oder CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Ausgabe einer Zusammenfassung des Modells\n",
    "# Die Funktion `summary` zeigt die Architektur des Modells, die Anzahl der Parameter und die Dimensionen der Zwischenausgaben\n",
    "summary(model, (3, 224, 224))  # Eingabegrösse: 3 Kanäle (RGB), 224x224 Pixel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Architektur des Modells\n",
    "# Dies zeigt die Struktur des ResNet50-Modells, einschliesslich aller Schichten und ihrer Parameter.\n",
    "print(model.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie hier zu sehen ist gibt es vier Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Erstellen eines Modells zur Merkmalsextraktion\n",
    "\n",
    "- Das Modell basiert auf ResNet50 und wurde so modifiziert, dass es als Feature-Extractor verwendet werden kann.\n",
    "- Es ermöglicht die Extraktion der Aktivierungen einer bestimmten Zielschicht (z. B. \"layer1\").\n",
    "- Der Code zeigt, wie das Modell mit einer Dummy-Eingabe verwendet werden kann, um die Form der Aktivierungen zu überprüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50V2Lightning(L.LightningModule):\n",
    "    def __init__(self, target_layer):\n",
    "        super(ResNet50V2Lightning, self).__init__()\n",
    "        # Laden des vortrainierten ResNet50-Modells mit ImageNet-Gewichten\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        # Entfernen der voll verbundenen Schicht (fully connected layer), um das Modell als Feature-Extractor zu verwenden\n",
    "        self.model.fc = nn.Identity()\n",
    "        # Speichern des Namens der Zielschicht, deren Aktivierungen extrahiert werden sollen\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Vorwärtsdurchlauf durch das Modell\n",
    "        for name, layer in self.model.named_children():\n",
    "            # Anwenden der aktuellen Schicht auf die Eingabe\n",
    "            x = layer(x)\n",
    "            # Überprüfen, ob die aktuelle Schicht die Zielschicht ist\n",
    "            if name == self.target_layer:\n",
    "                # Rückgabe der Aktivierungen der Zielschicht\n",
    "                return x\n",
    "        # Rückgabe der endgültigen Ausgabe, falls die Zielschicht nicht gefunden wurde\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Trainingsschritt: Berechnung des Verlusts (Cross-Entropy) zwischen den Vorhersagen und den Labels\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Konfiguration des Optimierers (Adam-Optimierer mit Lernrate 0.001)\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Definieren des Namens der Zielschicht\n",
    "target_layer = \"layer1\"  # Beispiel: Zielschicht ist \"layer1\"\n",
    "# target_layer = \"layer2\"  # Alternativ: Zielschicht ist \"layer2\"\n",
    "# target_layer = \"layer3\"  # Alternativ: Zielschicht ist \"layer3\"\n",
    "# target_layer = \"layer4\"  # Alternativ: Zielschicht ist \"layer4\"\n",
    "\n",
    "# Instanziieren des Modells mit der Zielschicht\n",
    "model = ResNet50V2Lightning(target_layer=target_layer)\n",
    "\n",
    "# Beispielverwendung mit einer Dummy-Eingabe\n",
    "dummy_input = torch.randn(\n",
    "    1, 3, 224, 224\n",
    ")  # Dummy-Bild mit der Grösse 224x224 und 3 Kanälen (RGB)\n",
    "output = model(dummy_input)  # Abrufen der Aktivierungen der Zielschicht\n",
    "print(output.shape)  # Ausgabe der Form der Aktivierungen\n",
    "\n",
    "# Aktivierungen in ein numpy-Array konvertieren\n",
    "activations = output.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# Plotten der Aktivierungen\n",
    "# Wir nehmen an, dass die Aktivierungen die Form (Batch, Channels, Height, Width) haben\n",
    "num_filters = activations.shape[1]  # Anzahl der Filter (Kanäle)\n",
    "fig, axes = plt.subplots(\n",
    "    1, min(num_filters, 8), figsize=(20, 5)\n",
    ")  # Zeige bis zu 8 Filter\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i >= num_filters:\n",
    "        break\n",
    "    # Zeige den i-ten Filter (wir nehmen die erste Batch-Dimension)\n",
    "    ax.imshow(activations[0, i, :, :], cmap=\"viridis\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Filter {i}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (c) Einrichten des Gradientenanstiegsprozesses\n",
    "\n",
    "- Die Funktion ``compute_loss`` berechnet den Mittelwert der Aktivierung eines bestimmten Filters in einer Zielschicht des Modells.\n",
    "- Sie ignoriert Randpixel, um Artefakte zu vermeiden.\n",
    "- Der Verlustwert gibt an, wie stark der ausgewählte Filter auf die Eingabe reagiert.\n",
    "- In der Beispielverwendung wird ein Dummy-Bild verwendet, um den Verlust für den Filter mit Index 0 zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(input_image, filter_index, model):\n",
    "    # Verschieben des Eingabebildes auf dasselbe Gerät wie das Modell\n",
    "    input_image = input_image.to(next(model.parameters()).device)\n",
    "\n",
    "    # Abrufen der Aktivierung aus dem Modell\n",
    "    activation = model(input_image)\n",
    "\n",
    "    # Vermeidung von Randartefakten, indem nur Nicht-Randpixel in den Verlust einbezogen werden\n",
    "    filter_activation = activation[:, filter_index, 2:-2, 2:-2]\n",
    "\n",
    "    # Berechnung des Mittelwerts der Aktivierung als Verlust\n",
    "    return torch.mean(filter_activation)\n",
    "\n",
    "\n",
    "# Beispielverwendung mit einer Dummy-Eingabe und einem Filterindex\n",
    "dummy_input = torch.randn(\n",
    "    1, 3, 224, 224\n",
    ")  # Dummy-Eingabebild mit der Grösse 224x224 und 3 Kanälen (RGB)\n",
    "filter_index = 0  # Beispiel-Filterindex\n",
    "loss = compute_loss(dummy_input, filter_index, model)  # Berechnung des Verlusts\n",
    "print(loss)  # Ausgabe des Verlusts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Die Funktion ``gradient_ascent_step`` optimiert ein Eingabebild, um die Aktivierung eines bestimmten Filters in einer Zielschicht zu maximieren.\n",
    "Dies wird durch Gradientenberechnung und Aktualisierung des Bildes erreicht.\n",
    "In der Beispielverwendung wird ein zufälliges Dummy-Bild optimiert, um die Aktivierung des Filters mit Index 0 zu maximieren.\n",
    "Solche Techniken werden oft verwendet, um zu visualisieren, welche Muster ein neuronales Netzwerk gelernt hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent_step(img, filter_index, learning_rate, model):\n",
    "    # Setzen des Eingabebildes auf \"requires_grad\", um Gradienten berechnen zu können\n",
    "    img.requires_grad = True\n",
    "    # Initialisieren des Optimierers (SGD) mit dem Eingabebild und der Lernrate\n",
    "    optimizer = torch.optim.SGD([img], lr=learning_rate)\n",
    "\n",
    "    # Zurücksetzen der Gradienten des Optimierers\n",
    "    optimizer.zero_grad()\n",
    "    # Berechnung des Verlusts für den angegebenen Filterindex\n",
    "    loss = compute_loss(img, filter_index, model)\n",
    "    # Rückwärtsdurchlauf zur Berechnung der Gradienten\n",
    "    loss.backward()\n",
    "\n",
    "    # Normalisieren der Gradienten, um stabile Updates zu gewährleisten\n",
    "    grads = img.grad / (torch.sqrt(torch.mean(img.grad**2)) + 1e-5)\n",
    "\n",
    "    # Aktualisieren des Bildes basierend auf den Gradienten und der Lernrate\n",
    "    img.data += learning_rate * grads.data\n",
    "\n",
    "    # Rückgabe des Verlusts und des aktualisierten Bildes\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "# Beispielverwendung mit einer Dummy-Eingabe und einem Filterindex\n",
    "dummy_input = torch.randn(1, 3, 224, 224, requires_grad=True)  # Dummy-Eingabebild\n",
    "filter_index = 0  # Beispiel-Filterindex\n",
    "learning_rate = 0.1  # Beispiel-Lernrate\n",
    "\n",
    "# Ausführen eines Gradientenanstiegsschritts\n",
    "loss, updated_img = gradient_ascent_step(\n",
    "    dummy_input, filter_index, learning_rate, model\n",
    ")\n",
    "print(loss)  # Ausgabe des Verlusts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (d) Einrichten der End-to-End-Filtervisualisierungsschleife\n",
    "\n",
    "- Der Code visualisiert, welche Eingaben einen bestimmten Filter in einem neuronalen Netzwerk (z. B. ResNet50) aktivieren.\n",
    "- Ein zufälliges Bild wird iterativ optimiert, um die Aktivierung eines bestimmten Filters zu maximieren.\n",
    "- Nach der Optimierung wird das Bild dekodiert und kann visualisiert werden, um zu verstehen, welche Muster der Filter gelernt hat.\n",
    "- Dies ist eine Technik der Explainable AI (XAI), um die Funktionsweise von neuronalen Netzwerken besser zu verstehen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfen, ob eine GPU verfügbar ist\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def initialize_image(img_width, img_height):\n",
    "    # Wir starten mit einem grauen Bild mit etwas zufälligem Rauschen\n",
    "    img = torch.rand((1, 3, img_width, img_height), device=device)\n",
    "    # ResNet50V2 erwartet Eingaben im Bereich [-1, +1].\n",
    "    # Hier skalieren wir unsere zufälligen Eingaben auf den Bereich [-0.125, +0.125]\n",
    "    return (img - 0.5) * 0.25\n",
    "\n",
    "\n",
    "def deprocess_image(img):\n",
    "    # Normalisieren des Arrays: Zentrieren auf 0 und Sicherstellen einer Varianz von 0.15\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    # Zuschneiden des Bildes (Center Crop)\n",
    "    img = img[:, 25:-25, 25:-25]\n",
    "\n",
    "    # Begrenzen auf den Bereich [0, 1]\n",
    "    img += 0.5\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "\n",
    "    # Konvertieren in ein RGB-Array\n",
    "    img *= 255\n",
    "    img = torch.clamp(img, 0, 255).byte().cpu().numpy().transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_filter(filter_index, model, img_width=224, img_height=224):\n",
    "    print(\"Visualisiere Filterindex:\", filter_index)\n",
    "    # Wir führen den Gradientenanstieg für 30 Schritte aus\n",
    "    iterations = 30\n",
    "    learning_rate = 10.0\n",
    "    img = initialize_image(img_width, img_height)\n",
    "    for iteration in range(iterations):\n",
    "        # Gradientenanstiegsschritt ausführen\n",
    "        loss, img = gradient_ascent_step(img, filter_index, learning_rate, model)\n",
    "\n",
    "    # Dekodieren des resultierenden Eingabebildes\n",
    "    img = deprocess_image(img.detach().cpu()[0])\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "# Verschieben des Modells auf die GPU, falls verfügbar\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Probieren wir es mit Filter 0 in der Zielebene aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(img, path):\n",
    "    # Konvertiert den Tensor in ein PIL-Bild und speichert es\n",
    "    pil_img = PILImage.fromarray(img)\n",
    "    pil_img.save(path)\n",
    "\n",
    "\n",
    "# Beispielverwendung\n",
    "# Visualisiert den Filter mit Index 0 und speichert das resultierende Bild\n",
    "loss, img = visualize_filter(0, model)\n",
    "save_img(img, \"0.png\")  # Speichert das Bild unter dem Namen \"0.png\"\n",
    "display(Image(filename=\"0.png\"))  # Zeigt das gespeicherte Bild an\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (e) Visualisierung der ersten 64 Filter in der Zielebene\n",
    "\n",
    "Erstellen wir nun ein 8x8-Gitter mit den ersten 64 Filtern\n",
    "in der Zielebene, um ein Gefühl für die Bandbreite\n",
    "der verschiedenen visuellen Muster zu bekommen, die das Modell gelernt hat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(img, path):\n",
    "    # Konvertiert den Tensor in ein PIL-Bild und speichert es\n",
    "    pil_img = PILImage.fromarray(img)\n",
    "    pil_img.save(path)\n",
    "\n",
    "\n",
    "# Berechnung der Eingabebilder, die die Aktivierungen der ersten 64 Filter\n",
    "# in unserer Zielschicht maximieren\n",
    "all_imgs = []\n",
    "for filter_index in range(64):\n",
    "    # Visualisierung des Filters mit dem aktuellen Index\n",
    "    # und Hinzufügen des resultierenden Bildes zur Liste\n",
    "    loss, img = visualize_filter(filter_index, model)\n",
    "    all_imgs.append(img)\n",
    "\n",
    "# Erstellen eines schwarzen Bildes mit genügend Platz für\n",
    "# unsere 8 x 8 Filter mit einer Grösse von 128 x 128 Pixeln und einem Rand von 5 Pixeln dazwischen\n",
    "margin = 5  # Rand zwischen den Bildern\n",
    "n = 8  # Anzahl der Bilder pro Zeile und Spalte\n",
    "cropped_width = 224 - 25 * 2  # Breite des zugeschnittenen Bildes\n",
    "cropped_height = 224 - 25 * 2  # Höhe des zugeschnittenen Bildes\n",
    "width = (\n",
    "    n * cropped_width + (n - 1) * margin\n",
    ")  # Gesamtbreite des zusammengesetzten Bildes\n",
    "height = (\n",
    "    n * cropped_height + (n - 1) * margin\n",
    ")  # Gesamthöhe des zusammengesetzten Bildes\n",
    "stitched_filters = np.zeros(\n",
    "    (height, width, 3), dtype=np.uint8\n",
    ")  # Initialisierung des schwarzen Bildes\n",
    "\n",
    "# Füllen des zusammengesetzten Bildes mit unseren gespeicherten Filtern\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img = all_imgs[i * n + j]  # Abrufen des Bildes für den aktuellen Filter\n",
    "        stitched_filters[\n",
    "            (cropped_height + margin) * i : (cropped_height + margin) * i\n",
    "            + cropped_height,\n",
    "            (cropped_width + margin) * j : (cropped_width + margin) * j + cropped_width,\n",
    "            :,\n",
    "        ] = img  # Platzieren des Bildes im zusammengesetzten Bild\n",
    "\n",
    "# Speichern und Anzeigen des zusammengesetzten Bildes mit den Filtern\n",
    "save_img(stitched_filters, \"stitched_filters.png\")  # Speichern des Bildes\n",
    "display(Image(filename=\"stitched_filters.png\"))  # Anzeigen des gespeicherten Bildes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Zusammenfassung\n",
    "- Das Notebook zeigt, wie CNN-Filter visualisiert werden können, um die erlernten Muster eines Modells zu interpretieren.\n",
    "- Es kombiniert Techniken wie Gradientenberechnung, Bildoptimierung und Visualisierung.\n",
    "- Die Ergebnisse helfen, die Funktionsweise von CNNs besser zu verstehen und sind ein Beispiel für Explainable AI (XAI)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "visualizing_what_convnets_learn",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
