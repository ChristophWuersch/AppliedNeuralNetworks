{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN12/12-Bilder_mit_VAEs_generieren_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bilderzeugung mit Variational Autoencodern (VAE)\n",
    "\n",
    "This notebook contains the second code sample found in Chapter 8, Section 4 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff), \n",
    "[LiveBook](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-12/171)\n",
    "\n",
    "\n",
    "- Das Sampling eines latenten Raums von Bildern, um völlig neue Bilder zu erzeugen oder vorhandene zu bearbeiten, gehört derzeit zu den verbreitetsten und erfolgreichsten Anwendungen kreativer KI. \n",
    "- In diesem und im folgenden Abschnitt werden wir einige allgemeine die Bilderzeugung betreffende Konzepte untersuchen sowie die Details der Implementierungen der beiden wichtigsten Verfahren auf diesem Gebiet betrachten: **Variational Autoencoders (VAEs)** und **Generative Adversarial Networks (GANs).** \n",
    "\n",
    "Die hier vorgestellten Verfahren sind keineswegs nur auf Bilder anwendbar – mithilfe von GANs oder VAEs ließen sich auch latente Räume von Klängen, Musikstücken oder sogar Texten entwickeln. Da in der Praxis allerdings Bilder die interessantesten Ergebnisse liefern, werden wir uns darauf konzentrieren.\n",
    "\n",
    "- [Kunst mit KI](https://towardsdatascience.com/next-level-art-and-the-future-of-work-and-leisure-f66049112e44)\n",
    "- [AI Art Gallery](http://www.aiartonline.com/)\n",
    "- [obvious-art](https://obvious-art.com/)\n",
    "- [Deep Fake](https://en.wikipedia.org/wiki/Deepfake)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling eines latenten Bilderraums\n",
    "\n",
    "- Der Bilderzeugung liegt die Idee zugrunde, einen niedrigdimensionalen latenten Raum zu verwenden (naturgemäss einen Vektorraum), in dem jedem Punkt ein fotorealistisches Bild zugeordnet werden kann. \n",
    "- Dieser **latente Raum** stellt eine **Mannigfaltigkeit** für die fotorealistischen Bilder dar, d.h. eine kompakte Repräsentation der Bilder, wobei ähnliche Bilder nahgelegene Punkte innerhalb dieses latenten Raumes einnehmen.\n",
    "\n",
    "\n",
    "Dem Modul, das diese Zuordnung (Einbettung) vornimmt, wird ein Punkt in diesem Vektorraum als Eingabe übergeben, und es gibt ein Bild (ein Pixelraster) aus. \n",
    "- Dieses Modul wird als **Generator** (wenn es sich um GANs handelt) bzw. als **Decodierer** (bei VAEs) bezeichnet. \n",
    "- Nach der Einrichtung eines solchen latenten Raums können entweder gezielt oder zufällig Punkte **gesampelt** werden, denen neu erzeugte Bilder zugeordnet sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GAN und VAE\n",
    "\n",
    "- **GANs** und **VAEs** sind zwei verschiedene Strategien zum Erlernen solcher latenten Bilderräume, die unterschiedliche Eigenschaften besitzen. \n",
    "- VAEs sind besonders gut für das Erlernen klar strukturierter Räume geeignet, in denen bestimmte Richtungen Achsen darstellen, die für die Variationen der Daten von Bedeutung sind (siehe Abbildung 8.10). \n",
    "- GANs können ebenfalls sehr realistisch wirkende Bilder erzeugen, aber der latente Raum, dem sie entstammen, besitzt weniger Struktur und Kontinuität."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [MIT Introduction to Deep Learning](http://introtodeeplearning.com/2020/slides/6S191_MIT_DeepLearning_L4.pdf)\n",
    "- http://introtodeeplearning.com/2020/index.html\n",
    "- [Diederik P. Kingma, M. Welling: An Introduction to Variational Autoencoders](https://arxiv.org/pdf/1906.02691.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "id = \"rZufA635dq4\"\n",
    "YouTubeVideo(id=id, width=640)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"XOxxPcy5Gr4\"\n",
    "YouTubeVideo(id=id, width=640)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Konzeptvektoren für das Bearbeiten von Bildern\n",
    "\n",
    "Im Zusammenhang mit den erörterten Worteinbettungen (word embeddigs) gab es bereits einen Hinweis auf Konzeptvektoren.\n",
    "Die zugrunde liegende Idee ist die gleiche:\n",
    "\n",
    "- Bestimmte Richtungen im latenten Repräsentationsraum oder im Einbettungsraum stellen womöglich Achsen interessanter Variationen der ursprünglichen Daten dar. \n",
    "- In einem latenten »Gesichter«-Raum könnte es beispielsweise einen »Lächeln«-Vektor geben, für den Folgendes gilt: Wenn der Punkt $z$ die eingebettete Repräsentation eines bestimmten Gesichts darstellt, dann verweist der Punkt $z + l$ auf die Repräsentation desselben Gesichts, das lächelt. \n",
    "Wenn ein solcher Vektor gefunden wird, lassen sich Bilder bearbeiten, indem man sie \n",
    "1. in den latenten Raum **projiziert**, \n",
    "2. ihre Repräsentation auf sinnvolle Weise **verschiebt** und\n",
    "3. die Repräsentation wieder **decodiert**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Im Bilderraum gibt es solche *Konzeptvektoren* für praktisch alle unabhängigen Dimensionen von Variationen. Für Gesichter sind beispielsweise Konzeptvektoren \n",
    "- zum Hinzufügen einer Sonnenbrille,\n",
    "- zum Entfernen einer Brille oder\n",
    "- zum Umwandeln eines männlichen Gesichts in ein weibliches denkbar.\n",
    "\n",
    "Die folgende Abbildung zeigt ein Beispiel für einen »Lächeln«-Vektor, den Tom White von der Victoria\n",
    "University School of Design in Neuseeland entdeckt hat. Zu diesem Zweck wurden VAEs mit einer aus den Gesichtern von Prominenten bestehenden Datenmenge (der sogenannten CelebA-Datenmenge) trainiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/laughing.jpg\" width=\"640\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational Autoencoders\n",
    "\n",
    "\n",
    "Variational Autoencoders wurden praktisch zeitgleich von [Kingma und Welling](https://arxiv.org/abs/1312.6114) im\n",
    "Dezember 2013 und von [Rezende, Mohamed und Wierstra](https://arxiv.org/abs/1401.4082) im Januar 2014 entdeckt.\n",
    "- Dabei handelt es sich um ein **generatives Modell**, das besonders gut für die Aufgabe geeignet ist, Bilder mit Konzeptvektoren zu bearbeiten. \n",
    "- Das Modell ist sozusagen eine moderne Version des Autoencoders (ein Typ neuronaler Netze, die eine Eingabe in einen niedrigdimensionalen latenten Raum projizieren und wieder decodieren), die auf Konzepte des **Deep Learnings und Bayes‘sche Inferenz** zurückgreift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Ein klassischer Autoencoder projiziert ein Bild mit einem **Codierer** in einen **latenten Vektorraum** und wandelt es per **Decodierer** wieder in eine Ausgabe mit denselben Dimensionen wie die des ursprünglichen Bilds um (siehe Abbildung).\n",
    "- Beim anschliessenden Training werden die Eingabebilder auch als Zielwerte verwendet, der Autoencoder erlernt also, die ursprünglichen Eingaben zu rekonstruieren.\n",
    "- Durch verschiedene Einschränkungen der Codierung (der Ausgabe des Codierers) kann der Autoencoder dazu gebracht werden, mehr oder weniger interessante latente Repräsentationen der Daten zu erlernen. \n",
    "- Am gebräuchlichsten ist eine **dünnbesetzte (hauptsächlich Nullen) auf wenige Dimensionen beschränkte Codierung.** In diesem Fall bietet der Codierer die Möglichkeit, die **Daten zu komprimieren**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\n",
    "    \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/ANN12/Bilder/autoencoder.jpg\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In der Praxis liefern **klassische Autoencoder** allerdings weder besonders nützliche noch vernünftig strukturierte latente Räume. Zur Komprimierung sind sie auch kaum zu gebrauchen. Aus diesen Gründen sind sie weitgehend aus der Mode gekommen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\n",
    "    \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/ANN12/Bilder/classical_autoencoder.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **VAEs** hingegen bereichern Autoencoder um ein wenig statistische Magie, die sie zum **Erlernen kontinuierlicher und hochgradig strukturierter latenter Räume zwingt**. \n",
    "- Sie haben sich als leistungsfähiges Werkzeug zur **Bilderzeugung** erwiesen.\n",
    "- Anstatt ein Eingabebild als komprimierten festgelegten Code im latenten Raum abzulegen, wandelt ein VAE das Bild in die **Parameter einer statistischen Verteilung** um, nämlich in Mittelwert und Varianz. \n",
    "- Wir gehen also von der Annahme aus, dass das Eingabebild durch einen statistischen Vorgang erzeugt wurde, dessen Zufälligkeit der Codierer und der Decodierer berücksichtigen müssen. \n",
    "- Der VAE verwendet die **Parameter Mittelwert und Varianz**, um der Verteilung ein zufälliges Element zu entnehmen, und decodiert es, um die ursprüngliche Eingabe zu rekonstruieren (siehe Abbildung).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\n",
    "    \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/ANN12/Bilder/variational_autoencoder.jpg\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reparametrisierungs-Trick\n",
    "\n",
    "**Die Zufälligkeit dieses Vorgangs erhöht die Stabilität und erzwingt überall im latenten\n",
    "Raum bedeutungsvolle Repräsentationen:**\n",
    "\n",
    "Alle im latenten Raum gesampelten Punkte können decodiert und in gültige Ausgaben umgewandelt werden.\n",
    "\n",
    "1. Der *Codierer* wandelt die Eingabebilder `input_img` in die beiden Parameter des latenten Repräsentationsraums `z_mean` und `z_log_variance` um.\n",
    "2. Wir entnehmen der latenten Normalverteilung, von der wir annehmen, dass sie das Bild erzeugt hat, einen zufälligen Punkt $z$ durch die Zuweisung `z = z_mean + exp(z_log_variance) * epsilon`. `epsilon` ist ein Tensor, der zufällige kleine Werte enthält (reparametrization trick).\n",
    "3. Der *Decodierer* ordnet diesem Punkt im latenten Raum wieder das ursprüngliche Eingabebild zu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/vae.png\" width=\"640\"  align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stetigkeit und niedrige Dimensionalität des latenten Raumes\n",
    "\n",
    "- Da `epsilon` zufällige Werte enthält, ist durch diese Vorgehensweise gewährleistet, dass jeder Punkt in der Umgebung der Stelle des latenten Raums, an der `input_image (z_mean)` codiert ist, in ein Bild decodiert werden kann, das `input_img` ähnelt, und so erzwingt, dass der latente Raum kontinuierlich bedeutungsvoll bleibt.\n",
    "- Zwei im latenten Raum nahe beieinanderliegende Punkte werden nach der Decodierung sehr ähnliche Bilder ergeben. \n",
    "- Diese **Stetigkeit und die niedrige Dimensionalität** des latenten Raums erzwingen, dass alle Richtungen im latenten Raum Achsen darstellen, die für die Variationen der Daten von Bedeutung sind. \n",
    "- Das verleiht dem latenten Raum Struktur und macht ihn besonders gut für die Bearbeitung mit Konzeptvektoren geeignet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Verlustfunktion\n",
    "\n",
    "Die Parameter eines VAEs werden mit zwei Verlustfunktionen trainiert:\n",
    "1. einem **Rekonstruktionsverlust**, der erzwingt, dass die decodierten Samples den ursprünglichen Eingaben entsprechen, sowie\n",
    "2. einem **Regularisierungsverlust**, der es erleichtert, wohlgeformte latente Räume zu erlernen, und die Überanpassung an die Trainingsdaten verringert. \n",
    "\n",
    "Betrachten wir also die Keras-Implementierung eines VAEs. Schematisch sieht sie folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. `z_mean, z_log_variance = encoder(input_img)`\n",
    "2. `z = z_mean + exp(z_log_variance) * epsilon`\n",
    "3. `reconstructed_img = decoder(z)`\n",
    "4. `model = Model(input_img, reconstructed_img)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import FashionMNIST, MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import pytorch_lightning as pl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LitAutoEncoder – Klassifizierungs-konditionierter Variational Autoencoder (VAE)**\n",
    "\n",
    "Dieses Modell ist ein **konditionierter VAE**, der mit PyTorch Lightning implementiert ist. Es kombiniert **Bilddaten (z. B. MNIST)** mit **Klassenzugehörigkeit (Labels)** zur Erzeugung latenter Repräsentationen und Rekonstruktionen.\n",
    "\n",
    "\n",
    "##### `one_hot_embedding(labels, num_classes)`\n",
    "Hilfsfunktion zur Umwandlung von Labels in One-Hot-Vektoren mit `torch.eye`.\n",
    "\n",
    "\n",
    "### Architektur – `LitAutoEncoder`\n",
    "\n",
    "#### Encoder\n",
    "- Besteht aus vier vollvernetzten Schichten (`Linear`) mit LeakyReLU-Aktivierungen.\n",
    "- Nimmt ein **eingeflattetes Bild (28x28) + One-Hot-Label (10 Klassen)** als Eingabe.\n",
    "- Gibt `latent_dimension * 2` Werte aus: \n",
    "  - Erste Hälfte: **Mittelwert (μ)**\n",
    "  - Zweite Hälfte: **Log-Varianz (log(σ²))** – für die Reparametrisierung.\n",
    "\n",
    "#### Decoder\n",
    "- Besteht aus vier `Linear`-Schichten mit LeakyReLU.\n",
    "- Nimmt einen latent-Vektor + One-Hot-Label und rekonstruiert ein Bild (28x28).\n",
    "\n",
    "\n",
    "### Vorwärtsdurchlauf (`forward`)\n",
    "- Gibt direkt den rohen Output des Encoders zurück (z. B. für Inferenz).\n",
    "\n",
    "\n",
    "#### `reparametrization(mu, log_var)`\n",
    "- Wendet den Reparametrisierungstrick an:  \n",
    "  $$ z = \\mu + \\epsilon \\cdot \\sigma $$\n",
    "  Damit ist das Sampling differenzierbar für das Training per Backpropagation.\n",
    "\n",
    "\n",
    "#### `training_step`\n",
    "Verarbeitung eines Batches während des Trainings:\n",
    "1. Bilddaten + One-Hot-Labels kombinieren.\n",
    "2. Durch den Encoder → erhält `mu` und `log_var`.\n",
    "3. Sampling mit Reparametrisierung.\n",
    "4. Kombiniertes Sample + Label → Decoder → Rekonstruiertes Bild (`x_hat`).\n",
    "5. Berechnung der Verluste:\n",
    "   - **Rekonstruktionsverlust**: Mittels MSE zwischen `x_hat` und Originalbild `x`.\n",
    "   - **Kullback-Leibler Divergenz (KLD)**: Regularisiert die latente Verteilung gegen eine Normalverteilung.\n",
    "6. Gesamtverlust = Skaliertes MSE + KLD.\n",
    "7. Logging der Metriken für TensorBoard etc.\n",
    "\n",
    "\n",
    "#### `validation_step`\n",
    "- Gleiche Schritte wie im Training – nur ohne Backpropagation.\n",
    "- Loggt separate Metriken für die Validierung.\n",
    "\n",
    "\n",
    "### `configure_optimizers`\n",
    "- Optimierer: **Adam** mit Lernrate 1e-3.\n",
    "\n",
    "\n",
    "### Anwendung\n",
    "Dieses Modell kann:\n",
    "- Bilder komprimieren & wiederherstellen,\n",
    "- latente Repräsentationen lernen,\n",
    "- Klassenbewusst neue Bilder generieren (z. B. eine \"4\" zeichnen lassen).\n",
    "\n",
    "**Typischer Use Case**: Generatives Modell auf dem MNIST-Datensatz oder ähnlichem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_embedding(labels, num_classes):\n",
    "    y = torch.eye(num_classes, device=labels.device)\n",
    "    return y[labels]\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, latent_dimension=32):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28 + 10, 2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, latent_dimension * 2),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dimension + 10, 2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(2048, 28 * 28),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def reparametrization(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var).sqrt()  # standard deviation\n",
    "        eps = torch.randn_like(std)  # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std)  # sampling as if coming from the input space\n",
    "        return sample\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop. It is independent of forward\n",
    "        x, labels = batch\n",
    "        onehot = one_hot_embedding(labels, 10)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        m = torch.cat([x, onehot], 1)\n",
    "        z = self.encoder(m)\n",
    "        mu = z[:, :32]\n",
    "        log_var = z[:, 32:]\n",
    "\n",
    "        sample = self.reparametrization(mu, log_var)\n",
    "\n",
    "        q = torch.cat([sample, onehot], 1)\n",
    "        x_hat = self.decoder(q)\n",
    "\n",
    "        recon_loss = F.mse_loss(x_hat, x)\n",
    "        KLD = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        ramp_weight = min([1, self.global_step / 2500])\n",
    "        loss = (55000 / 256) * recon_loss + KLD\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, on_step=False)\n",
    "        self.log(\"kl\", KLD, on_epoch=True, prog_bar=True, on_step=False)\n",
    "        self.log(\"recon\", recon_loss, on_epoch=True, prog_bar=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, labels = batch\n",
    "        onehot = one_hot_embedding(labels, 10)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        m = torch.cat([x, onehot], 1)\n",
    "        z = self.encoder(m)\n",
    "        mu = z[:, :32]\n",
    "        log_var = z[:, 32:]\n",
    "        sample = self.reparametrization(mu, log_var)\n",
    "\n",
    "        q = torch.cat([sample, onehot], 1)\n",
    "        x_hat = self.decoder(q)\n",
    "\n",
    "        recon_loss = F.mse_loss(x_hat, x)\n",
    "        KLD = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        loss = (55000 / 256) * recon_loss + KLD\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, on_step=False)\n",
    "        self.log(\"val_kl\", KLD, on_epoch=True, on_step=False)\n",
    "        self.log(\"val_recon\", recon_loss, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training eines Variational Autoencoders mit PyTorch Lightning**\n",
    "\n",
    "In diesem Codeabschnitt wird der **`LitAutoEncoder`** auf dem **MNIST-Datensatz** trainiert – mit automatischem Logging und Validierung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"vae\")\n",
    "\n",
    "dataset = MNIST(\n",
    "    os.getcwd() + \"data/datasets\", download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "train, val = random_split(dataset, [55000, 5000])\n",
    "\n",
    "autoencoder = LitAutoEncoder(latent_dimension=32)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"auto\", devices=1, max_epochs=10, logger=logger)\n",
    "trainer.fit(\n",
    "    autoencoder,\n",
    "    DataLoader(train, batch_size=128, num_workers=10, pin_memory=True),\n",
    "    DataLoader(val, batch_size=128, num_workers=10, pin_memory=True),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Neueste `metrics.csv`-Datei finden (Logging-Auswertung)**\n",
    "\n",
    "Dieses Skript durchsucht das Log-Verzeichnis eines PyTorch Lightning Experiments (z. B. VAE) und findet automatisch die neueste `metrics.csv`-Datei für die Analyse der Trainingsmetriken.\n",
    "\n",
    "\n",
    "### Zweck\n",
    "Das Ziel ist es, **automatisch die aktuellste Version des Log-Ordners** zu identifizieren (z. B. `version_0`, `version_1`, ...), in dem Lightning seine Metriken speichert.\n",
    "\n",
    "\n",
    "### Was passiert genau?\n",
    "\n",
    "1. **`find_latest_metrics_path(base_log_dir)`**\n",
    "   - Liest alle Unterordner im Log-Verzeichnis, die mit `version_` beginnen.\n",
    "   - Sortiert diese Ordner nach ihrer Nummer (z. B. `version_0`, `version_1`, ...).\n",
    "   - Wählt den Ordner mit der höchsten Nummer (= letzter Trainingslauf).\n",
    "   - Gibt den Pfad zur zugehörigen Datei `metrics.csv` zurück.\n",
    "\n",
    "2. **`base_log_dir = os.path.join(\"logs\", \"vae\")`**\n",
    "   - Setzt das Basisverzeichnis, in dem die Logs gespeichert wurden (z. B. durch den `CSVLogger` beim Training).\n",
    "\n",
    "3. **`metrics_path = find_latest_metrics_path(...)`**\n",
    "   - Ruft die oben beschriebene Funktion auf, um den Pfad zur neuesten Log-Datei zu bekommen.\n",
    "\n",
    "4. **`print(...)`**\n",
    "   - Gibt den Pfad zur Konsole aus, z. B.:\n",
    "     ```\n",
    "     Neueste metrics.csv gefunden unter: logs/vae/version_3/metrics.csv\n",
    "     ```\n",
    "\n",
    "\n",
    "**Anwendung:**  \n",
    "Nützlich für Auswertungen mit Pandas, Plotten von Trainings- und Validierungsverlauf oder automatisierte Reports nach dem Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def find_latest_metrics_path(base_log_dir):\n",
    "    version_dirs = [d for d in os.listdir(base_log_dir) if d.startswith(\"version_\")]\n",
    "    if not version_dirs:\n",
    "        raise FileNotFoundError(\"Keine version_x Ordner im Log-Verzeichnis gefunden.\")\n",
    "\n",
    "    # Nach Versionsnummer sortieren\n",
    "    version_dirs.sort(key=lambda x: int(x.split(\"_\")[1]))\n",
    "    latest_version = version_dirs[-1]\n",
    "\n",
    "    return os.path.join(base_log_dir, latest_version, \"metrics.csv\")\n",
    "\n",
    "\n",
    "# Hauptverzeichnis für Logs\n",
    "base_log_dir = os.path.join(\"logs\", \"vae\")\n",
    "metrics_path = find_latest_metrics_path(base_log_dir)\n",
    "\n",
    "print(\"Neueste metrics.csv gefunden unter:\", metrics_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bedingte Bildgenerierung mit einem trainierten VAE**\n",
    "\n",
    "Dieser Code generiert **neue MNIST-Bilder** für jede Ziffer (0 bis 9), indem er den Decoder des VAE mit einem zufälligen Latent-Vektor kombiniert und mit einem festen Label (als One-Hot-Vektor) \"füttert\".\n",
    "\n",
    "\n",
    "### Schleife über alle Ziffern (0–9)\n",
    "\n",
    "Für jede Ziffer wird ein bedingter Datensatz generiert und visualisiert:\n",
    "\n",
    "#### 1. One-Hot-Vektor für das Ziel-Label\n",
    "\n",
    "- Erstellt `512` One-Hot-Vektoren (für z. B. Ziffer 5 steht an Position 5 eine 1).\n",
    "- Ziel: Alle generierten Bilder sollen zur **gleichen Ziffer** gehören.\n",
    "\n",
    "```python\n",
    "onehot = torch.zeros(((16**2) * 2, 10))\n",
    "onehot[:, digit] = 1.0\n",
    "```\n",
    "\n",
    "\n",
    "#### 2. Zufällige latente Vektoren erzeugen\n",
    "\n",
    "- `512` zufällige Vektoren aus einer Normalverteilung.\n",
    "- Repräsentieren unterschiedliche Varianten der Ziel-Ziffer im Latent-Space.\n",
    "\n",
    "```python\n",
    "z = torch.randn(((16**2) * 2, 32)).float()\n",
    "```\n",
    "\n",
    "\n",
    "#### 3. Latent-Vektor + Label kombinieren und durch Decoder schicken\n",
    "\n",
    "- Kombination aus latenten Informationen (`z`) und klassenspezifischem Label (`onehot`).\n",
    "- Decoder erzeugt `512` Bilder (Größe: 28×28) aus diesen zusammengesetzten Eingaben.\n",
    "- `detach()` entfernt sie vom Rechen-Graphen, da keine Gradienten berechnet werden müssen.\n",
    "\n",
    "```python\n",
    "q = torch.cat([z, onehot], 1)\n",
    "y = autoencoder.decoder(q).detach()\n",
    "```\n",
    "\n",
    "\n",
    "#### 4. Umformung & Visualisierung\n",
    "\n",
    "- Bilder werden ins richtige Format gebracht.\n",
    "- `make_grid`: Erstellt ein großes Rasterbild aus allen generierten Ziffern (32 pro Zeile).\n",
    "\n",
    "```python\n",
    "y = y.reshape((512, 1, 28, 28))\n",
    "y_grid = make_grid(y_images, nrow=32)\n",
    "```\n",
    "\n",
    "\n",
    "#### 5. Anzeigen des Bildrasters\n",
    "\n",
    "- Zeigt das generierte Raster für jede Ziffer (`digit`).\n",
    "- Es sollte pro Label eine Sammlung realistisch aussehender Ziffern angezeigt werden, z. B. 512 unterschiedliche Versionen der „7“.\n",
    "\n",
    "```python\n",
    "plt.imshow(y_grid[0].cpu(), cmap=\"binary\", vmin=0, vmax=1)\n",
    "plt.title(\"Samples conditioned to the label [{}]\".format(digit))\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "### Ziel\n",
    "\n",
    "Dieses Verfahren demonstriert die **kontrollierte Generierung**:\n",
    "\n",
    "> Der VAE erzeugt neue Bilder, die **gezielt** zu einer bestimmten Ziffer gehören, gesteuert durch den One-Hot-Vektor.\n",
    "\n",
    "Perfekt zur **Visualisierung des latenten Raums** und zur **Evaluierung der Generierungsfähigkeit** des Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for digit in range(10):\n",
    "    onehot = torch.zeros(((16**2) * 2, 10))\n",
    "    onehot[:, digit] = 1.0\n",
    "    z = torch.randn(((16**2) * 2, 32)).float()\n",
    "\n",
    "    q = torch.cat([z, onehot], 1)\n",
    "    y = autoencoder.decoder(q).detach()\n",
    "    y = y.reshape((16**2) * 2, 1, 28, 28)\n",
    "    y_images = list(iter(y))\n",
    "\n",
    "    y_grid = make_grid(y_images, nrow=16 * 2)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(y_grid[0].cpu(), cmap=\"binary\", vmin=0, vmax=1)\n",
    "    plt.title(\"Samples conditioned to the label [{}]\".format(digit))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where's the intelligence?\n",
    "\n",
    "- Was garantiert, dass bei Variational Autoencodern eine sinnvolle Repräsentation der Daten im latenten Raum generiert wird?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Imports:** PyTorch- und torchvision-Module für Modellierung, Datenverarbeitung und Bildspeicherung werden geladen.\n",
    "- **Batchgrösse:** Es wird eine Batchgrösse von `100` definiert.\n",
    "- **MNIST-Datensatz:**\n",
    "  - Trainingsdaten werden heruntergeladen und als Tensor transformiert.\n",
    "  - Testdaten werden lokal vorausgesetzt und ebenfalls in Tensoren umgewandelt.\n",
    "- **DataLoader:**\n",
    "  - `train_loader`: Lädt Trainingsdaten zufällig (für besseres Training).\n",
    "  - `test_loader`: Lädt Testdaten in fester Reihenfolge (für Evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "bs = 100\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data/mnist_data/\",\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data/mnist_data/\",\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=False,\n",
    ")\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=bs, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=bs, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Modell:** Es wird ein Variational Autoencoder (VAE) als `nn.Module` definiert.\n",
    "- **Encoder:**\n",
    "  - Besteht aus zwei ReLU-aktivierten Linear-Schichten (`fc1`, `fc2`).\n",
    "  - Gibt Mittelwert `mu` (`fc31`) und Log-Varianz `log_var` (`fc32`) des latenten Raums aus.\n",
    "- **Sampling:** \n",
    "  - Aus `mu` und `log_var` wird mit der Reparametrisierungstrick ein latenter Vektor `z` gezogen.\n",
    "- **Decoder:**\n",
    "  - Rekonstruiert den Eingabevektor aus `z` durch drei Linear-Schichten (`fc4`–`fc6`) mit ReLU und Sigmoid am Ende.\n",
    "- **Forward-Pass:**\n",
    "  - Input `x` wird auf 784-Dim flatten, durch Encoder geschickt, `z` gesampled und durch Decoder rekonstruiert.\n",
    "  - Gibt `rekonstruktion`, `mu` und `log_var` zurück.\n",
    "- **Modellinstanzierung:**\n",
    "  - VAE wird mit Eingabedimension 784 (MNIST-Bilder), zwei versteckten Schichten und latentem Raum mit 2 Dimensionen erstellt.\n",
    "  - Falls CUDA verfügbar ist, wird das Modell auf die GPU verschoben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)  # mu, log_var\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)  # return z sample\n",
    "\n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "\n",
    "# build model\n",
    "vae = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeige das Modell\n",
    "vae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Optimizer:** Es wird der Adam-Optimizer für die Parameter des VAE verwendet.\n",
    "- **Loss-Funktion:** Kombiniert zwei Komponenten:\n",
    "  - **Rekonstruktionsverlust (BCE):** Binary Cross Entropy zwischen Eingabe `x` und Rekonstruktion `recon_x`.\n",
    "  - **KL-Divergenz (KLD):** Misst die Abweichung der latenten Verteilung von einer Standardnormalverteilung.\n",
    "- **Gesamtkosten:** Summe aus `BCE` und `KLD` wird zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "\n",
    "\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction=\"sum\")\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **train-Funktion:** Führt einen Trainingsdurchlauf über alle Batches durch.\n",
    "- **Modus:** Modell wird in Trainingsmodus versetzt (`vae.train()`).\n",
    "- **Schleife über DataLoader:**\n",
    "  - Daten werden auf die GPU verschoben.\n",
    "  - Gradienten werden auf Null gesetzt (`optimizer.zero_grad()`).\n",
    "  - Vorwärtsdurchlauf: Rekonstruktion + latente Parameter (`mu`, `log_var`).\n",
    "  - Verlustberechnung mit Rekonstruktionsfehler + KL-Divergenz.\n",
    "  - Backward-Pass und Optimierungsschritt.\n",
    "  - Zwischenergebnisse werden regelmäßig ausgegeben (alle 100 Batches).\n",
    "- **Epoch-Ausgabe:** Am Ende wird der durchschnittliche Verlust über alle Trainingsdaten ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(data),\n",
    "                )\n",
    "            )\n",
    "    print(\n",
    "        \"====> Epoch: {} Average loss: {:.4f}\".format(\n",
    "            epoch, train_loss / len(train_loader.dataset)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **test-Funktion:** Bewertet das Modell auf dem Testdatensatz.\n",
    "- **Modus:** Modell wird in Evaluierungsmodus versetzt (`vae.eval()`), Dropout & BatchNorm deaktiviert.\n",
    "- **Ohne Gradienten:** `torch.no_grad()` spart Speicher und Rechenzeit.\n",
    "- **Schleife über Testdaten:**\n",
    "  - Daten werden auf die GPU verschoben.\n",
    "  - Vorwärtsdurchlauf liefert Rekonstruktion und latente Parameter.\n",
    "  - Verlust pro Batch wird summiert.\n",
    "- **Ergebnis:** Durchschnittlicher Testverlust über alle Daten wird ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.cuda()\n",
    "            recon, mu, log_var = vae(data)\n",
    "\n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(\"====> Test set loss: {:.4f}\".format(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training über 50 Epochen:** \n",
    "  - Für jede Epoche wird:\n",
    "    - Die `train()`-Funktion aufgerufen (Training des VAE).\n",
    "    - Danach die `test()`-Funktion ausgeführt (Evaluation des Modells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 51):\n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Output-Verzeichnis:** Ordner `./data/samples` wird erstellt, falls er noch nicht existiert.\n",
    "- **Generierung & Visualisierung:**\n",
    "  - In zwei Schleifendurchläufen werden jeweils 128 zufällige Punkte `z` aus dem latenten Raum (2D) erzeugt.\n",
    "  - Der Decoder wandelt diese in Bilder um.\n",
    "  - Die generierten Bilder werden:\n",
    "    - Als PNG-Datei gespeichert (`save_image`).\n",
    "    - Als Grid visualisiert (`make_grid` + `matplotlib`), je 16 Bilder pro Zeile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sicherstellen, dass der Ordner existiert\n",
    "output_dir = \"./data/samples\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Mehrere Beispiele generieren und plotten\n",
    "with torch.no_grad():\n",
    "    for i in range(2):  # 10 verschiedene Beispiele\n",
    "        z = torch.randn(128, 2).cuda()  # Mehr zufällige Punkte im latenten Raum\n",
    "        sample = vae.decoder(z).cuda()  # Bilder generieren\n",
    "\n",
    "        # Bild speichern\n",
    "        save_image(sample.view(128, 1, 28, 28), f\"{output_dir}/sample_{i}.png\")\n",
    "\n",
    "        # Bilder plotten\n",
    "        grid = make_grid(\n",
    "            sample.view(128, 1, 28, 28), nrow=16, normalize=True\n",
    "        )  # Größeres Grid\n",
    "        plt.figure(figsize=(12, 12))  # Größere Figur\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap=\"gray\")\n",
    "        plt.title(f\"Generated Samples - Batch {i + 1}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Modell in Eval-Modus:** `vae.eval()` deaktiviert Trainingseffekte (z.B. Dropout).\n",
    "- **Latent Space Grid:**\n",
    "  - Ein 2D-Gitter aus Punkten im latenten Raum wird definiert (30x30 Werte von -3 bis 3).\n",
    "  - Für jedes Koordinatenpaar `(x, y)` wird ein latenter Vektor `z` erzeugt und vom Decoder in ein Bild umgewandelt.\n",
    "- **Bild-Grid:**\n",
    "  - Die Bilder werden zeilen- und spaltenweise zu einem großen Bildraster (`make_grid`) zusammengesetzt.\n",
    "  - Dieses Raster zeigt eine visuelle Interpolation durch den latenten Raum.\n",
    "- **Visualisierung & Speicherung:**\n",
    "  - Das große Bildraster wird geplottet und als PNG gespeichert (`latent_space_walk_large.png`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "grid_size = 30\n",
    "grid_x = np.linspace(-3, 3, grid_size)\n",
    "grid_y = np.linspace(-3, 3, grid_size)\n",
    "\n",
    "samples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for y in grid_y:\n",
    "        row = []\n",
    "        for x in grid_x:\n",
    "            z = torch.tensor([[x, y]], dtype=torch.float32).cuda()\n",
    "            img = vae.decoder(z).view(1, 28, 28)\n",
    "            row.append(img)\n",
    "        samples.append(torch.cat(row, dim=0))\n",
    "\n",
    "all_samples = torch.cat(samples, dim=0).unsqueeze(1)\n",
    "grid = make_grid(all_samples, nrow=grid_size, normalize=True, padding=2)\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.figure(figsize=(20, 20))  # Größer!\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Großer Latent Space Walk\")\n",
    "plt.show()\n",
    "\n",
    "# Optional speichern\n",
    "save_image(grid, \"data/latent_space_walk_large.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
