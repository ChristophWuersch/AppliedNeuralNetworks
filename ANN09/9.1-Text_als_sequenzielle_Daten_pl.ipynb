{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN09/9.1-Text_als_sequenzielle_Daten_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This notebook contains the first code sample found in Chapter 6, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In diesem Kapitel werden wir Deep-Learning-Modelle erkunden, die Texte (in Form von Wortsequenzen oder Zeichenfolgen), Zeitreihen und sequenzielle Daten im Allgemeinen verarbeiten können. Die beiden grundlegenden Deep-Learning-Algorithmen\n",
    "für die sequenzielle Verarbeitung von Daten sind **rekurrente neuronale Netze (RNNs) und 1-D-CNNs**, die eindimensionale Variante der CNNs, die wir in den vorangegangenen Kapiteln erörtert haben. In diesem Kapitel werden wir beide\n",
    "Ansätze betrachten. \n",
    "\n",
    "Die Anwendungsmöglichkeiten dieser Algorithmen sind vielfältig:\n",
    "\n",
    "- **Klassifizierung von Dokumenten** und Zeitreihen, wie beispielsweise die Erkennung des Themas eines Artikels oder des Autors eines Buchs\n",
    "- Vergleich von Zeitreihen, beispielsweise die Abschätzung, wie eng verwandt zwei Dokumente oder zwei Börsenkürzel sind\n",
    "- **Übersetzung:** Erlernen der Umwandlung von einer Sequenz in eine andere, wie z.B. die Übertragung eines englischen Satzes ins Französische\n",
    "- **Stimmungsanalyse** (sentiment analysis), wie die Klassifizierung von Tweets oder Filmbewertungen als positiv oder negativ\n",
    "- **Zeitreihenvorhersage**, wie etwa die Prognose des zukünftigen Wetters an einem bestimmten Ort anhand der vorangegangenen Wetterdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 9. Text als sequenzielle Daten und Anwendungen von NLP\n",
    "### (NLP=Natural Language Processing)\n",
    "\n",
    "Text ist die wohl verbreitetste Form **sequenzieller Daten**. Man kann Text als eine Sequenz von Zeichen oder Wörtern auffassen, aber üblicherweise betrachtet man die Wörter. \n",
    "\n",
    "Die in den folgenden Abschnitten vorgestellten Deep-Learning-Modelle zur Verarbeitung von Texten können ein elementares Verständnis natürlicher Sprache erreichen, das für folgende Anwendungen ausreicht:\n",
    "- die **Klassifizierung** von Dokumenten,\n",
    "- die **Stimmungsanalyse** (sentiment analysis), \n",
    "- die **Erkennung von Autoren** oder sogar das \n",
    "- **Beantworten von Fragen** (in eingeschränktem Kontext, chatbots).\n",
    "\n",
    "Sie sollten sich bei der Lektüre dieses Kapitels natürlich darüber im Klaren sein, dass **keins dieser Deep-Learning-Modelle einen Text wirklich so wie ein Mensch versteht**. \n",
    "- Die Modelle können jedoch die statistische Struktur der Schriftsprache erkennen, und das genügt, um viele einfache textbezogene Aufgaben zu lösen. \n",
    "- Bei der Verarbeitung natürlicher Sprache durch Deep Learning wird die Mustererkennung auf Wörter, Sätze und Textpassagen angewendet, ähnlich wie beim maschinellen Sehen die Mustererkennung auf Pixel angewandt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vektorisierung\n",
    "\n",
    "Ebenso wie alle anderen NNs nehmen Deep-Learning-Modelle keinen reinen Text als Eingabe entgegen, sondern numerische Tensoren. \n",
    "- Die **Umwandlung von Text in numerische Tensoren wird als Vektorisierung bezeichnet** und kann auf verschiedene Weise durchgeführt werden:\n",
    "\n",
    "- Teilen Sie den Text in *Wörter* auf und wandeln Sie die einzelnen Wörter in Vektoren um.\n",
    "- Teilen Sie den Text in *Zeichen* auf und wandeln Sie die einzelnen Zeichen in Vektoren um.\n",
    "- Extrahieren Sie *N-Gramme* aus Wörtern oder Zeichen und wandeln Sie die einzelnen N-Gramme in Vektoren um. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bei der Verarbeitung natürlicher Sprache werden zusammenhängende Textfragmente – Wörter, Buchstaben oder Symbole – als **N-Gramme** bezeichnet.\n",
    "\n",
    "<img src=\"Bilder/Token_Vektorisierung.png\" width=\"340\" height=\"340\" align=\"center\"/>\n",
    "Aus einem Text werden Tokens, und aus den Tokens werden Vektoren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokens\n",
    "\n",
    "- Die verschiedenen Bestandteile, in die Sie einen Text zerlegen können (**Wörter, Zeichen oder N-Gramme**), bezeichnet man zusammengenommen als **Tokens** und die Aufteilung von Text in solche Tokens als **Tokenisierung**.\n",
    "- Bei jeder Vektorisierung von Text kommt irgendeine Form der Tokenisierung zum Einsatz, die den erzeugten Tokens einen numerischen Vektor zuordnet.\n",
    "- Diese Vektoren werden zu **Sequenztensoren** gebündelt und in DNNs eingespeist. \n",
    "\n",
    "Es gibt mehrere Möglichkeiten, einem Token einen Vektor zuzuordnen. In diesem Abschnitt werden\n",
    "wir die beiden wichtigsten behandeln: \n",
    "1.  die *One-hot-Codierung* von Tokens und\n",
    "2. die *Tokeneinbettung*, die typischerweise ausschliesslich für Wörter verwendet wird, was man dann als Worteinbettung (word embedding) bezeichnet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### N-Gramme und das Bag-of-words-Modell\n",
    "\n",
    "N-Gramme von Wörtern sind Gruppen von N (oder weniger) aufeinanderfolgenden Wörtern, die Sie einem Satz entnehmen können. Dasselbe Konzept ist auch auf Zeichen statt Wörter anwendbar.\n",
    "\n",
    "Betrachten Sie als einfaches Beispiel den Satz »The cat sat on the mat.«, der in Bigramme (N-Gramme der Größe 2) zerlegt werden kann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "    \"The\",\n",
    "    \"The cat\",\n",
    "    \"cat\",\n",
    "    \"cat sat\",\n",
    "    \"sat\",\n",
    "    \"sat on\",\n",
    "    \"on\",\n",
    "    \"on the\",\n",
    "    \"the\",\n",
    "    \"the mat\",\n",
    "    \"mat\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Der Satz könnte auch in die folgende Menge von Trigrammen zerlegt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "    \"The\",\n",
    "    \"The cat\",\n",
    "    \"cat\",\n",
    "    \"cat sat\",\n",
    "    \"The cat sat\",\n",
    "    \"sat\",\n",
    "    \"sat on\",\n",
    "    \"on\",\n",
    "    \"cat sat on\",\n",
    "    \"on the\",\n",
    "    \"the\",\n",
    "    \"sat on the\",\n",
    "    \"the mat\",\n",
    "    \"mat\",\n",
    "    \"on the mat\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So eine Menge wird als **Bag-of-words** (»Beutel-voller-Wörter«, in diesem Fall Bag of-bigrams bzw. Bag-of-trigrams) bezeichnet. Der Begriff »Bag« weist darauf hin, dass Sie es nicht mit einer Liste oder einer Sequenz, sondern mit einer Menge\n",
    "von Tokens zu tun haben, denn die Tokens besitzen keine bestimmte Reihenfolge.\n",
    "Diese Methoden der Tokenisierung heissen **Bag-of-words-Modelle**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One-hot-Codierung von Wörtern und Zeichen\n",
    "\n",
    "Die **One-hot-Codierung** ist die gebräuchlichste und einfachste Methode, ein Token in einen Vektor umzuwandeln. Wir haben die One-hot-Codierung zuvor schon bei den IMDb- und Reuters-Beispielen in Aktion gesehen (die Codierung von Wörtern in diesen beiden Beispielen). \n",
    "\n",
    "- Bei der One-hot-Codierung wird jedem Wort ein eindeutiger Integerindex zugeordnet. \n",
    "- Dieser Index `i` wird anschliessend in einen binären Vektor der Größe `N` umgewandelt (`N` bezeichnet die Grösse des Vokabulars (dictionaries), also die Anzahl unterschiedlicher Wörter). \n",
    "- Der Vektor enthält bis auf eine Ausnahme nur Nullen, das i-te Element ist 1. \n",
    "- Die One-hot-Codierung ist natürlich auch auf Zeichen anwendbar. \n",
    "\n",
    "Um zu verdeutlichen, wie die One-hot-Codierung funktioniert und wie sie implementiert wird, finden Sie in den folgenden Listings zwei Beispiele. Das erste codiert Wörter und das zweite Zeichen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This is our initial data; one entry per \"sample\"\n",
    "# (in this toy example, a \"sample\" is just a sentence, but\n",
    "# it could be an entire document).\n",
    "samples = [\"the cat sat on the mat.\", \"the dog ate my homework.\"]\n",
    "\n",
    "# First, build an index of all tokens in the data.\n",
    "token_index = {}\n",
    "i = 0\n",
    "for sample in samples:\n",
    "    # We simply tokenize the samples via the `split` method.\n",
    "    # in real life, we would also strip punctuation and special characters\n",
    "    # from the samples.\n",
    "    for word in sample.split():\n",
    "        print(\"(%i %s)\" % (i + 1, word))\n",
    "        if word not in token_index:\n",
    "            i += 1\n",
    "            # Assign a unique index to each unique word (dictionary)\n",
    "            token_index[word] = len(token_index) + 1\n",
    "            # Note that we don't attribute index 0 to anything.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we vectorize our samples.\n",
    "# We will only consider the first `max_length` words in each sample.\n",
    "max_length = 10\n",
    "\n",
    "# This is where we store our results:\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "results.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "list(enumerate(sample.split()))[:max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "token_index.get(\"dog\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "samples[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "list(enumerate(samples[0].split()))[:max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "list(enumerate(samples[1].split()))[:max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1, :, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Character level one-hot encoding (toy example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "characters = string.printable  # All printable ASCII characters.\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample[:max_length]):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "results.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1, :, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenizer\n",
    "\n",
    "Beachten Sie, dass **Hilfsfunktionen** für die One-hot-Codierung von Wörtern\n",
    "und Zeichen integriert sind. \n",
    "- Sie sollten diese Hilfsfunktionen auch verwenden, denn sie bieten eine Reihe wichtiger Features, wie das Entfernen von Sonderzeichen oder dass nur die N am häufigsten in der Datenmenge vorkommenden Wörter berücksichtigt werden (eine häufig vorgenommene Beschränkung, die sehr grosse Eingabevektoren verhindern soll)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Download von `punkt_tab` muss im lokalen Verzeichnis der virtuellen Umgebung geschehen:\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "nltk.download(\"punkt\")  # Nur beim ersten Mal nötig\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "\n",
    "# 1. Tokenisieren der Sätze in Wörter (word_tokenize macht auch . und , einzeln)\n",
    "tokenized_samples = [nltk.word_tokenize(sent.lower()) for sent in samples]\n",
    "\n",
    "# 2. Vokabular erstellen\n",
    "word_index = {}\n",
    "index = 1  # Startindex bei 1\n",
    "for sentence in tokenized_samples:\n",
    "    for word in sentence:\n",
    "        if word not in word_index:\n",
    "            word_index[word] = index\n",
    "            index += 1\n",
    "\n",
    "# 3. Texte in Sequenzen von Integern umwandeln (wie texts_to_sequences)\n",
    "sequences = [[word_index[word] for word in sentence] for sentence in tokenized_samples]\n",
    "\n",
    "print(f\"Found {len(word_index)} unique tokens.\")\n",
    "pprint(word_index)\n",
    "print(\"Sequences:\", sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-hot-Hashing-Trick\n",
    "\n",
    "Es gibt eine Variante der One-hot-Codierung, den sogenannten *One-hot-Hashing-Trick*, den Sie verwenden können, wenn die Anzahl unterschiedlicher Tokens in Ihrem Vokabular zu gross ist, um sie explizit zu handhaben. \n",
    "- Anstatt jedem Wort ausdrücklich einen Index zuzuweisen und eine Referenz auf diese Indizes in einem Dictionary zu speichern, können Sie mit einer *Hashfunktion Vektoren fester Grösse* erzeugen. \n",
    "- Dazu wird typischerweise eine äusserst **leichtgewichtige (d.h. einfach berechenbare) Hashfunktion** verwendet. Der Hauptvorteil dieser Methode besteht darin, dass die Notwendigkeit entfällt, einen expliziten Wortindex verwalten zu müssen. \n",
    "- Das spart Speicherplatz und gestattet es, die **Daten in Echtzeit zu codieren**, denn Sie können die Tokenvektoren schon erzeugen, bevor sämtliche Daten verfügbar sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Dieser Ansatz hat allerdings den Nachteil, dass er für **Hashkollisionen** anfällig ist:\n",
    "- Zwei verschiedenen Wörtern könnte derselbe Hashwert zugewiesen werden. \n",
    "- Ein Machine-Learning-Modell wäre dann nicht mehr in der Lage, diese beiden Wörter zu unterscheiden. \n",
    "- Die Wahrscheinlichkeit für Hashkollisionen sinkt, wenn die Dimensionalität des Hashraums beträchtlich grösser ist als die Gesamtzahl der eindeutigen Tokens, für die Hashwerte erzeugt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier als Beispiel ein Word-level one-hot encoding mit dem hashing trick (toy example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "dimensionality = 1000  # Größe der Hash-Vektoren\n",
    "max_length = 10  # Maximale Anzahl Wörter pro Sample\n",
    "\n",
    "# Initialisiere den Tensor mit Nullen\n",
    "results = torch.zeros((len(samples), max_length, dimensionality))\n",
    "\n",
    "# Iteriere über die Samples\n",
    "for i, sample in enumerate(samples):\n",
    "    words = sample.replace(\".\", \"\").split()  # Entferne Punkt am Ende\n",
    "    for j, word in list(enumerate(words))[:max_length]:\n",
    "        # Berechne Hash-Code und bring ihn in Bereich [0, dimensionality)\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1.0\n",
    "        print(f\"{word} \\thash code: {index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "results.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "abs(hash(\"The\")) % dimensionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
