{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN09/9.0-NLP_Basics_Theory-ger_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id='toc1_'></a>[NLP - From Tokenization to Word Embeddings](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dies ist eine kurze Einführung in die Verarbeitung von natürlicher Sparche (NLP = natural language processing) mit Methoden des Maschinellen Lernens. \n",
    "- Wir beginnen mit den Grundlagen der computergestützten natürlichen Sprachverarbeitung wie *Tokenisierung, Stemming* oder *Lemmatisierung*. \n",
    "- Dank der riesigen Datenmengen und der vorhandenen Rechenleistung (GPUs) war es in den letzten Jahren möglich, große Fortschritte auf dem Gebiet der NLP zu machen, auch dank der Verwendung komplexer Deep-Learning-Modelle wie `Seq2Seq` oder `Transformer`.\n",
    "- Die heutigen Modelle haben Millionen oder sogar Milliarden von trainierbaren Parametern und benötigen Millionen von Dokumenten für den Trainingsprozess. \n",
    "\n",
    "Ganz am Ende des Notizbuchs befindet sich ein Link zu einem Github-Repository, das die Implementierung eines BERT-Transformers für eine Frage- und Antwortaufgabe enthält."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Inhaltsverzeichnis](#toc0_)\n",
    "\n",
    "* [1. An overview of NLP - natural language processing](#chapter1)\n",
    "* [2.NLTK - natural language toolkit](#chapter2)\n",
    "    * [2.1 Some Theory behind NLTK](#section_2_1)\n",
    "        * [2.1.1 Tokenizing](#section_2_1_1)\n",
    "        * [2.1.2 Stemming](#section_2_1_2)\n",
    "        * [2.1.3 Lemmatizing](#section_2_1_3)      \n",
    "    * [2.2 Practical example of NLTK](#section_2_2)\n",
    "        * [2.2.1 Tokenizing](#section_2_2_1)\n",
    "        * [2.2.2 Stemming](#section_2_2_2)\n",
    "        * [2.2.3 Lemmatizing](#section_2_2_3)\n",
    "* [3. Word embeddings](#chapter3)\n",
    "    * [3.1 Overview of word embeddings](#section_3_1)\n",
    "    * [3.2 One-hot encoding](#section_3_2)\n",
    "    * [3.3 Embedding layer](#section_3_3)\n",
    "        * [3.3.1 Training of a embedding layer](#section_3_3_1)\n",
    "        * [3.3.2 Embedding layer](#section_3_3_2)\n",
    "          * [3.3.2.1 Example of a embedding layer](#section_3_3_2_1)\n",
    "    * [3.4 Word2Vec](#section_3_4)\n",
    "        * [3.4.1 One word context model](#section_3_4_1)\n",
    "        * [3.4.2 Skip-gram model](#section_3_4_2)\n",
    "            * [3.4.2.1 word2vec Skip-gram model python implementation](#section_3_4_2_1)\n",
    "        * [3.4.3 Continuous bag-of-word](#section_3_4_3)\n",
    "    * [3.5 GloVe - Global vectors for word representation](#section_3_5)\n",
    "        * [3.5.1 Basic principle of GloVe](#section_3_5_1)\n",
    "        * [3.5.2 Mathematical background of the GloVe approach](#section_3_5_2)\n",
    "        * [3.5.3 Training details of GloVe](#section_3_5_3)\n",
    "* [4. Sequence2Sequence models](#chapter4)\n",
    "    * [4.1 Encoder - Decoder principle](#section_4_1)\n",
    "    * [4.2 Mathematical background](#section_4_2)\n",
    "    * [4.3 Practical implementation](#section_4_3)\n",
    "* [5. Attention mechanism](#chapter5)\n",
    "    * [5.1 Visual intuition of attention mechanism](#section_5_1)\n",
    "    * [5.2 Mathematical background of attention mechanism](#section_5_2)\n",
    "        * [5.2.1 Bahdanau model](#section_5_2_1)\n",
    "        * [5.2.2 Luong model](#section_5_2_2)\n",
    "    * [5.3 Self-attention mechanism](#section_5_3)\n",
    "    * [5.4 Multi-head attention mechanism](#section_5_4)\n",
    "* [6. Literature](#chapter7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[](#toc0_)\n",
    "\n",
    "Die Verarbeitung natürlicher Sprache (NLP) ist ein aktives Forschungsgebiet der Linguistik, Informatik und künstlichen Intelligenz. Das Hauptziel von NLP ist die Fähigkeit eines Computers, den Inhalt von Texten oder Dokumenten zu verstehen. Auf dem Gebiet des NLP gibt es viele verschiedene anspruchsvolle Aufgaben zu lösen:\n",
    "\n",
    "\n",
    "- **Maschinelle Übersetzung:**\n",
    "    - ist die Aufgabe, einen Satz $x$ aus einer Sprache in einen Satz $y$ in einer anderen Sprache zu übersetzen. Eines der bekanntesten Beispiele dafür ist: www.deepl.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    \n",
    "- **Textklassifikation:**\n",
    "    - Bei der Textklassifizierung geht es darum, die Bedeutung von unstrukturiertem Text zu verstehen und ihn in vordefinierte Kategorien (Tags) einzuordnen. Eine der beliebtesten Textklassifizierungsaufgaben ist die Stimmungsanalyse (sentiment analysis), die darauf abzielt, unstrukturierte Daten nach Stimmungen zu kategorisieren.\n",
    "    \n",
    "    - Eine grundlegende Aufgabe bei der Textklassifikation / Stimmungsanalyse ist die Klassifizierung der Polarität eines gegebenen Textes auf Dokument-, Satz- oder Merkmals-/Aspekt-Ebene - ob die ausgedrückte Meinung in einem Dokument, einem Satz oder einem Entitätsmerkmal/Aspekt positiv, negativ oder neutral ist. [1]\n",
    "    \n",
    "- **Semantische Analyse:**\n",
    "    - Semantische Aufgaben analysieren die Struktur von Sätzen, Wortinteraktionen und verwandte Konzepte in dem Versuch, die Bedeutung von Wörtern zu entdecken und das Thema eines Textes zu verstehen. [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **Part-of-Speech-Tagging:**\n",
    "    - Part-of-Speech-Tagging (PoS) ist ein beliebtes Verfahren zur Verarbeitung natürlicher Sprache, das sich auf die Kategorisierung von Wörtern in einem Text (Korpus) in Übereinstimmung mit einem bestimmten Wortteil bezieht, abhängig von der Definition des Wortes und seinem Kontext. PoS-Tagging ist nützlich, um Beziehungen zwischen Wörtern zu erkennen und somit die Bedeutung von Sätzen zu verstehen.\n",
    "    \n",
    "    - Beispiel: \"Kundendienst\": NOUN, \"könnte\": VERB, \"nicht\": ADVERB, \"sein\": VERB, \"besser\": ADJEKTIV, \"!\": INTERPUNKTION [1]\n",
    "    \n",
    "    \n",
    "- **Text-Zusammenfassung:**\n",
    "    - Die Textzusammenfassung im NLP ist der Prozess der Zusammenfassung der wichtigsten Informationen in großen Texten zum schnelleren Konsum.\n",
    "   \n",
    "   \n",
    "- **Generative Modelle für NLP:**\n",
    "    - Generative Modelle werden normalerweise auf einer großen Datenmenge trainiert, mit der Fähigkeit, anschließend neue Texte zu erstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[](#toc0_)\n",
    "\n",
    "### <a id='toc1_3_1_'></a>[](#toc0_)\n",
    "\n",
    "NLTK ist eine führende Plattform für die Erstellung von Python-Programmen für die Arbeit mit menschlichen Sprachdaten. Es bietet einfach zu bedienende Schnittstellen zu über 50 Korpora und lexikalischen Ressourcen wie `WordNet`, zusammen mit einer Reihe von Textverarbeitungsbibliotheken für Klassifizierung, *Tokenisierung, Stemming, Tagging, Parsing* und *semantische Schlussfolgerungen*, Wrapper für industrielle NLP-Bibliotheken und ein aktives Diskussionsforum. [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[](#toc0_)\n",
    "\n",
    "Ein Tokenizer zerlegt unstrukturierte Daten und natürlichsprachliche Texte in Informationsbrocken, die als diskrete Elemente betrachtet werden können. Normalerweise ist dies der erste Schritt in jedem NLP-Projekt. Die beiden gebräuchlichsten Versionen von Tokenizern sind das \"wortbasierte Tokenisieren\" und das \"satzbasierte Tokenisieren\". \n",
    "\n",
    "**Tokenisierung nach Wörtern:**\n",
    "\n",
    "Wörter sind so etwas wie die Atome der natürlichen Sprache. Sie sind die kleinste Bedeutungseinheit, die für sich genommen noch einen Sinn ergibt. Die Tokenisierung eines Textes nach Wörtern ermöglicht es, Wörter zu identifizieren, die besonders häufig vorkommen. Wenn Sie z. B. eine Gruppe von Stellenausschreibungen analysieren, könnten Sie feststellen, dass das Wort \"Python\" häufig vorkommt. Das könnte auf eine hohe Nachfrage nach Python-Kenntnissen hindeuten, aber man müsste genauer hinsehen, um mehr zu erfahren.\n",
    "\n",
    "\n",
    "**Satzweise Tokenisierung:**\n",
    "\n",
    "Wenn Sie nach Sätzen tokenisieren, können Sie analysieren, wie sich diese Wörter zueinander verhalten, und mehr Kontext erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ExampleText = \"\"\"In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing,\n",
    "                due in part to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks.\n",
    "                For example in language modeling, parsing, and many others.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "word_tokenize(ExampleText)[0:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(ExampleText)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc1_3_3_'></a>[](#toc0_)\n",
    "\n",
    "Stemming und Lemmatisierung sind *Textnormalisierungstechniken* (oder manchmal auch Wortnormalisierung genannt) im Bereich der Verarbeitung natürlicher Sprache, die verwendet werden, um Texte, Wörter und Dokumente für die weitere Verarbeitung vorzubereiten.\n",
    "- Die mathematischen Modelle benötigen eine vektorielle Beschreibung eines Wortes oder eines Satzes und können nicht direkt mit Zeichenketten arbeiten.\n",
    "- **Stemming** und **Lemmatisierung** werden seit den 1960er Jahren in der Informatik untersucht und Algorithmen entwickelt [3]. Sie dienen der Standardisierung der Texteingaben.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Beim *Stemming* handelt es sich um eine Textverarbeitungsaufgabe, bei der Wörter auf ihre Wurzel (root), d. h. den Kernteil eines Wortes, reduziert werden. \n",
    "- Zum Beispiel haben die Wörter \"helfen\" und \"Helfer\" die gleiche Wurzel \"helfen\". Das Stemming ermöglicht es, sich auf die Grundbedeutung eines Wortes zu konzentrieren und nicht auf alle Details, wie es verwendet wird. \n",
    "- Das Stemmen eines Wortes oder eines Satzes kann zu Wörtern führen, die keine richtigen Wörter sind. \n",
    "- Wortstämme werden durch das *Entfernen der Suffixe oder Präfixe* eines Wortes gebildet. \n",
    "- Das *Natural Language Toolkit (NLTK)* enthält mehr als nur einen Stemming-Algorithmus. Für die englische Sprache können Sie zwischen **PorterStammer** und **LancasterStammer** wählen, wobei PorterStemmer der älteste ist, der 1979 entwickelt wurde. LancasterStemmer wurde 1990 entwickelt und verwendet einen aggressiveren Ansatz als der Porter Stemming Algorithmus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Porter Stemmer**: Porter Stemmer verwendet Suffixstripping, um Wortstämme zu erzeugen. Beachten Sie, wie der Porter Stemmer die Wurzel (Stamm) des Wortes \"cats\" durch einfaches Entfernen des \"s\" nach cat erzeugt. Dies ist ein Suffix, das an cat angehängt wird, um es plural zu machen. Aber wenn Sie sich \"trouble\", \"troubling\" und \"troubled\" ansehen, werden sie zu \"trouble\" gestemmt, weil der Porter Stemmer Algorithmus nicht der Linguistik folgt, sondern einer Reihe von Regeln für verschiedene Fälle, die in Phasen (Schritt für Schritt) angewandt werden, um Stämme zu erzeugen. Dies ist der Grund, warum Porter Stemmer nicht oft Wortstämme generiert, die tatsächlich englische Wörter sind.[3]\n",
    "\n",
    "\n",
    "**Lancaster Stemmer**: Der Lancaster Stemmer (*Paice-Husk Stemmer*) ist ein iterativer Algorithmus mit extern gespeicherten Regeln. Er enthält eine Tabelle mit etwa 120 Regeln, die durch den letzten Buchstaben eines Suffixes indiziert sind. Bei jeder Iteration wird versucht, eine anwendbare Regel für den letzten Buchstaben des Wortes zu finden. Jede Regel gibt entweder die Löschung oder die Ersetzung einer Endung an. If there is no such rule, it terminates. It also terminates if a word starts with a vowel and there are only two letters left or if a word starts with a consonant and there are only three characters left. Andernfalls wird die Regel angewandt, und der Vorgang wird wiederholt.[3]\n",
    "\n",
    "\n",
    "**Snowball Stemmer**: Der Snowball Stemmer ist ein nicht-englischer Stemmer. Mit dem Snowball Stemmer ist es möglich, einen eigenen Sprachstemmer zu erstellen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_4_'></a>[](#toc0_)\n",
    "\n",
    "**Lemmatisierung reduziert im Gegensatz zu Stemming die gebeugten Wörter richtig, um sicherzustellen, dass das Wurzelwort zur Sprache gehört. In der Lemmatisierung heißt das Wurzelwort Lemma.**\n",
    "\n",
    "Ein Lemma (Plural Lemmata) ist die kanonische Form, Wörterbuchform oder Zitierform einer Menge von Wörtern. *Runs, running, ran* sind zum Beispiel alle Formen des Wortes *run*, daher ist *run* das Lemma all dieser Wörter. Da die Lemmatisierung ein tatsächliches Wort der Sprache zurückgibt, wird sie dort verwendet, wo es notwendig ist, gültige Wörter zu erhalten. [3]\n",
    "\n",
    "Die Python NLTK-Bibliothek bietet den `WordNet` Lemmatizer, der die WordNet-Datenbank [4] verwendet, um die Lemmata der Wörter nachzuschlagen.\n",
    "\n",
    "\n",
    "**Welche Methode (Stemmer oder Lemmatizer) soll verwendet werden?** Die Stemmer-Methode ist viel schneller als die Lemmatisierung, aber sie liefert im Allgemeinen schlechtere Ergebnisse. Die Verwendung von Stemming kann dazu führen, dass der generierte Stamm kein richtiges Wort ist, da kein Korpus verwendet wird. \n",
    "Es hängt ein wenig von dem aktuellen Problem ab, an dem Sie arbeiten. Wenn Sie eine Sprachanwendung erstellen, bei der Sprache und Grammatik wichtig sind, dann ist der Lemmatisierungsansatz besser als der Stemming-Ansatz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_5_'></a>[](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example text\n",
    "text = \"\"\"In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, for example in language modeling, parsing, and many others. This is increasingly important in medicine and healthcare, where NLP is being used to analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_6_'></a>[](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenizer --> split the text in single words\n",
    "nltk.word_tokenize(text)[0:10]  # show the first 10 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_7_'></a>[](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Snowball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# print out all actual supported languaged\n",
    "SnowballStemmer.languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# porter stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "plurals = [\n",
    "    \"caresses\",\n",
    "    \"flies\",\n",
    "    \"dies\",\n",
    "    \"mules\",\n",
    "    \"denied\",\n",
    "    \"died\",\n",
    "    \"agreed\",\n",
    "    \"owned\",\n",
    "    \"humbled\",\n",
    "    \"sized\",\n",
    "    \"meeting\",\n",
    "    \"starting\",\n",
    "    \"siezing\",\n",
    "    \"itemization\",\n",
    "    \"sensational\",\n",
    "    \"traditional\",\n",
    "    \"reference\",\n",
    "    \"colonizer\",\n",
    "    \"plotted\",\n",
    "    \"tome\",\n",
    "    \"amazing\",\n",
    "]\n",
    "\n",
    "# print some examples of the stems\n",
    "for word in plurals:\n",
    "    print(f\"{word} --> {porter_stemmer.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# create an instance of the Snowball stemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# print some examples of the stems\n",
    "for word in plurals:\n",
    "    print(f\"{word} --> {snowball_stemmer.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(porter_stemmer.stem(\"fairly\"))\n",
    "print()\n",
    "print(snowball_stemmer.stem(\"fairly\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergleicht man die beiden Stemmer (Porter- und Snowball-Stemmer), so wird deutlich, dass sie bei dem oben verwendeten Wort identische Ergebnisse liefern. Für das Wort \"fairly\" funktioniert der Schneeballstemmer besser und liefert den richtigen Stamm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_8_'></a>[](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for word in plurals:\n",
    "    print(f\"{word} --> {lemmatizer.lemmatize(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize(\"fairly\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[](#toc0_)\n",
    "### <a id='toc1_4_1_'></a>[](#toc0_)\n",
    "\n",
    "In der NLP ist die Worteinbettung eine Projektion eines Wortes, das aus Zeichen besteht, in aussagekräftige Vektoren aus reellen Zahlen. Konzeptionell handelt es sich um eine mathematische Einbettung von einer Dimension N (alle Wörter in einem bestimmten Korpus) - oft wird eine einfache One-Hot-Codierung verwendet - in einen kontinuierlichen Vektorraum mit einer viel geringeren Dimensionalität, typischerweise werden 128 oder 256 Dimensionen verwendet. Die Worteinbettung ist ein entscheidender Vorverarbeitungsschritt für das Training eines neuronalen Netzes.\n",
    "\n",
    "Es gibt viele verschiedene Ansätze für die Worteinbettung, die in den folgenden Abschnitten dieses Jupyter-Notizbuchs erläutert werden.\n",
    "\n",
    "**Es gibt zwei Haupteigenschaften für eine sinnvolle Projektion:**\n",
    "\n",
    "   - Verteilte Darstellung von Konzepten und Kontinuität von Wörtern mit ähnlichen Eigenschaften\n",
    "   - Ermöglicht das Erlernen dieser Projektion für die gegebene Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/EmbeddingOverview.jpg\" width=\"1000\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.1 - Illustration of a word embedding [5]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[](#toc0_)\n",
    "\n",
    "Einseitig kodierte Wörter sind nicht geeignet, um eine NLP-Aufgabe erfolgreich zu lösen. Nehmen wir an, ein beliebiges Wörterbuch besteht aus 5000 verschiedenen Wörtern. Das bedeutet, dass bei der Verwendung von One-Hot-Kodierung jedes Wort durch einen Vektor der Länge 5000 repräsentiert wird, aber 4999 dieser Einträge sind Null. Daraus lässt sich schließen, dass die Dimensionalität sehr hoch wird und der Merkmalsraum sehr spärlich ist. Außerdem gibt es keine Verbindung von Wörtern mit ähnlicher Bedeutung, wie in Abbildung 1 oben zu sehen ist. [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/OneHotEncoding.png\" width=\"700\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.2 - Example of a one-hot encoding for words [7]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_3_'></a>[](#toc0_)\n",
    "\n",
    "Ein Standardansatz besteht darin, die one-hot kodierten Token (meist Wörter oder Sätze) in eine Einbettungsschicht einzugeben. Während des Trainings versucht das Modell, eine geeignete Einbettung zu finden (niedrigere Dimensionalität als die Eingabeschicht). Die Position eines Wortes innerhalb des Vektorraums wird aus dem Text gelernt und basiert auf den Wörtern, die das Wort umgeben, wenn es verwendet wird. In einigen Fällen kann es nützlich sein, eine vortrainierte Einbettung zu verwenden, die auf einem grossen Wortkorpus trainiert wurde. Abbildung 3 zeigt eine schematische Architektur einer wortbasierten Einbettungsschicht.\n",
    "\n",
    "- **Eingabe:** Ein-Hot-Codierung des Wortes in einem Vokabular\n",
    "- **Ausgabe:** ein Vektor mit N Dimensionen (vom Benutzer vorgegeben, wahrscheinlich mit Hyperparameter-Tuning)\n",
    "\n",
    "Es gibt viele verschiedene Ansätze für die Einbettung von Wörtern, einige der populärsten werden wir in den folgenden Abschnitten näher betrachten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/EmbeddingLayer.jpeg\" width=\"600\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.3 - Visualization of a embedding layer [8]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_4_'></a>[](#toc0_)\n",
    "\n",
    "Es gibt drei Hauptoptionen, um eine Einbettungsschicht zu trainieren, und es hängt von dem Problem der Verarbeitung natürlicher Sprache ab, welcher Ansatz gewählt werden sollte, um das Problem zu lösen.\n",
    "\n",
    "**Drei Hauptoptionen für das Training einer Einbettungsschicht:**\n",
    "\n",
    "   - Trainieren Sie die Gewichte der Einbettungsschicht von Grund auf für eine bestimmte Aufgabe.\n",
    "   - Verwendung einer vortrainierten Einbettung wie **word2vec, GloVe, FastText, ELMo**, (diese Ansätze werden später noch genauer erläutert).\n",
    "   - Ähnlicher Ansatz wie beim Transferlernen: Nehmen Sie eine vortrainierte Einbettung und passen Sie sie durch Training (mit einer sehr kleinen Lernrate, z.B. `1e-5`) für die gegebene Aufgabe an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_5_'></a>[](#toc0_)\n",
    "\n",
    "PyTorch bietet eine Einbettungsschicht (`nn.Embedding`), die für neuronale Netze wie RNNs (rekurrente neuronale Netze) oder CNNs auf Textdaten angewendet werden kann. Diese Schicht wird üblicherweise als erste Schicht einer komplexeren Architektur verwendet. Die `Embedding`-Schicht erwartet mindestens zwei Eingabeparameter:\n",
    "\n",
    "- `num_embeddings`: Ganzzahlig. Die Grösse des Vokabulars, d.h. der maximale Integer-Index + 1.\n",
    "- `embedding_dim`: Ganzzahlig. Die Dimension der dichten Einbettung.\n",
    "\n",
    "Die Länge der Eingabesequenzen (`input_length`) muss nicht explizit angegeben werden, da PyTorch die Eingabeform automatisch erkennt. Falls nach der Einbettung jedoch ein `Linear`-Layer folgt, sollte die Sequenzlänge beim Design berücksichtigt werden (z. B. durch Flattening).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_6_'></a>[](#toc0_)\n",
    "\n",
    "Das folgende Beispiel basiert auf einem Blogeintrag von Jason Brownlee in Machinelearningmastery: [10]\n",
    "\n",
    "Wir erstellen einen kleinen Korpus, der aus 10 Dokumenten bzw. Sätzen besteht. Jeder Text wurde manuell in zwei Klassen, positiv (1) und negativ (0), eingeteilt. Dies ist ein sehr kleines Beispiel für ein Problem der Textklassifizierung oder Stimmungsanalyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's define our documents\n",
    "docs = [\n",
    "    \"Well done!\",\n",
    "    \"Good work\",\n",
    "    \"Great effort\",\n",
    "    \"nice work\",\n",
    "    \"Excellent!\",\n",
    "    \"Perfect!\",\n",
    "    \"really good\",\n",
    "    \"Weak\",\n",
    "    \"Poor effort!\",\n",
    "    \"not good\",\n",
    "    \"poor work\",\n",
    "    \"Could have done better.\",\n",
    "]\n",
    "\n",
    "# in this example we try to classify the documents in two classes (positiv and negative)\n",
    "labels = np.array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Der erste Schritt ist nun, jedes Dokument in eine geeignete Interger-Kodierung zu konvertieren. Der einfachste Ansatz ist die bekannte One-Hot-Kodierung. Es gibt auch einige anspruchsvollere Ansätze wie TF-IDF (term frequency - inverse document frequency), aber wir werden in diesem Notizbuch nicht näher darauf eingehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "# Hash-based one-hot encoding (only indices)\n",
    "def torch_one_hot_hash(text, vocab_size):\n",
    "    words = text.lower().split()\n",
    "    hashed = [\n",
    "        (int(hashlib.md5(w.encode()).hexdigest(), 16) % (vocab_size - 1)) + 1\n",
    "        for w in words\n",
    "    ]\n",
    "    return hashed\n",
    "\n",
    "\n",
    "# Encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [torch_one_hot_hash(doc, vocab_size) for doc in docs]\n",
    "print(encoded_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Offensichtlich haben die verschiedenen kodierten Dokumente unterschiedliche Längen. PyTorch bevorzugt die gleiche Länge der Eingabe für alle Dokumente. Daher können wir die Funktion `pad_sequence` aus `torch.nn.utils.rnn` verwenden, um die Sequenzen aufzufüllen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "max_length = 4\n",
    "# encoded_docs is a list of lists with integers (e.g., generated via hashing)\n",
    "# We need to convert them into LongTensors\n",
    "encoded_docs_tensors = [torch.tensor(doc, dtype=torch.long) for doc in encoded_docs]\n",
    "\n",
    "# Padding post: All sequences are padded with 0 at the end\n",
    "padded_docs = pad_sequence(encoded_docs_tensors, batch_first=True, padding_value=0)\n",
    "\n",
    "# Truncating: Trim to max_length (if longer)\n",
    "if padded_docs.size(1) < max_length:\n",
    "    # Add extra padding if shorter\n",
    "    pad_len = max_length - padded_docs.size(1)\n",
    "    extra_pad = torch.zeros((padded_docs.size(0), pad_len), dtype=torch.long)\n",
    "    padded_docs = torch.cat((padded_docs, extra_pad), dim=1)\n",
    "else:\n",
    "    padded_docs = padded_docs[:, :max_length]\n",
    "\n",
    "print(padded_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In einem nächsten Schritt erstellen wir eine einfache ANN-Architektur (künstliches neuronales Netz).\n",
    "- Als letzte Schicht benötigen wir eine dichte Schicht mit nur einem Neuron und einer Sigmoid-Aktivierungsfunktion für die binäre Klassifizierung. \n",
    "- Wir können auch eine dichte Schicht mit zwei Neuronen nehmen, aber dann benötigen wir eine Softmax-Aktivierung anstelle der Sigmoid-Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BinaryClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length):\n",
    "        super(BinaryClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(embedding_dim * input_length, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "embedding_dim = 8\n",
    "model = BinaryClassificationModel(vocab_size, embedding_dim, max_length)\n",
    "\n",
    "# Print the model summary\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(1, max_length),\n",
    "    dtypes=[torch.long],\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# Ensure the dummy input tensor is on the same device as the model\n",
    "device = next(model.parameters()).device\n",
    "dummy_input = torch.randint(0, vocab_size, (1, max_length), dtype=torch.long).to(device)\n",
    "\n",
    "# Generate a visualization of the model\n",
    "model_graph = make_dot(model(dummy_input), params=dict(model.named_parameters()))\n",
    "\n",
    "# Save the visualization to a file or render it\n",
    "model_graph.render(\"model_architecture\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Convert labels to a PyTorch tensor and move it to the same device as the model\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Move padded_docs to the same device as the model\n",
    "padded_docs = padded_docs.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(padded_docs)\n",
    "    loss = criterion(outputs, labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(padded_docs)\n",
    "    predictions = (outputs > 0.5).float()\n",
    "    accuracy = (predictions == labels_tensor).float().mean()\n",
    "    print(\"Accuracy: %f\" % (accuracy.item() * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die resultierende Genauigkeit zeigt, dass das Modell innerhalb von 50 Epochen lernen konnte, die beiden Klassen zu 100 % richtig vorherzusagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_7_'></a>[](#toc0_)\n",
    "\n",
    "`Word2Vec` bezieht sich auf eine Art von Modellen, die eine Einbettung von Wörtern in den Raum mit kontextueller Ähnlichkeit erzeugen, d.h. Wörter, die gemeinsame Kontexte haben, werden in unmittelbarer Nähe angeordnet. In den nächsten Abschnitten werden wir die drei Hauptarchitekturen des word2Vec-Ansatzes erläutern. Für weitere Details verweisen wir auf die Originalarbeiten. [11][12]\n",
    "\n",
    "Eine Hypothese des `word2vec`-Ansatzes ist, dass ein Wort durch seinen \"Kontext\" repräsentiert wird, oder anders ausgedrückt, es wird ein Fenster um das Zielwort erstellt und Wörter, die in das Fenster fallen, werden ohne Rücksicht auf die Reihenfolge in die \"Tasche\" aufgenommen. Bei der word2vec-Zuordnung geht es nicht nur darum, verwandte Wörter zu gruppieren, sondern auch darum, eine gewisse \"Pfadbedeutung\" zu erhalten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Beispiel:**\n",
    "\n",
    "$$\\boxed{\\vec{v}_{\\mathrm{women}} + (\\vec{v}_{\\mathrm{king}} - \\vec{v}_{\\mathrm{man}} ) \\sim \\vec{v}_{\\mathrm{queen}} }$$\n",
    "\n",
    "\n",
    "Für den `word2vec`-Ansatz gibt es viele verschiedene Architekturansätze:\n",
    "\n",
    "   - **Ein-Wort-Kontext:** verwendet nur ein einziges Wort als Kontext (diese Strategie wird nicht oft verwendet, hauptsächlich ein Bildungsansatz).\n",
    "   - **Skip-Gram-Modell:** Eine Erweiterung des Ein-Wort-Kontextes zu einem Mehr-Wort-Kontext am Ausgang.\n",
    "   - **Kontinuierliches Bag-of-Word-Modell:** Eine Erweiterung des Ein-Wort-Kontextes zu einem Mehr-Wort-Kontext bei der Eingabe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc1_4_8_'></a>[](#toc0_)\n",
    "\n",
    "Der **Ein-Wort-Ansatz** nimmt nur ein Wort als Eingabe und versucht, auf der Grundlage dieser Eingabe das Ausgabewort vorherzusagen. Dieser Ansatz ist in der Praxis nicht weit verbreitet, es handelt sich eher um einen pädagogischen Ansatz, da das Modell keinen Kontext in einem bestimmten Dokument lernen kann.\n",
    "\n",
    "**Lossfunktion des Ein-Wort-Kontextmodells:**\n",
    "\n",
    "$$ \\boxed{J = -\\log \\left[p(w_o|w_i) \\right] }$$\n",
    "\n",
    "Das Modell versucht einfach, den negativen Logarithmus der Wahrscheinlichkeit des Ausgabeworts bei gegebenem Eingabewort zu maximieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/OneWordContext.JPG\" width=\"500\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.4 - one word context model[13]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_9_'></a>[](#toc0_)\n",
    "\n",
    "Ein besser geeigneter Ansatz als das Ein-Wort-Kontextmodell ist das **Skip-Gram-Modell**. Dieses Modell nimmt ein Zielwort als Eingabe, um den Kontext (benachbarte Wörter) vorherzusagen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/Skip-Gram.JPG\" width=\"500\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.5 - skip-gram model [13]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verlustfunktion des Skip-Gram-Modells**\n",
    "\n",
    "Die Verlustfunktion unterscheidet sich nicht wesentlich von der Verlustfunktion des Wortkontextmodells. \n",
    "\n",
    "$$\\mathcal{L} = -\\log  p(w_{o,1},w_{o,2},...,w_{o,c}|w_{I})$$\n",
    "\n",
    "$$ \\mathcal{L} = -\\sum_{c=1}^{C}u_{j_c} + C\\cdot \\log \\left[\\sum_{j'=1}^{V} \\exp\\left({u_{j'}} \\right) \\right]$$\n",
    "\n",
    "Mehr über das mathematische Konzept hinter dem Modell finden Sie in der Originalarbeit [14]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_10_'></a>[](#toc0_)\n",
    "\n",
    "Die folgende Implementierung aus einem Tensorflow-Tutorial [15] soll helfen, den Skip-Gram-Ansatz in der Praxis zu verstehen. Wir verwenden nur einen Satz als Textkorpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Set the number of threads for data loading\n",
    "AUTOTUNE = torch.get_num_threads()\n",
    "print(f\"Number of threads for data loading: {AUTOTUNE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: tokenize the training sentence\n",
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = sentence.lower().split()  # Tokenize the sentence into words\n",
    "print(len(tokens))  # Output the number of tokens\n",
    "print(tokens)  # Display the tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a vocabulary to save mappings from tokens to integer indices.\n",
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab[\"<pad>\"] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create an inverse vocabulary to save mappings from integer indices to tokens.\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# vectorize the sentence\n",
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In PyTorch gibt es keine eingebaute Funktion dafür, aber man kann dieses Verhalten sehr einfach selbst umsetzen. Die zentrale Idee ist dabei, aus einer Sequenz von Token sogenannte (Zielwort, Kontextwort)-Paare zu erzeugen – je nach gewünschtem window_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skip_grams(sequence, window_size):\n",
    "    skip_grams = []\n",
    "    for center_idx in range(len(sequence)):\n",
    "        window_start = max(0, center_idx - window_size)\n",
    "        window_end = min(len(sequence), center_idx + window_size + 1)\n",
    "\n",
    "        for context_idx in range(window_start, window_end):\n",
    "            if context_idx != center_idx:\n",
    "                skip_grams.append((sequence[center_idx], sequence[context_idx]))\n",
    "    return skip_grams\n",
    "\n",
    "\n",
    "positive_skip_grams = generate_skip_grams(example_sequence, window_size=2)\n",
    "\n",
    "print(len(positive_skip_grams))\n",
    "print(positive_skip_grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# show some positive samples\n",
    "for target, context in positive_skip_grams[:5]:\n",
    "    print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die Funktion skipgrams gibt alle positiven Skip-Gram-Paare zurück, indem sie über ein bestimmtes Fenster gleitet. Ein positives Skip-Gram besteht aus einem Zielwort und einem Wort aus dessen Kontextfenster.\n",
    "\n",
    "Um das Modell robuster zu machen, werden zusätzlich auch negative Skip-Grams verwendet – also Wortpaare, die im Kontext nicht zusammengehören. Dafür wählt man zufällig Wörter aus dem Vokabular aus, die nicht der echte Kontext zum Zielwort sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "print(\"target word:\", target_word)\n",
    "print(\"context word:\", context_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def negative_sampling(context_word, num_ns, vocab_size, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Alle möglichen Kandidaten, außer dem echten Kontextwort\n",
    "    candidates = torch.tensor(\n",
    "        [i for i in range(vocab_size) if i != context_word], dtype=torch.long\n",
    "    )\n",
    "\n",
    "    # Zufällige Auswahl ohne Wiederholung (unique=True)\n",
    "    negative_samples = candidates[torch.randperm(len(candidates))[:num_ns]]\n",
    "\n",
    "    return negative_samples\n",
    "\n",
    "\n",
    "vocab_size = 8\n",
    "num_ns = 4\n",
    "seed = 42\n",
    "\n",
    "# Negatives samplen\n",
    "neg_samples = negative_sampling(context_word, num_ns, vocab_size, seed)\n",
    "\n",
    "print(neg_samples)  # Tensor mit negativen Wort-IDs\n",
    "\n",
    "\n",
    "print([inverse_vocab[idx.item()] for idx in neg_samples])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Für ein gegebenes positives (`target_word, context_word`) Skip-Gramm gibt es nun auch `num_ns` negative gesampelte Kontext-Wörter, die nicht in der Nachbarschaft von `target_word` erscheinen.\n",
    "- Wir fassen das eine positive Kontextwort und `num_ns` negative Kontextwörter in einem Tensor zusammen. \n",
    "- Dies ergibt einen Satz positiver Skip-Gramme (gekennzeichnet mit 1) und negativer Stichproben (gekennzeichnet mit 0) für jedes Zielwort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Add a dimension so you can use concatenation (on the next step).\n",
    "negative_sampling_candidates = neg_samples.unsqueeze(1).to(device)\n",
    "\n",
    "# Concat positive context word with negative sampled words.\n",
    "context = torch.cat(\n",
    "    [torch.tensor([[context_word]], device=device), negative_sampling_candidates], dim=0\n",
    ")\n",
    "\n",
    "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n",
    "label = torch.tensor([1] + [0] * num_ns, dtype=torch.int64, device=device)\n",
    "\n",
    "# Reshape target to shape (1,) and context and label to (num_ns+1,).\n",
    "target = torch.tensor([target_word], device=device).squeeze()\n",
    "context = context.squeeze()\n",
    "label = label.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the context and the corresponding labels for the target word from the skip-gram example above.\n",
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[int(c.cpu())] for c in context]}\")\n",
    "print(f\"label           : {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ein Tupel von `(target, context, label)` Tensoren stellt ein Trainingsbeispiel für das Training des **Skip-Gram Negative Sampling Word2Vec Modells** dar. Beachten Sie, dass das Ziel die Shape `(1,)` hat, während der Kontext und das Label die Form `(1+num_ns,)` haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target  :\", target)\n",
    "print(\"context :\", context)\n",
    "print(\"label   :\", label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_11_'></a>[](#toc0_)\n",
    "\n",
    "Beim kontinuierlichen Bag-of-Word-Ansatz (CBOW) versucht das Modell, das eigentliche Wort auf der Grundlage einiger umliegender Wörter vorherzusagen, in diesem Sinne ist es der umgekehrte Ansatz des Skip-Gram-Modells. Die Reihenfolge der umgebenden Wörter hat keinen Einfluss auf die Vorhersage (daher: bag-of-words).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/cbow.png\" width=\"600\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.6 - Continuous bag-of-word model [13]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verlustfunktion (loss) des kontinuierlichen Bag-of-Word-Modells**\n",
    "\n",
    "Die Verlustfunktion ist ähnlich wie die Verlustfunktion des Ein-Wort-Modells. Der einzige Unterschied besteht darin, dass mehr als ein Wort gegeben ist.\n",
    "\n",
    "$$\\mathcal{L} = -\\log\\,p(w_o|w_{I,1},...,w_{I,C})$$\n",
    "\n",
    "$$\\mathcal{L} = -u_{j*} + \\log \\left[ \\sum_{j'=1}^{V} \\exp(u_{j'}) \\right]$$\n",
    "\n",
    "$$\\mathcal{L} = -v_{w_0}^T \\cdot h + \\log \\sum_{j'=1}^{V} \\exp(v_{w_j}^T \\cdot h)$$\n",
    "\n",
    "Mehr über das mathematische Konzept, das dem Modell zugrunde liegt, finden Sie in der Originalarbeit [14]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_12_'></a>[](#toc0_)\n",
    "\n",
    "Ein weiterer, fortschrittlicherer Ansatz für die Worteinbettung ist das GloVe-Modell. GloVe bedeutet globale Vektoren für die Wortdarstellung. Dieser Ansatz basiert nicht auf einem reinen ANN-Ansatz (künstliches neuronales Netz), sondern auch auf statistischen Ansätzen. Der GloVe-Ansatz kombiniert im Wesentlichen die Vorteile des word2vec-Ansatzes und des LSA-Ansatzes. LSA bedeutet Latent Semantic Analysis und war einer der ersten Ansätze zur Vektoreinbettung von Wörtern.\n",
    "\n",
    "GloVe ist ein unüberwachter Lernalgorithmus zur Gewinnung von Vektordarstellungen für Wörter. Das Training erfolgt auf der Grundlage aggregierter globaler Wort-Wort-Ko-Okzidenz-Statistiken aus einem Korpus, und die resultierenden Repräsentationen zeigen interessante lineare Substrukturen des Wortvektorraums. [16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_13_'></a>[](#toc0_)\n",
    "\n",
    "Zunächst einige grundlegende Notationen:\n",
    "\n",
    "- $X$ ist die Matrix der Wort-Wort-Co-Occurrence. Die Matrix ist quadratisch und hat die Form der Anzahl der Wörter im Korpus.\n",
    "- $X_i = \\sum(X_{ik})$ -> Die Anzahl, wie oft ein beliebiges Wort im Kontext des Wortes i erscheint.\n",
    "- $P_{ij} = P(i|j) = X_{ij}/X_i$ -> die Wahrscheinlichkeit, dass Wort j im Kontext von Wort i vorkommt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bei einem Korpus mit $V$ Wörtern hat die Kookkurenzmatrix $X$ die Form von $V\\times V$, wobei die i-te Zeile und j-te Spalte von $X,X_{ij}$ angibt, wie oft das Wort $i$ mit dem Wort $j$ koinzident ist.\n",
    "\n",
    "Als einfaches Beispiel verwenden wir den folgenden Satz: \n",
    "\n",
    "**\"the cat sat on the mat\".**\n",
    "\n",
    "Wir verwenden eine Fenstergrösse von \"eins\" für dieses Beispiel, aber es ist auch möglich / sinnvoll, eine größere Fenstergröße zu verwenden. Abbildung 7 zeigt die resultierende **Koinzidenzmatrix** des obigen Beispielsatzes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/cooccurancematrix.png\" width=\"600\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.7 - Example of a simple Co-occurence matrix [17]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Um die Ähnlichkeit zwischen Wörtern zu messen, benötigen wir jeweils drei Wörter. Die folgende Tabelle zeigt ein solches Beispiel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/GloVe_Probability.jpg\" width=\"600\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.8 - GloVe probability table [18]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die obige Tabelle zeigt die Koinzidenzwahrscheinlichkeiten für die Zielwörter Eis und Dampf mit ausgewählten Kontextwörtern aus einem Korpus von sechs Milliarden Token. Nur im Verhältnis hebt sich das Rauschen von nicht-diskriminierenden Wörtern wie Wasser und Mode\n",
    "aus, so dass große Werte (viel grösser als eins) gut mit eisspezifischen Eigenschaften und kleine Werte (viel kleiner als eins) gut mit dampfspezifischen Eigenschaften korrelieren.\n",
    "\n",
    "Sie können sehen, dass bei zwei Wörtern, d. h. Eis und Dampf, das dritte Wort k (auch \"Sondenwort\" genannt):\n",
    "\n",
    "- dem Eis sehr ähnlich, aber für Dampf irrelevant ist (z. B. k=fest), wird $P_{ik}/P_{jk}$ sehr hoch sein (>1),\n",
    "- ist dem Dampf sehr ähnlich, aber für Eis irrelevant (z. B. k=Gas), $P_{ik}/P_{jk}$ wird sehr klein sein (<1),\n",
    "- mit einem der beiden Begriffe verwandt oder nicht verwandt ist, dann wird $P_{ik}/P_{jk}$ nahe bei 1 liegen.\n",
    "    \n",
    "Wenn wir also einen Weg finden, $P_{ik}/P_{jk}$ in die Berechnung von Wortvektoren einzubeziehen, dann erreichen wir das Ziel, beim Lernen von Wortvektoren globale Statistiken zu verwenden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_14_'></a>[](#toc0_)\n",
    "\n",
    "Es kann gezeigt werden, dass ein geeigneter Ausgangspunkt für das Lernen von Wortvektoren die Arbeit mit Verhältnissen von Koinzidenzwahrscheinlichkeiten anstelle der Wahrscheinlichkeiten selbst sein könnte. Wir nehmen die folgende Gleichung als Ausgangspunkt:\n",
    "\n",
    "$$F(w_i, w_j, \\tilde{w_k}) = \\frac{P_{ik}}{P_{jk}}$$\n",
    "\n",
    "In der obigen Formel kann $F()$ als eine komplizierte Funktion aufgefasst werden, die durch ein neuronales Netz parametrisiert ist. Das Verhältnis $P_{ik}/P_{jk}$ hängt von drei Wörtern $i$,$j$ und $k$ ab. In der obigen Formel sind $w$ die Wortvektoren und $\\tilde{w}$ sind die einzelnen Kontextvektoren.\n",
    "\n",
    "Nach einigen Umrechnungsschritten kommen wir zu folgender Formel:\n",
    "\n",
    "$$w_i^T \\cdot \\tilde{w_k} + b_i + \\tilde{b_k} = \\log(X_{ik})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Ein wesentlicher Nachteil dieses Modells besteht darin, dass es alle Kookkurrenzen gleichmässig gewichtet, auch solche, die selten oder nie auftreten. \n",
    "- Solche seltenen Ko-Okkurrenzen sind verrauscht und enthalten weniger Informationen als die häufigeren - doch selbst die Nulleinträge machen 75-95% der Daten in X aus, je nach Grösse des Vokabulars und des Korpus. D\n",
    "- Die Autoren des GloVe-Beitrags [18] haben ein neues gewichtetes Kleinste-Quadrate-Regressionsmodell vorgeschlagen, das diese Probleme angeht. Die Gewichtungsfunktion $f(X_{ij})$ ist in Abbildung 9 dargestellt.\n",
    "\n",
    "**Verlustfunktion des GloVe-Modells**\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i,j=1}^{V} f(X_{ij})(w_i^T\\tilde{w_j}+b_i+\\tilde{b_j}-\\log(X_{ij}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/wheightfunction.png\" width=\"500\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.9 - Example of the weightfunction $f(X_{ij})$ [18]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_15_'></a>[](#toc0_)\n",
    "\n",
    "Glove basiert auf Matrixfaktorisierungsverfahren für die Wort-Kontext-Matrix, auch bekannt als Co-Occurrence-Matrix. Zunächst wird eine große Matrix von (Wörter - Kontext) Ko-Okzidenz-Informationen erstellt (die violette Matrix in Abbildung 10 unten), d.h. für jedes \"Wort\" (die Zeilen) wird gezählt, wie häufig wir dieses Wort in einem \"Kontext\" (die Spalten) in einem großen Korpus sehen. Die Zahl der \"Kontexte\" ist natürlich groß, da sie im Wesentlichen kombinatorisch ist. Die Idee ist nun, die Matrix durch Faktorisierung zu approximieren, wie in der folgenden Abbildung 10 dargestellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Wir faktorisieren also diese Matrix, um eine niederdimensionale Matrix (Wort - Merkmale) zu erhalten (die orangefarbene Matrix in Abbildung 10 unten), wobei nun jede Zeile eine Vektorrepräsentation für das entsprechende Wort ergibt. Im Allgemeinen wird dies durch Minimierung eines \"Rekonstruktionsverlustes\" erreicht. Mit diesem Verlust wird versucht, die niederdimensionalen Repräsentationen zu finden, die den größten Teil der Varianz in den hochdimensionalen Daten erklären können. [19]\n",
    "\n",
    "Meistens werden die Wort-Merkmal-Matrix und die Merkmal-Kontext-Matrix zufällig initialisiert und es wird versucht, sie zu multiplizieren, um eine Wort-Kontext-Co-Occurrence-Matrix zu erhalten, die der ursprünglichen Matrix so ähnlich wie möglich ist. Nach dem Training liefert die Wort-Merkmal-Matrix die gelernte Wort-Einbettung für jedes Wort, wobei die Anzahl der Spalten (Merkmale) bis zu einer bestimmten Anzahl von Dimensionen, die vom Benutzer als Hyperparameter angegeben werden, vorhanden sein kann. [20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/GloVe_Martices.png\" width=\"800\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.10 - Conceptual model for the GloVe model [20]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[](#toc0_)\n",
    "\n",
    "Der Ansatz der Squence-to-Sequence-Modelle, oder kurz Seq2seq-Modelle, wurde 2014 von Google eingeführt [31]. Ziel ist es, einen Eingangsvektor mit fester Länge auf einen Ausgangsvektor mit ebenfalls fester Länge abzubilden, wobei die Länge von Eingang und Ausgang unterschiedlich sein kann. Das Modell besteht im Wesentlichen aus einem Encoder-Teil (eine RNN- oder LSTM-Schicht) und einem Decoder-Teil (eine weitere RNN- oder LSTM-Schicht)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_1_'></a>[](#toc0_)\n",
    "\n",
    "Ein Encoder verarbeitet die Eingangssequenz und komprimiert die Informationen in einen Kontextvektor $h_t$ fester Länge. Der Decoder wird mit dem Kontextvektor $h_t$ initialisiert, um die transformierte Ausgabe auszugeben. Der Encoder besteht aus einer Einbettungsschicht, gefolgt von einer RNN-Schicht oder einer LSTM-Schicht. Die Stärke dieses Modells liegt darin, dass es Sequenzen unterschiedlicher Länge (Eingangs- und Ausgangsvektor) abbilden kann.\n",
    "\n",
    "Ein einfaches Seq2Seq-Modell kann bei kurzen Textsequenzen recht gut funktionieren, hat aber Schwierigkeiten bei langen Sequenzen, da der Kontextvektor eine feste Länge hat und viele Informationen kodieren muss. Eine mögliche Lösung, um das Problem langer Sequenzen zu überwinden, besteht darin, den Aufmerksamkeitsmechanismus einzubauen. In Abschnitt 5 wird der Aufmerksamkeitsmechanismus ausführlicher erläutert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/seq2seq.png\" width=\"800\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.17 - Sequence to sequence architecture [32]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_2_'></a>[](#toc0_)\n",
    "\n",
    "Das folgende Beispiel zeigt ein einfaches Beispiel für ein Seq2Seq-Modell, das mit PyTorch implementiert wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim=32, hidden_dim_enc=128, hidden_dim_dec=128, output_dim=32\n",
    "    ):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "\n",
    "        # Encoder-LSTM\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_dim, hidden_size=hidden_dim_enc, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Decoder-LSTM (bekommt Encoder-State)\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=input_dim, hidden_size=hidden_dim_dec, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Dense Layer auf Decoder-Ausgabe\n",
    "        self.fc = nn.Linear(hidden_dim_dec, output_dim)\n",
    "\n",
    "        # Softmax\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        # Encoder → gibt nur hidden+cell zurück\n",
    "        _, (hidden, cell) = self.encoder(encoder_input)\n",
    "\n",
    "        # Decoder mit initial_state = encoder_states\n",
    "        decoder_output, _ = self.decoder(decoder_input, (hidden, cell))\n",
    "\n",
    "        # Dense Layer + Softmax\n",
    "        output = self.fc(decoder_output)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Beispiel zur Initialisierung des Modells\n",
    "input_sequence_len = 8\n",
    "output_sequence_len = 8\n",
    "hidden_dim_enc = 16\n",
    "hidden_dim_dec = 16\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    input_sequence_len, output_sequence_len, hidden_dim_enc, hidden_dim_dec\n",
    ")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Seq2Seq model\n",
    "simpleSeq2Seq = Seq2SeqModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Beispielhafte Input-Shape (batch_size, seq_len)\n",
    "summary(simpleSeq2Seq)  # z.B. batch_size=32, sequence length=10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[](#toc0_)\n",
    "\n",
    "- In der bisherigen Architektur (Sequenz-zu-Sequenz-Modelle) komprimierte der Kodierer den gesamten Quellensatz in einen einzigen Vektor, den so genannten Kontextvektor $h_t$. \n",
    "- Dies kann sehr schwierig sein - die Anzahl der möglichen Bedeutungen der Quelle ist unendlich. \n",
    "- Wenn der Kodierer gezwungen ist, alle Informationen in einen einzigen Vektor zu packen, wird er wahrscheinlich etwas vergessen. \n",
    "- Es ist nicht nur für den Kodierer schwer, alle Informationen in einen einzigen Vektor zu packen - es ist auch für den Dekodierer schwer, alle wichtigen Informationen aus dem Kontextvektor $h_t$ zu extrahieren. \n",
    "- Der Dekoder sieht nur eine Darstellung der Quelle. Bei jedem Generierungsschritt können jedoch verschiedene Teile der Quelle nützlicher sein als andere. In der gegenwärtigen Situation muss der Decoder jedoch relevante Informationen aus derselben festen Darstellung extrahieren. [33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Aufmerksamkeitsmechanismus (*attention mechanism*) ist ein Teil eines neuronalen Netzes. Bei jedem Decoderschritt entscheidet er, welche Teile der Quelle für den Decoder wichtig sind. In diesem Fall muss der Kodierer nicht die gesamte Quelle in einen einzigen Vektor komprimieren - er gibt Repräsentationen für alle Quell-Token (z. B. alle RNN-Zustände anstelle des letzten)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_1_'></a>[](#toc0_)\n",
    "\n",
    "Der Aufmerksamkeitsmechanismus ist ein Teil bzw. eine zusätzliche Schicht eines neuronalen Netzes. Bei jedem Decoderschritt entscheidet die Aufmerksamkeitsschicht, welche Teile der Eingabe wichtiger sind als andere. Der Aufmerksamkeitsmechanismus erhält bei jedem Decoderschritt den aktuellen Decoderzustand $h_t$ und alle Encoderzustände $s_1,s_2,...,s_m$. Für jeden Kodiererzustand ermittelt der Aufmerksamkeitsmechanismus die \"Relevanz\" für einen bestimmten Dekodiererzustand. Die folgenden Abbildungen 18 bis 24 zeigen eine visuelle Veranschaulichung des Aufmerksamkeitsmechanismus in einem Sequenz-zu-Sequenz-Modell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/att_overview.png\" width=\"700\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.18 - illustrated attention overview [34]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_1_1_'></a>[Step 0 Prepare hidden states](#toc0_)\n",
    "\n",
    "Bereiten Sie alle versteckten Zustände des Encoders (grün) und den ersten versteckten Zustand des Decoders (rot) vor [34]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/step_0.gif\" width=\"700\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.19 - illustrated attention step 0 [34]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <a id='toc1_6_1_2_'></a>[Step 1 Obtain a score for every encoder hidden state](#toc0_)\n",
    "Ein Score (Skalarwert) wird durch eine Scoring-Funktion, auch Alignment-Score-Funktion oder Alignment-Modell genannt, ermittelt. [34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/step_1.gif\" width=\"700\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.20 - illustrated attention step 1 [34]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_1_3_'></a>[Step 2 Run all the scores through a softmax layer](#toc0_)\n",
    "Legen Sie die Ergebnisse in eine Softmax-Schicht ein, so dass sich die Softmax-Werte zu eins addieren. Diese softmaxed Scores stellen die Aufmerksamkeitsverteilung dar. [34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/step_2.gif\" width=\"700\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.21 - illustrated attention step 2 [34]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_1_4_'></a>[Step 3 Multiply each encoder hidden state by its softmaxed score](#toc0_)\n",
    "Durch Multiplikation jedes versteckten Zustands des Encoders mit seiner softmaximierten Punktzahl (Skalar) erhalten wir den Ausrichtungsvektor. Dies ist genau der Mechanismus, bei dem die Ausrichtung (Aufmerksamkeit) stattfindet. [34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/step_3.gif\" width=\"700\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.22 - illustrated attention step 3 [34]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_1_5_'></a>[Step 4 Sum up the alignment (attention) vectors](#toc0_)\n",
    "Die Ausrichtungsvektoren werden summiert und ergeben den Kontextvektor. Ein Kontextvektor ist eine aggregierte Information der Alignment-Vektoren aus dem vorherigen Schritt. [34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/step_4.gif\" width=\"700\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.23 - illustrated attention step 4 [34]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_1_6_'></a>[Step 5 Feed the context vector into the decoder](#toc0_)\n",
    "Die Art und Weise, wie dies geschieht, hängt vom Architekturdesign ab. Im nächsten Schritt wird der verborgene Zustand des Decoders verwendet, um den nächsten Kontextvektor neu zu berechnen. [34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/step_5.gif\" width=\"700\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.24 - illustrated attention step 5 [34]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_2_'></a>[](#toc0_)\n",
    "Es gibt hauptsächlich drei verschiedene Methoden zur Berechnung von Aufmerksamkeitswerten:\n",
    "- **Skalarprodukt - die einfachste Methode**\n",
    "    - Scoring-Funktion: \n",
    "    $$\\mathrm{score}(h_t,s_k) = h^T_ts_k$$\n",
    "- **bilineare Funktion (auch bekannt als \"Luong-Aufmerksamkeit\")** weitere Details finden sich in der Originalarbeit [35]\n",
    "    - Scoring-Funktion: \n",
    "    $$\\mathrm{score}(h_t,s_k) = h^T_tWs_k$$\n",
    "- **multi-layer perceptron (\"Bahdanau attention\")** weitere Details finden sich in der Originalarbeit [36]\n",
    "    - Scoring-Funktion: \n",
    "    $$\\mathrm{score}(h_t,s_k) = w^T_2 \\cdot \\tanh(W_1[h_t,s_k])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_3_'></a>[](#toc0_)\n",
    "\n",
    "Der Kodierer des bahdanau-Modells besteht aus zwei RNN-Schichten, einer Vorwärts- und einer Rückwärtsschicht (bidirektional), die die Eingaben in beide Richtungen lesen. Für jedes Token werden die Zustände der beiden RNNs miteinander verknüpft. Um eine Aufmerksamkeitsbewertung zu erhalten, wird ein mehrschichtiges Perzeptron (MLP) auf einen Encoder- und einen Decoder-Zustand angewendet. Die Aufmerksamkeit wird zwischen den Decoderschritten verwendet: Der Zustand $h_{t-1}$ wird zur Berechnung der Aufmerksamkeit und ihrer Ausgabe $c^t$ verwendet, und sowohl $h_{t-1}$ als auch $c^t$ werden im Schritt t an den Decoder weitergegeben.\n",
    "\n",
    "Abbildung 25 unten zeigt eine schöne Übersicht über den bidirektionalen Encoderteil (in grün und gelb) und den Decoderteil (rot). Auf der linken Seite ist die typische bahdanau'sche Bewertungsfunktion dargestellt, die oben in Abschnitt 5.2 gezeigt wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/bahdanau_model.png\" width=\"800\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.25 - Architecture of the bahdanau model [33]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_4_'></a>[](#toc0_)\n",
    "\n",
    "Der Kodierer des Luong-Modells verwendet nur eine unidirektionale RNN-Schicht. Der Aufmerksamkeitsmechanismus wird wie im Bahdanau-Modell nach dem RNN-Decoder-Schritt $t$ vor der Vorhersage angewendet. Der Zustand $h_t$ wird zur Berechnung der Aufmerksamkeit und ihrer Ausgabe $c^{(t)}$ verwendet. Das Luong-Modell verwendet eine einfachere Scoring-Funktion (bilinear) als das Bahdanau-Modell. [33] Eine Übersicht ist in Abbildung 26 unten dargestellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/luong_model.png\" width=\"800\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.26 - Architecture of the luong model [37]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_5_'></a>[](#toc0_)\n",
    "\n",
    "Der Mechanismus der **Self-Attention** wurde in der Arbeit \"Attention is all you need\" [38] vorgestellt. Dieser Ansatz ähnelt dem grundlegenden Aufmerksamkeitsmechanismus, der zuvor gezeigt wurde. Die **Self-Attention**  ist eine der Schlüsselkomponenten der Transformer-Architektur, die wir in Abschnitt 6 näher erläutern werden. Der Unterschied zwischen Aufmerksamkeit und **Self-Attention**  besteht darin, dass die Selbstaufmerksamkeit zwischen Repräsentationen gleicher Art wirkt: z. B. zwischen allen Encoderzuständen in einer Schicht. **Self-Attention**  ist der Teil des Modells, in dem Token miteinander interagieren. Jedes Token \"schaut\" sich andere Token im Satz mit einem Aufmerksamkeitsmechanismus an, sammelt Kontext und aktualisiert die vorherige Repräsentation des \"Selbst\".\n",
    "\n",
    "Formal wird diese Intuition der Selbstaufmerksamkeit mit einer Frage-Schlüssel-Wert-Aufmerksamkeit umgesetzt. Jedes Eingabe-Token einer Selbstaufmerksamkeitsschicht erhält drei Repräsentationen (Matrizen), die den Rollen entsprechen, die es spielen kann:\n",
    "\n",
    "- **Anfrage = Query** - die nach Informationen fragt\n",
    "- **Schlüssel = Key** - er sagt, dass er eine Information hat\n",
    "- **Wert = Value** - gibt die Information an\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Die Selbstbeobachtungsfunktion kann als Abbildung einer Anfrage ($W_Q$) und eines Satzes von Schlüssel-Wert-Paaren ($W_K$, $W_V$) auf eine Ausgabe beschrieben werden. Die Eingabe besteht aus einem Abfragevektor und Schlüssel-Werte-Vektoren (Dimension der Werte: $d_v$, Dimension der Schlüssel: $d_k$). Auf der Grundlage dieser drei Vektoren berechnet die Selbstaufmerksamkeitsschicht die drei Matrizen $W_Q$, $W_K$ und $W_V$. Mit diesen drei Matrizen können wir die Aufmerksamkeitsgewichte (die Ausgangsmatrix der Aufmerksamkeitsschicht) berechnen.\n",
    "\n",
    "**Formel zur Berechnung des Outputs der Selbstaufmerksamkeit:**\n",
    "\n",
    "$$\\mathrm{selfattenton}(q,k,v) = \\mathrm{softmax} \\left( \\frac{qk^t}{\\sqrt{d_k}} \\right) v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/SelfAttention.png\" width=\"600\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.27 - Overview of the self-attention mechanism [38]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_6_'></a>[](#toc0_)\n",
    "\n",
    "Der Multi-Head-Attention-Mechanismus ist eine Erweiterung des Self-Attention-Mechanismus und ist ebenfalls in der Transformer-Architektur implementiert. Die Hauptidee der Autoren des Papers [38] war es, statt einer einzigen Aufmerksamkeitsfunktion mit $d_{model}$-dimensionalen Schlüsseln, Werten und Abfragen, die Abfragen, Schlüssel und Werte $h$ mal mit verschiedenen, gelernten linearen Projektionen (den $W_K$, $W_Q$ und $W_V$ Matrizen) linear zu projizieren.\n",
    "\n",
    "$$\n",
    "\\mathrm{MultiHead}(Q,K,V) = \\mathrm{concat}(\\mathrm{head}_1,...,\\mathrm{head}_h) W^O\n",
    "$$, \n",
    "\n",
    "wobei: \n",
    "\n",
    "$$\\mathrm{head}_i = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"Bilder/multihead_Attention.png\" width=\"700\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig.28 - Overview of the multi-head attention mechanism [38]</b></figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have access, do not hesitate to contact the authors.\n",
    "\n",
    "robin.vetsch@ost.ch or christoph.wuersch@ost.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- [1] https://monkeylearn.com/natural-language-processing/ (08.12.2021)\n",
    "- [2] https://www.nltk.org/ (08.12.2021)\n",
    "- [3] https://www.datacamp.com/community/tutorials/stemming-lemmatization-python (08.12.2021)\n",
    "- [4] https://wordnet.princeton.edu/ (01.10.2021)\n",
    "- [5] https://towardsdatascience.com/legal-applications-of-neural-word-embeddings-556b7515012f (08.12.2021)\n",
    "- [6] https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12 (10.09.2021)\n",
    "- [7] https://medium.com/intelligentmachines/word-embedding-and-one-hot-encoding-ad17b4bbe111 (08.12.2021)\n",
    "- [8] https://medium.com/@zeeshanmulla/word-embeddings-in-natural-language-processing-nlp-5be7d6fb1d73 (08.12.2021)\n",
    "- [9] https://keras.io/api/layers/core_layers/embedding/ (10.09.2021)\n",
    "- [10] https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/ (10.09.2021)\n",
    "- [11] https://arxiv.org/pdf/1301.3781.pdf (08.12.2021)\n",
    "- [12] https://arxiv.org/abs/1310.4546 (08.12.2021)\n",
    "- [13] https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa (09.12.2021)\n",
    "- [14] https://arxiv.org/pdf/1411.2738.pdf (09.12.2021)\n",
    "- [15] https://www.tensorflow.org/tutorials/text/word2vec (09.12.2021)\n",
    "- [16] https://nlp.stanford.edu/projects/glove/ (09.12.2021)\n",
    "- [17] https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010 (09.12.2021)\n",
    "- [18] https://nlp.stanford.edu/pubs/glove.pdf (09.12.2021)\n",
    "- [19] https://machinelearninginterview.com/topics/natural-language-processing/what-is-the-difference-between-word2vec-and-glove/ (11.10.2021)\n",
    "- [20] https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-glove.html (10.12.2021)\n",
    "- [21] https://fasttext.cc/ (10.12.2021)\n",
    "- [22] https://arxiv.org/pdf/1607.04606.pdf (10.12.2021)\n",
    "- [23] https://arxiv.org/abs/1607.01759 (10.12.2021)\n",
    "- [24] https://towardsdatascience.com/fasttext-for-text-classification-a4b38cbff27c (10.12.2021)\n",
    "- [25] https://towardsdatascience.com/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08 (10.12.2021)\n",
    "- [26] https://amitness.com/2020/06/fasttext-embeddings/ (10.12.2021)\n",
    "- [27] https://jalammar.github.io/illustrated-bert/ (14.10.2021)\n",
    "- [28] https://medium.com/saarthi-ai/elmo-for-contextual-word-embedding-for-text-classification-24c9693b0045 (10.12.2021)\n",
    "- [29] https://arxiv.org/pdf/1508.06615.pdf (10.12.2021)\n",
    "- [30] https://arxiv.org/pdf/1505.00387.pdf (10.12.2021)\n",
    "- [31] https://arxiv.org/pdf/1409.3215.pdf (10.12.2021)\n",
    "- [32] https://docs.chainer.org/en/v7.8.0/examples/seq2seq.html (10.12.2021)\n",
    "- [33] https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html (03.11.2021)\n",
    "- [34] https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3 (10.12.2021)\n",
    "- [35] https://arxiv.org/abs/1508.04025 (10.12.2021)\n",
    "- [36] https://arxiv.org/pdf/1409.0473.pdf (10.12.2021)\n",
    "- [37] https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#self_attention (11.12.2021)\n",
    "- [38] https://towardsdatascience.com/transformers-89034557de14 (11.12.2021)\n",
    "- [39] https://www.aisangam.com/blog/difference-between-feed-forward-neural-network-and-rnn/ (11.12.2021)\n",
    "- [40] https://openai.com/blog/better-language-models/ (11.12.2021)\n",
    "- [41] https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf (11.12.2021)\n",
    "- [42] https://arxiv.org/pdf/1810.04805.pdf (11.12.2021)\n",
    "- [43] https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg (11.12.2021)\n",
    "- [44] https://towardsdatascience.com/review-highway-networks-gating-function-to-highway-image-classification-5a33833797b5 (15.12.2021)\n",
    "- [45] Rothman Denis (2021). Transformers for Natural Language Processing. Birmingham, England: Packt Publishing Ltd.\n",
    "- [46] https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec (15.12.2021)\n",
    "- [47] https://arxiv.org/pdf/1512.03385.pdf (15.12.2021)\n",
    "- [48] https://towardsdatascience.com/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655 (15.12.2021)\n",
    "- [49] https://arxiv.org/pdf/1609.08144v2.pdf (15.12.2021)\n",
    "- [50] https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf (17.12.2021)\n",
    "- [51] https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf (17.12.2021)\n",
    "- [52] https://arxiv.org/pdf/2005.14165.pdf (17.12.2021)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
