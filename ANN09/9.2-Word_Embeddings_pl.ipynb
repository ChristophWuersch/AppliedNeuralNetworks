{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN09/9.2-Word_Embeddings_pl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This notebook contains the first code sample found in Chapter 6, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 9.2 Word Embeddings, GloVe and Text classification\n",
    "In diesem Notebook werden wir die Konzepte und die Verwendung von Worteinbettungen in NLP am Beispiel von Glove erklären. Dann werden wir versuchen, die vortrainierten Glove-Worteinbettungen anzuwenden, um ein Textklassifizierungsproblem mit dieser Technik zu lösen.\n",
    "\n",
    "\n",
    "- Die Verwendung von **dichtbesetzten Wortvektoren** ist eine weitere verbreitete und leistungsfähige Methode, einem Wort einen Vektor zuzuordnen. \n",
    "- Man spricht hier auch von einer Worteinbettung. \n",
    "- Die von der One-hot-Codierung erzeugten Vektoren sind binär, **dünnbesetzt (enthalten vorwiegend Nullen) und sehr hochdimensional** (die Dimensionalität entspricht der Anzahl der Wörter des Vokabulars). \n",
    "- Worteinbettungen (word embeddings) hingegen sind niedrigdimensionale **Fliesskommazahlvektoren, also** vergleichsweise\n",
    "dichtbesetzt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/word_embeddings.png\" width=\"440\" height=\"440\" align=\"center\"/>\n",
    "\n",
    "*Die durch One-hot-Codierung oder Hashing erzeugten Wortrepräsentationen sind dünnbesetzt, hochdimensional und fest einprogrammiert. Worteinbettungen dagegen sind dichtbesetzt, relativ niedrigdimensional und anhand der Daten erlernt.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings trainieren\n",
    "\n",
    "- Im Gegensatz zu den durch One-hot-Codierung erzeugten Wortvektoren werden **Word Embeddings anhand der Daten erlernt**. \n",
    "- Worteinbettungen sind nicht selten 256-, 512- oder 1.024-dimensional, wenn das Vokabular entsprechend gross ist.\n",
    "- Andererseits ergeben sich bei der One-hot-Codierung oft Vektoren, die 20.000- dimensional (in diesem Fall kann ein Vokabular von 20.000 Wörtern erfasst werden) oder noch grösser sind. Worteinbettungen bringen also mehr Informationen in sehr viel weniger Dimensionen unter.\n",
    "\n",
    "Es gibt **zwei Möglichkeiten**, Worteinbettungen zu erzeugen:\n",
    "\n",
    "- Lassen Sie das Modell die Worteinbettungen erlernen, während die eigentliche Hauptaufgabe erledigt wird, wie etwa die Klassifizierung von Dokumenten oder die Vorhersage von Stimmungslagen. Anfangs ergeben sich zufällige Wortvektoren, später werden die Wortvektoren auf die gleiche Weise erlernt wie die Gewichtungen eines NNs.\n",
    "- Lesen Sie in Ihr Modell Worteinbettungen ein, die durch andere Machine-Learning- Aufgaben berechnet worden sind. Man bezeichnet diese als vortrainierte Worteinbettungen.\n",
    "\n",
    "Wir werden beide Methoden betrachten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Worteinbettungen mit dem `Embedding` Layer erlernen\n",
    "\n",
    "\n",
    "- Die einfachste Methode, einem Wort einen dichtbesetzten Vektor zuzuordnen, ist die Auswahl eines *zufälligen Vektors*. \n",
    "- Dieser Ansatz bringt jedoch das Problem mit sich, dass der resultierende Einbettungsraum *keine Struktur* besitzt.\n",
    "- Beispielsweise hätten die Wörter toll und super womöglich völlig verschiedene Einbettungen, obwohl sie in den meisten Sätzen austauschbar sind. Für ein NN ist es schwierig, mit einem solchen verrauschten und strukturlosen Einbettungsraum zurechtzukommen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Betrachten wir das Ganze etwas abstrakter: \n",
    "- **Die geometrischen Beziehungen zwischen Wortvektoren sollten die semantischen Beziehungen zwischen den Wörtern widerspiegeln.** \n",
    "- Wortvektoren haben die Aufgabe, die menschliche Sprache auf einen geometrischen Raum abzubilden.\n",
    "- Bei einem vernünftigen Einbettungsraum würden Sie beispielsweise erwarten, dass *Synonyme in ähnlichen Wortvektoren eingebettet* sind. \n",
    "- Und im Allgemeinen würden Sie erwarten, dass der *geometrische Abstand (wie die L2-Norm) zwischen zwei beliebigen Wortvektoren in einer bestimmten Beziehung zum semantischen Abstand der dazugehörigen Wörter steht*: Wörter mit ganz verschiedenen Bedeutungen sind an weit voneinander entfernten Positionen eingebettet, während sich miteinander verwandte Wörter näher sind. \n",
    "- Neben dem Abstand könnte man auch bestimmten *Richtungen im Einbettungsraum* eine Bedeutung verleihen wollen. Betrachten wir zur Verdeutlichung ein konkretes Beispiel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die folgende Abbildung zeigt vier in eine zweidimensionale Ebene eingebettete Wörter: Katze, Hund, Wolf und Tiger. \n",
    "\n",
    "Mit den hier gewählten Vektorrepräsentationen können bestimmte semantische Beziehungen zwischen diesen Wörtern als geometrische\n",
    "Abbildungen codiert werden. \n",
    "- So führt beispielsweise der gleiche Vektor sowohl von Katze zu Tiger als auch von Hund zu Wolf. Man könnte diesen Vektor als »vom Haustier zum Wildtier« interpretieren. \n",
    "- Auf ähnliche Weise führt ein weiterer Vektor von Hund zu Katze bzw. von Wolf zu Tiger, den man wiederum als »von hundeartig zu katzenartig« interpretieren kann.\n",
    "\n",
    "Für in der Praxis auftretende Worteinbettungsräume sind »Geschlecht«- und »Plural«-Vektoren typische Beispiele für sinnvolle geometrische Abbildungen. \n",
    "- Wenn man beispielsweise zum Vektor »König« den Vektor »weiblich« hinzuaddiert, ergibt sich »Königin«. \n",
    "- Durch das Hinzuaddieren eines »Plural«-Vektors erhält man »Könige«. \n",
    "- Worteinbettungsräume enthalten typischerweise Tausende dieser interpretierbaren und potenziell nützlichen Vektoren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "display(\n",
    "    Image.open(\n",
    "        requests.get(\n",
    "            \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/ANN09/Bilder/Beispiel_Word_Embedding.PNG\",\n",
    "            stream=True,\n",
    "        ).raw\n",
    "    ).convert(\"RGB\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Daher ist es vernünftig, bei jeder neuen Aufgabe auch einen neuen Worteinbettungsraum\n",
    "zu erlernen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grosser Filmkritik-Datensatz\n",
    "Dies ist ein Datensatz für die binäre Stimmungs-Klassifikation, der wesentlich mehr Daten enthält als frühere Benchmark-Datensätze. Wir stellen einen Satz von 25.000 hochpolaren Filmkritiken zum Training und 25.000 zum Testen zur Verfügung. Es gibt auch zusätzliche unbeschriftete Daten zur Verwendung. Es werden Rohtext und bereits verarbeitete Bag of Words-Formate bereitgestellt. Weitere Einzelheiten finden Sie in der README-Datei, die in der Veröffentlichung enthalten ist.\n",
    "\n",
    "Link zum Datensatz: http://ai.stanford.edu/~amaas/data/sentiment/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden die Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet kagglehub\n",
    "\n",
    "import kagglehub\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def download_and_extract_glove_to_data(data_dir=\"data\"):\n",
    "    # Erstelle Zielordner falls nötig\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Lade GloVe über kagglehub (landet im Cache-Ordner)\n",
    "    cached_path = kagglehub.dataset_download(\"sawarn69/glove6b100dtxt\")\n",
    "    print(\"Heruntergeladen in Cache:\", cached_path)\n",
    "\n",
    "    # Kopiere den Inhalt des Cache-Ordners nach data/\n",
    "    for item in os.listdir(cached_path):\n",
    "        source_path = os.path.join(cached_path, item)\n",
    "        target_path = os.path.join(data_dir, item)\n",
    "\n",
    "        # Wenn es eine ZIP-Datei ist, entpacken\n",
    "        if item.endswith(\".zip\"):\n",
    "            with zipfile.ZipFile(source_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(data_dir)\n",
    "            print(f\"Entpacke ZIP-Datei: {item}\")\n",
    "        else:\n",
    "            # Ansonsten einfach die Datei/Ordner verschieben\n",
    "            print(f\"Kopiere {source_path} nach {target_path}...\")\n",
    "            shutil.copy(source_path, target_path)\n",
    "\n",
    "    print(\"Fertig: GloVe-Dateien sind jetzt in\", data_dir)\n",
    "\n",
    "\n",
    "# Aufruf\n",
    "download_and_extract_glove_to_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Zusätzliche Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import module to split the datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import modules to evaluate the metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "# Wir brauchen diese Funktion einmalig, um das GloVe-Format in Word2Vec-Format zu konvertieren\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# === Globale Parameter ===\n",
    "glove_filename = \"glove.6B.100d.txt\"\n",
    "train_filename = \"train.csv\"\n",
    "\n",
    "# Verzeichnispfade\n",
    "DATA_PATH = \"data\"\n",
    "glove_path = os.path.join(DATA_PATH, glove_filename)\n",
    "\n",
    "train_path = DATA_PATH\n",
    "test_path = DATA_PATH\n",
    "\n",
    "# Relevante Spalten\n",
    "TEXT_COLUMN = \"text\"\n",
    "TARGET_COLUMN = \"target\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding \n",
    "Was ist ein Word Embedding? Word Embeddings sind eine Art der Wortdarstellung, die es ermöglicht, dass Wörter mit ähnlicher Bedeutung eine ähnliche Darstellung erhalten. Es handelt sich um eine verteilte Darstellung für Text, die vielleicht einer der wichtigsten Durchbrüche für die beeindruckende Leistung von Deep-Learning-Methoden bei anspruchsvollen Problemen der natürlichen Sprachverarbeitung ist.\n",
    "\n",
    "Word Embeddings sind in der Tat eine Klasse von Techniken, bei denen einzelne Wörter als reellwertige Vektoren in einem vordefinierten Vektorraum dargestellt werden. Jedes Wort wird einem Vektor zugeordnet, und die Vektorwerte werden auf eine Art und Weise erlernt, die einem neuronalen Netz ähnelt, weshalb diese Technik oft in den Bereich des Deep Learning eingeordnet wird. Die verteilte Darstellung wird auf der Grundlage der Verwendung von Wörtern gelernt. Dies ermöglicht es, dass Wörter, die auf ähnliche Weise verwendet werden, ähnliche Repräsentationen ergeben, die ihre Bedeutung auf natürliche Weise erfassen.\n",
    "\n",
    "Die Einbettung von neuronalen Netzen hat 3 Hauptzwecke:\n",
    "\n",
    "- Die Suche nach den nächsten Nachbarn im Einbettungsraum. Diese können verwendet werden, um Empfehlungen auf der Grundlage von Benutzerinteressen oder Clusterkategorien auszusprechen.\n",
    "- Als Input für ein maschinelles Lernmodell für eine überwachte Aufgabe.\n",
    "- Zur Visualisierung von Konzepten und Beziehungen zwischen Kategorien.\n",
    "\n",
    "Wie können wir die Word Embedding erhalten?\n",
    "\n",
    "## Embedding Layer\n",
    "Eine Einbettungsschicht ist eine Worteinbettung, die in einem neuronalen Netzmodell für eine bestimmte Aufgabe der natürlichen Sprachverarbeitung gelernt wird. Die Dokumente oder der Korpus der Aufgabe werden gesäubert und aufbereitet, und die Größe des Vektorraums wird als Teil des Modells festgelegt, beispielsweise 50, 100 oder 300 Dimensionen. Die Vektoren werden mit kleinen Zufallszahlen initialisiert. Die Einbettungsschicht wird am vorderen Ende eines neuronalen Netzes verwendet und wird mit Hilfe des Backpropagation-Algorithmus auf überwachte Weise angepasst.\n",
    "\n",
    "Es handelt sich um eine flexible Schicht, die auf verschiedene Weise verwendet werden kann, z. B:\n",
    "\n",
    "- Sie kann allein verwendet werden, um eine Worteinbettung zu lernen, die gespeichert und später in einem anderen Modell verwendet werden kann.\n",
    "- Sie kann als Teil eines Deep-Learning-Modells verwendet werden, wobei die Einbettung zusammen mit dem Modell selbst gelernt wird.\n",
    "- Es kann verwendet werden, um ein vortrainiertes Worteinbettungsmodell zu laden, eine Art von Transfer-Lernen.\n",
    "Dieser Ansatz erfordert eine große Menge an Trainingsdaten und kann sehr langsam sein, lernt aber eine Einbettung, die sowohl auf die spezifischen Textdaten als auch auf die gewünschte NLP-Aufgabe zugeschnitten ist.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "Word2Vec ist eine statistische Methode zum effizienten Lernen einer eigenständigen Worteinbettung aus einem Textkorpus. Sie wurde 2013 von Tomas Mikolov et al. bei Google entwickelt, um das auf neuronalen Netzen basierende Training der Einbettung effizienter zu gestalten, und ist seitdem zum De-facto-Standard für die Entwicklung von vortrainierten Worteinbettungen geworden.\n",
    "\n",
    "Es lernt die Worteinbettung auf eine von zwei Arten:\n",
    "\n",
    "- entweder anhand des Kontexts, um ein Zielwort vorherzusagen, eine Methode, die als Continuous Bag of Words (CBOW) bekannt ist\n",
    "- oder die Verwendung eines Wortes zur Vorhersage eines Zielkontextes, was als Skip-Gram bezeichnet wird, z. B. möchten wir c Kontextwörter vorhersagen, die ein Zielwort in der Eingabe haben. Die letztere Methode liefert in der Regel genauere Ergebnisse bei großen Datensätzen.\n",
    "\n",
    "<img src=\"Bilder/CBOW-model.png\" width=\"700\"  align=\"center\"/>\n",
    "\n",
    "Ein gut trainierter Satz von Wortvektoren wird ähnliche Wörter nahe beieinander in diesem Raum platzieren. Die Wörter „Eiche“, „Ulme“ und „Birke“ könnten sich in einer Ecke anhäufen, während „Krieg“, „Konflikt“ und „Streit“ sich in einer anderen Ecke aneinander drängen.\n",
    "\n",
    "## Glove\n",
    "Der GloVe-Algorithmus (Global Vectors for Word Representation) ist eine Erweiterung der word2vec-Methode zum effizienten Lernen von Wortvektoren, die von Pennington et al. in Stanford entwickelt wurde. GloVe ist ein unüberwachter Lernalgorithmus zur Gewinnung von Vektordarstellungen für Wörter. Das Training erfolgt auf der Grundlage aggregierter globaler Wort-Wort-Ko-Okzidenz-Statistiken aus einem Korpus, und die resultierenden Repräsentationen zeigen interessante lineare Unterstrukturen des Wortvektorraums.\n",
    "\n",
    "GloVe ist ein Ansatz, der sowohl die globalen Statistiken von Matrixfaktorisierungsverfahren wie LSA (Latent Semantic Analysis) mit dem lokalen kontextbasierten Lernen in word2vec verbindet. Anstatt ein Fenster zu verwenden, um den lokalen Kontext zu definieren, konstruiert GloVe eine explizite Wort-Kontext- oder Wort-Ko-Occurrence-Matrix unter Verwendung von Statistiken über den gesamten Textkorpus.\n",
    "\n",
    "Eine ausführliche Erklärung finden Sie unter dem nächsten Link: Jeffrey Pennington, Richard Socher, und Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "## Laden einer vortrainierten Worteinbettung: GloVe\n",
    "Dateien mit den vortrainierten Vektoren Glove finden Sie auf vielen Seiten wie Kaggle oder unter dem oben genannten Link der Stanford University. Wir werden die Datei ``glove.6B.100d.txt`` verwenden, die die Glove-Vektoren enthält, die auf dem Wikipedia- und GigaWord-Datensatz trainiert wurden.\n",
    "\n",
    "GloVe und Word2Vec sind zwei unterschiedliche Formate für vortrainierte Wortvektoren.\n",
    "- Viele Tools (wie `gensim`) erwarten Vektoren im Word2Vec-Format (also mit Headerzeile und Leerzeichen als Trennzeichen).\n",
    "- GloVe-Dateien (z. B. `glove.6B.300d.txt`) haben keine Headerzeile und sind durch Leerzeichen getrennt, aber nicht direkt kompatibel mit dem Word2Vec-Ladeformat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_filename = \"glove.6B.100d.txt\"  # Beispielname\n",
    "glove_path = f\"data/{glove_filename}\"\n",
    "word2vec_output_file = f\"data/{glove_filename}.word2vec\"\n",
    "\n",
    "# Nur konvertieren, wenn die Datei noch nicht existiert\n",
    "if not os.path.exists(word2vec_output_file):\n",
    "    print(f\"Konvertiere {glove_filename} in Word2Vec-Format...\")\n",
    "    glove2word2vec(glove_path, word2vec_output_file)\n",
    "    print(\"Konvertierung abgeschlossen.\")\n",
    "else:\n",
    "    print(f\"Datei {word2vec_output_file} existiert bereits. Überspringe Konvertierung.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser Vokabular enthält also 400K Wörter, die durch einen Merkmalsvektor der Form 100 dargestellt werden. Jetzt können wir die Glove-Einbettungen im word2vec-Format laden und dann einige Analogien analysieren. Wenn wir auf diese Weise eine bereits trainierte word2vec-Einbettung verwenden wollen, können wir einfach den Dateinamen ändern und den gesamten unten stehenden Code wiederverwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the Stanford GloVe model\n",
    "word2vec_output_file = \"data/\" + glove_filename + \".word2vec\"\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "# Show a word embedding\n",
    "print(\"King: \", model.get_vector(\"king\"))\n",
    "\n",
    "result = model.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=1)\n",
    "\n",
    "print(\"Most similar word to King + Woman: \", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysieren des Vektorraums und Finden von Analogien\n",
    "Da unsere Wörter numerische Vektoren sind, können wir die Abstände zwischen den Wörtern messen und vergleichen, um einige der Eigenschaften zu zeigen, die diese Einbettungen bieten.\n",
    "\n",
    "Zum Beispiel können wir einige Analogien vergleichen. Die bekannteste ist die folgende: König - Mann + Frau = Königin. Mit anderen Worten: Die Addition der Vektoren, die mit den Wörtern König und Frau verbunden sind, und die Subtraktion von Mann ist gleich dem Vektor, der mit Königin verbunden ist. Mit anderen Worten, wenn wir den Begriff „Mann“ vom Begriff „König“ subtrahieren, erhalten wir eine Darstellung der „Königswürde“. Wenn wir dann das Wort „Frau“ zu diesem Konzept addieren, erhalten wir das Wort „Königin“. Ein anderes Beispiel ist: Frankreich - Paris + Rom = Italien. In diesem Fall erfasst die Vektordifferenz zwischen Paris und Frankreich das Konzept des Landes.\n",
    "\n",
    "Nun werden wir einige dieser Analogien in verschiedenen Themenbereichen aufzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=1)\n",
    "print(\"King - Man + Woman = \", result)\n",
    "result = model.most_similar(positive=[\"rome\", \"france\"], negative=[\"paris\"], topn=1)\n",
    "print(\"France - Paris + Rome = \", result)\n",
    "result = model.most_similar(positive=[\"english\", \"france\"], negative=[\"french\"], topn=1)\n",
    "print(\"France - french + english = \", result)\n",
    "result = model.most_similar(\n",
    "    positive=[\"june\", \"december\"], negative=[\"november\"], topn=1\n",
    ")\n",
    "print(\"December - November + June = \", result)\n",
    "result = model.most_similar(positive=[\"sister\", \"man\"], negative=[\"woman\"], topn=1)\n",
    "print(\"Man - Woman + Sister = \", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können beobachten, wie die Wortvektoren Informationen enthalten, um Länder mit Nationalitäten, Monaten des Jahres, Familienbeziehungen usw. in Verbindung zu bringen.\n",
    "\n",
    "Aber nicht immer erhalten wir die erwarteten Ergebnisse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But not always we get the expected result\n",
    "result = model.most_similar(positive=[\"aunt\", \"nephew\"], negative=[\"niece\"], topn=1)\n",
    "print(\"France - Paris + Rome = \", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können extrahieren, welche Wörter einem anderen Wort am ähnlichsten sind, sodass sie im Vektorraum \"sehr nah\" beieinander liegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.most_similar(positive=[\"spain\"], topn=10)\n",
    "print(\"10 most similar words to Spain: \", result)\n",
    "\n",
    "result = model.most_similar(positive=[\"football\"], topn=10)\n",
    "print(\"\\n10 most similar words to Football: \", result)\n",
    "\n",
    "result = model.most_similar(positive=[\"doctor\"], topn=10)\n",
    "print(\"\\n10 most similar words to Doctor: \", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets show some measure of similarities between words\n",
    "result = model.similar_by_word(\"cat\")\n",
    "print(\" Cat is similar to {}: {:.4f}\".format(*result[0]))\n",
    "result = model.similar_by_word(\"father\")\n",
    "print(\" Father is similar to {}: {:.4f}\".format(*result[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Idee hinter allen Worteinbettungen ist es, mit ihnen so viele semantische/morphologische/kontextuelle/hierarchische/etc. Informationen wie möglich zu erfassen, aber in der Praxis ist eine Methode definitiv besser als die andere für eine bestimmte Aufgabe (z.B. ist LSA ziemlich effektiv, wenn man im niedrigdimensionalen Raum für die Analyse von eingehenden Dokumenten aus dem gleichen Bereich arbeitet wie die, die bereits verarbeitet und in die Term-Dokument-Matrix eingefügt wurden). Das Problem der Auswahl der besten Einbettungen für ein bestimmtes Projekt ist immer das Problem des Versuch-und-Irrtum-Ansatzes, so dass das Erkennen, warum in einem bestimmten Fall ein Modell besser funktioniert als das andere, bei der realen Arbeit ausreichend hilft.\n",
    "\n",
    "## Visualisierung von Worteinbettungen\n",
    "Ein weiterer spannender Vorgang, den wir mit Einbettungen durchführen können, ist die Visualisierung. Wenn wir sie in einem zweidimensionalen Raum aufzeichnen, können wir sehen, wie die Wörter miteinander verwandt sind. Die meisten ähnlichen Wörter sollten in Gruppen dargestellt werden, während nicht verwandte Wörter in einem großen Abstand erscheinen. Dies erfordert eine weitere Dimensionsreduktionstechnik, um die Dimensionen auf 2 oder 3 zu reduzieren. Die beliebteste Technik zur Dimensionsreduktion ist selbst eine Einbettungsmethode: t-Distributed Stochastic Neighbor Embedding (TSNE).\n",
    "\n",
    "t-SNE steht für t-distributed stochastic neighbor embedding (t-verteilte stochastische Nachbarschaftseinbettung). Es handelt sich um ein Verfahren zur Dimensionalitätsreduktion, das sich am besten für die Visualisierung hochdimensionaler Datensätze eignet.\n",
    "\n",
    "TSNE ist eine Technik des vielfältigen Lernens, d. h. es wird versucht, hochdimensionale Daten auf eine niedrigdimensionale Mannigfaltigkeit abzubilden und eine Einbettung zu schaffen, die versucht, die lokale Struktur innerhalb der Daten zu erhalten. Es wird fast ausschließlich für die Visualisierung verwendet, da die Ausgabe stochastisch ist und die Umwandlung neuer Daten nicht unterstützt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE  # final reduction\n",
    "import numpy as np  # array handling\n",
    "\n",
    "\n",
    "def display_closestwords_tsnescatterplot(model, dim, words):\n",
    "    arr = np.empty((0, dim), dtype=\"f\")\n",
    "    word_labels = words\n",
    "\n",
    "    # get close words\n",
    "    # close_words = [model.similar_by_word(word) for word in words]\n",
    "\n",
    "    # add the vector for each of the closest words to the array\n",
    "    close_words = []\n",
    "    for word in words:\n",
    "        arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "        close_words += model.similar_by_word(word)\n",
    "\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "\n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    # np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\"offset points\")\n",
    "    plt.xlim(x_coords.min() + 0.00005, x_coords.max() + 0.00005)\n",
    "    plt.ylim(y_coords.min() + 0.00005, y_coords.max() + 0.00005)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def tsne_plot(model, words):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    # for word in model.wv.vocab:\n",
    "    for word in words:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "\n",
    "    tsne_model = TSNE(\n",
    "        perplexity=10, n_components=2, init=\"pca\", n_iter=2500, random_state=23\n",
    "    )\n",
    "    new_values = tsne_model.fit_transform(np.array(tokens))\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i])\n",
    "        plt.annotate(\n",
    "            labels[i],\n",
    "            xy=(x[i], y[i]),\n",
    "            xytext=(5, 2),\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"right\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden zum Beispiel hundert Wörter aus unseren Worteinbettungen und auch die ähnlichsten Wörter zu den Begriffen Frau und Auto einzeichnen. Alle ähnlichen Wörter sollten dicht beieinander liegen, während die restlichen Wörter über den Vektorraum verteilt erscheinen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_closestwords_tsnescatterplot(model, 100, ['man', 'dog'])\n",
    "# words = model.index_to_key\n",
    "words1 = model.similar_by_word(\"food\", topn=30)\n",
    "words2 = model.similar_by_word(\"woman\", topn=30)\n",
    "words3 = model.similar_by_word(\"car\", topn=30)\n",
    "words = [w[0] for w in words1] + [w[0] for w in words2] + [w[0] for w in words3]\n",
    "# print(words)\n",
    "tsne_plot(model, words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der obigen Abbildung ist zu erkennen, dass die ähnlichsten Wörter zu „Frau“ und „Auto“ sehr nahe beieinander liegen, während die anderen Wörter gleichmäßig im Vektorraum verteilt sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden des Datensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "\n",
    "# Lade und extrahiere IMDb-Daten\n",
    "def download_and_extract_imdb(data_dir=\"data\", dataset_folder=\"aclImdb\"):\n",
    "    data_path = os.path.join(data_dir, dataset_folder)\n",
    "    archive_path = os.path.join(data_dir, \"aclImdb_v1.tar.gz\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "        print(\"Downloading IMDb dataset...\")\n",
    "\n",
    "        import ssl\n",
    "\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        urllib.request.urlretrieve(url, archive_path)\n",
    "\n",
    "        with tarfile.open(archive_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=data_dir, filter=\"data\")\n",
    "\n",
    "        print(\"Dataset downloaded and extracted to:\", data_path)\n",
    "    else:\n",
    "        print(\"IMDb dataset already exists at:\", data_path)\n",
    "\n",
    "\n",
    "# Lese IMDb-Dateien\n",
    "def load_imdb_dataset(path, sentiment):\n",
    "    data = []\n",
    "    labels = []\n",
    "    folder = os.path.join(path, sentiment)\n",
    "    for filename in os.listdir(folder):\n",
    "        with open(os.path.join(folder, filename), encoding=\"utf-8\") as f:\n",
    "            data.append(f.read())\n",
    "            labels.append(1 if sentiment == \"pos\" else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "# Download IMDb-Daten\n",
    "download_and_extract_imdb()\n",
    "\n",
    "# Lade Trainings- und Testdaten\n",
    "train_pos, y_train_pos = load_imdb_dataset(\"data/aclImdb/train\", \"pos\")\n",
    "train_neg, y_train_neg = load_imdb_dataset(\"data/aclImdb/train\", \"neg\")\n",
    "test_pos, y_test_pos = load_imdb_dataset(\"data/aclImdb/test\", \"pos\")\n",
    "test_neg, y_test_neg = load_imdb_dataset(\"data/aclImdb/test\", \"neg\")\n",
    "\n",
    "X_train_raw = train_pos + train_neg\n",
    "Y_train = y_train_pos + y_train_neg\n",
    "X_test_raw = test_pos + test_neg\n",
    "Y_test = y_test_pos + y_test_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Zielpfad für die heruntergeladene Datei\n",
    "output_path = \"data/GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# URL der Datei\n",
    "url = \"https://huggingface.co/spaces/CS482Project/Milestone_5-Search_UI_and_pitch_video_voiceover/resolve/f73493055a3f08700fae815fbbd1ca0511e29a04/Data/GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# Lade Word2Vec (z.B. GoogleNews)\n",
    "print(\"Lade Word2Vec-Modell (kann ein paar Minuten dauern)...\")\n",
    "\n",
    "# Prüfe, ob Datei schon existiert\n",
    "if not os.path.exists(output_path):\n",
    "    print(\"Datei nicht gefunden – lade sie herunter...\")\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "    print(f\"Datei wurde heruntergeladen und unter '{output_path}' gespeichert.\")\n",
    "else:\n",
    "    print(f\"Datei bereits vorhanden unter '{output_path}' – überspringe Download.\")\n",
    "\n",
    "# Lade das Modell\n",
    "print(\"Lade das Word2Vec-Modell...\")\n",
    "model = KeyedVectors.load_word2vec_format(output_path, binary=True)\n",
    "print(\"Modell erfolgreich geladen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwendung der Worteinbettung auf eine Textklassifizierungsaufgabe\n",
    "Wir haben nun unsere Wortrepräsentation, einen Vektor für jedes Wort in unserem Vokabular. Aber wir müssen uns mit ganzen Sätzen befassen, also müssen wir eine Satzeinbettung erstellen, d.h. wir brauchen einen Vektor, der den ganzen Satz repräsentiert, und jedes Merkmal im Vektor wird auf den Worteinbettungen basieren. Da es viele Möglichkeiten gibt und wir dieses Thema nicht behandeln werden, wenden wir eine sehr einfache Methode an: Der i-te Wert in der Satzeinbettung ist der Mittelwert der i-ten Werte in der Worteinbettung aller Wörter des Satzes.\n",
    "\n",
    "Wir erstellen eine Klasse, die unser Vokabular und die Vektoren der Handschuhe enthält, und wandeln dann jede Rezension (in unserem Beispiel einen Satz) in eine Vektordarstellung um, wie wir es zuvor beschrieben haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Minimaler Text-Cleaner\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z']\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Vektorisierer\n",
    "class Word2VecVectorizer:\n",
    "    def __init__(self, model):\n",
    "        print(\"Loading in word vectors...\")\n",
    "        self.word_vectors = model\n",
    "        print(\"Finished loading in word vectors.\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        D = self.word_vectors.vector_size\n",
    "        X = np.zeros((len(data), D))\n",
    "        emptycount = 0\n",
    "\n",
    "        for i, sentence in enumerate(data):\n",
    "            tokens = clean_text(sentence).split()\n",
    "            vecs = []\n",
    "            for word in tokens:\n",
    "                try:\n",
    "                    vecs.append(self.word_vectors[word])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            if len(vecs) > 0:\n",
    "                X[i] = np.mean(vecs, axis=0)\n",
    "            else:\n",
    "                emptycount += 1\n",
    "        print(f\"Samples with no known words: {emptycount}/{len(data)}\")\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Nächstes erstellen wir ein Vectorizer-Objekt, mit dessen Hilfe wir unsere Bewertungen in Vektoren, also eine numerische Darstellung, umwandeln können. Mit diesen Vektoren können wir dann unseren Klassifikator füttern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vektorisierung\n",
    "vectorizer = Word2VecVectorizer(model)\n",
    "X_train = vectorizer.transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainieren eines Klassifikators auf den Satzeinbettungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Klassifikation\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "clf.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierungen\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index=[0, 1], columns=[0, 1])\n",
    "    sn.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"r--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse\n",
    "print(\"Train score:\", clf.score(X_train, Y_train))\n",
    "print(\"Test score:\", clf.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersagen\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.classification_report(Y_test, y_pred, digits=5))\n",
    "plot_confusion_matrix(Y_test, y_pred)\n",
    "plot_roc_curve(Y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where's the intelligence?\n",
    "\n",
    "- Was ist smart an diesen Word-Embeddings?\n",
    "- Lesen Sie bei Chris Manning, wie die GloVe genau funktioniern: https://nlp.stanford.edu/projects/glove/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
