{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U01/WoodsMammography-PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "\n",
    "# Ein neuronales Netz zur Klassifizierung von Brustkrebs \n",
    "### (Woods Mammography Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Source: https://machinelearningmastery.com/develop-a-neural-network-for-woods-mammography-dataset/\n",
    "\n",
    "In dieser Übungsaufgabe erfahren Sie, wie Sie ein neuronales Mehrschicht-Perceptron-Netzwerkmodell für den Wood's Mammographie-Klassifizierungsdatensatz entwickeln.\n",
    "\n",
    "Nach erfolgreichem Abschluss dieser Übungsaufgabe werden Sie wissen:\n",
    "\n",
    "- Wie man den Wood's Mammography-Datensatz lädt und zusammenfasst und die Ergebnisse nutzt, um Vorschläge für die Datenaufbereitung und die zu verwendenden Modellkonfigurationen zu machen.\n",
    "- Wie man die Lerndynamik von einfachen MLP-Modellen auf dem Datensatz untersucht.\n",
    "- Wie man robuste Schätzungen der Modellleistung entwickelt, die Modellleistung abstimmt und Vorhersagen für neue Daten macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## (a) Laden des Datensatzes\n",
    "\n",
    "Der Datensatz geht auf Kevin Woods et al. und die 1993 veröffentlichte Arbeit mit dem Titel [Comparative Evaluation Of Pattern Recognition Techniques For Detection Of Microcalcifications In Mammography](https://www.worldscientific.com/doi/abs/10.1142/S0218001493000698) zurück.\n",
    "\n",
    "Jason Brownlee hat auf folgendem GitHub-Ordner noch weitere ML-Datensätze zusammengestellt, die für das Traineren von einfachen ML-Modellen verwendet werden können: https://github.com/jbrownlee/Datasets/tree/master\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n",
    "#df = pd.read_csv('mammography.csv', header=None)\n",
    "columns=['f0','f1','f2','f3','f4','f5','label']\n",
    "df=pd.read_csv('https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/U01/mammography.csv', header=None)\n",
    "df.columns = columns\n",
    "#infoFile='mammography.names'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## (b) Plotten von Histogrammen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df.hist(bins=51,figsize=(16,9));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "- Es ist zu erkennen, dass die meisten Variablen vielleicht eine Exponentialverteilung haben, und vielleicht ist Variable 5 (die letzte Eingabevariable) normalverteilt mit einigen Ausreissern/fehlenden Werten.\n",
    "- Es könnte von Vorteil sein, eine *Potenztransformation* (`PowerTransformer`) auf jede Variable anzuwenden, um die Wahrscheinlichkeitsverteilung weniger schief zu machen, was die Leistung des Modells wahrscheinlich verbessern wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## (c) Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# split into input and output columns\n",
    "\n",
    "le=LabelEncoder()\n",
    "\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "# ensure all data are floating point values\n",
    "X = X.astype('float32')\n",
    "# encode strings to integer\n",
    "y = le.fit_transform(y)\n",
    "df['label'] = np.float32(y)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))   \n",
    "\n",
    "sns.heatmap(df.corr(),annot=True,linewidths=.5,cmap=\"YlGnBu\",square=True, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## (d) Klasse für Datensatz (`Dataset`) und `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features (X) and target (y)\n",
    "X = torch.tensor(df.iloc[:, :-1].values, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(df.iloc[:, -1].values, dtype=torch.float32).to(device)  # Use long for class indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Dataset class\n",
    "class MammographyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create dataset and split into train and validation sets\n",
    "dataset = MammographyDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the number of input features\n",
    "n_features = X.shape[1]\n",
    "print('Input Features: %i' % (n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## (e) Modell erstellen und kompilieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Determine the number of input features\n",
    "n_features = X.shape[1]\n",
    "print('Input Features:', n_features)\n",
    "\n",
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features,n_hidden=64):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_hidden)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)  # Dropout layer after fc1\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)  # Dropout layer after fc2\n",
    "        self.fc3 = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Apply dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # Apply dropout after activation\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = Net(n_features).to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## (f) Definiere den Optimierer und die Verlustfunktion (loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## (g) Training für 100 Epochen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.float().unsqueeze(1))  # Ensure target is float for BCELoss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        predicted = (output > 0.5).float()\n",
    "        train_total += target.size(0)\n",
    "        train_correct += (predicted == target.float().unsqueeze(1)).sum().item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accuracies.append(100 * train_correct / train_total)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target.float().unsqueeze(1))\n",
    "            val_loss += loss.item()\n",
    "            predicted = (output > 0.5).float()\n",
    "            val_total += target.size(0)\n",
    "            val_correct += (predicted == target.float().unsqueeze(1)).sum().item()\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accuracies.append(100 * val_correct / val_total)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "          f'Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, '\n",
    "          f'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## (h) Modell speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'mammography_model.pth')\n",
    "\n",
    "# Load the saved model\n",
    "model = Net(n_features).to(device)\n",
    "model.load_state_dict(torch.load('mammography_model.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## (i) Lernkurven darstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## (j) Endgültiges Modell und Vorhersagen machen\n",
    "\n",
    "Sobald wir uns für eine Modellkonfiguration entschieden haben, können wir ein endgültiges Modell auf allen verfügbaren Daten trainieren und es verwenden, um Vorhersagen für neue Daten zu treffen.\n",
    "\n",
    "In diesem Fall verwenden wir das Modell mit Dropout und einer kleinen Losgröße als endgültiges Modell.\n",
    "\n",
    "Wir können die Daten vorbereiten und das Modell wie zuvor anpassen, allerdings mit dem gesamten Datensatz anstelle einer Trainingsuntermenge des Datensatzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a final model and make predictions on new data for the mammography dataset\n",
    "\n",
    "# define a row of new data\n",
    "row = [0.23001961,5.0725783,-0.27606055,0.83244412,-0.37786573,0.4803223]\n",
    "\n",
    "# Make prediction on new data\n",
    "row = torch.tensor(row, dtype=torch.float32).to(device)\n",
    "yhat = (model(row) > 0.5).int().item()\n",
    "yhat = le.inverse_transform([yhat])\n",
    "print('Predicted:', yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachLe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
