{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U01/MyFirstPyTorchNN-PimaIndian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" height=\"120\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "\n",
    "## Ein erstes Neuronales Netz mit `PyTorch`: Pima Indians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## (a) Datensatz laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/U01/diabetes.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## (b) Kurze EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool_)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, vmin=0.0, center=0, annot=True,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:8].values\n",
    "y = df.iloc[:,8].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## (c) Vorbereitung eines Datensatzes in PyTorch: Dataset-Klasse\n",
    "\n",
    "In PyTorch sind Datensätze und DataLoader essenziell, um Daten effizient für das Training und die Evaluation von Modellen bereitzustellen. Hier wird beschrieben, wie man einen eigenen Datensatz vorbereitet, ihn in ein `Dataset`-Objekt einbindet und mit einem `DataLoader` verwendet.\n",
    "Zuerst müssen die Daten in geeignete PyTorch-Tensoren umgewandelt werden.\n",
    "- **`X`**: Enthält die Eingabe-Features (z. B. Spalten 0–7 eines DataFrames).\n",
    "- **`y`**: Enthält die Zielwerte (z. B. Spalte 8 eines DataFrames).\n",
    "- **`dtype=torch.float32`**: Konvertiert die Daten in den Gleitkomma-Datentyp, der häufig in PyTorch verwendet wird.\n",
    "- **`.to(device)`**: Verschiebt die Tensoren optional auf die GPU oder CPU, je nach Bedarf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into features (X) and target (y)\n",
    "X = torch.tensor(df.iloc[:, 0:8].values, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(df.iloc[:, 8].values, dtype=torch.float32).unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### **Erstellung einer benutzerdefinierten Dataset-Klasse**\n",
    "\n",
    "PyTorch bietet die Möglichkeit, eigene Datensätze zu erstellen, indem man von der Klasse `torch.utils.data.Dataset` erbt. Dies ermöglicht die flexible Handhabung von Daten.\n",
    "\n",
    "- **`__init__`**: Initialisiert die Klasse mit den Eingabedaten (`X`) und den Zielwerten (`y`).\n",
    "- **`__len__`**: Gibt die Anzahl der Datenpunkte im Datensatz zurück.\n",
    "- **`__getitem__`**: Ermöglicht den Zugriff auf die Datenpunkte über einen Index. Dies wird vom `DataLoader` verwendet, um Daten in Batches zu laden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## (d) Aufteilen in einen Trainings und Validierungsdatensatz & DataLoader\n",
    "\n",
    "Teilen Sie den geladenen Datensatz in einen Trainings- und einen Testdatensatz auf (Aufteilung nach Zeilen) und verwenden Sie einen die Trainingsdaten (80%), um das Modell zu trainieren, und den Validierungsdatensatz (20%), um die Leistung des Modells auf neuen Daten zu schätzen. Erzeugen Sie einen `DataLoader` für die Trainings- und für die Validierungsdaten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and split into train and validation sets\n",
    "dataset = DiabetesDataset(X, y)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "\n",
    "Ein `DataLoader` wird verwendet, um Daten in Mini-Batches zu laden und optional zufällig zu mischen.\n",
    "- **`batch_size`**: Die Anzahl der Datenpunkte pro Batch.\n",
    "- **`shuffle=True`**: Mischt die Daten zu Beginn jeder Epoche.\n",
    "\n",
    "Der DataLoader kann in der Trainingsschleife verwendet werden, um Mini-Batches zu laden:\n",
    "\n",
    "```python\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    # Weiterverarbeitung (z. B. Modellvorhersage und Verlustberechnung)\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    # Backpropagation und Optimierung\n",
    "```\n",
    "\n",
    "Dieses Vorgehen ermöglicht eine effiziente und skalierbare Verarbeitung von Datensätzen in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "print_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "inputs, labels = next(iter(print_loader))  # Get the first batch\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## (e) Modell erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)\n",
    "# Assuming 'model' is your defined neural network\n",
    "model = model.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 128)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)  # Dropout layer after fc1\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)  # Dropout layer after fc2\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Apply dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # Apply dropout after activation\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)\n",
    "# Assuming 'model' is your defined neural network\n",
    "model = model.to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## (f) Modell darstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Summarize the model\n",
    "summary(model, input_size=(1, 8))  # Adjust input_size as per your model's requirement\n",
    "\n",
    "# Visualize the model architecture\n",
    "x = torch.randn(1, 1, 8)  # Adjust input size as per your model's requirement\n",
    "y = model(x)\n",
    "make_dot(y, params=dict(model.named_parameters())).render(\"model_architecture\", format=\"png\")\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## (g) Modell kompilieren und trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 300\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        predicted = (output > 0.5).float()\n",
    "        train_total += target.size(0)\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accuracies.append(100 * train_correct / train_total)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            predicted = (output > 0.5).float()\n",
    "            val_total += target.size(0)\n",
    "            val_correct += (predicted == target).sum().item()\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accuracies.append(100 * val_correct / val_total)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "          f'Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, '\n",
    "          f'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluierung des Modells (Beispiel)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        predicted = (output > 0.5).float()\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print('Accuracy: {}'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## (h) Modell abspeichern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Sie können Ihr Modell speichern, indem Sie die Funktion `save()` für das Modell aufrufen und den Dateinamen angeben.\n",
    "Das folgende Beispiel demonstriert dies, indem zunächst ein Modell angepasst, ausgewertet und in der Datei \"model.h5\" gespeichert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'diabetes_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## (i) Modell laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Load the saved state dictionary\n",
    "state_dict = torch.load('diabetes_model.pth')\n",
    "\n",
    "# Create an instance of your model\n",
    "model = Net()\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Now you can use the loaded model for inference or further training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## (j) Lernkurven grafisch darstellen\n",
    "\n",
    "Die Funktion `fit()` gibt ein `History`-Objekt zurück, das den Verlust und die Genauigkeit am Ende jeder Epoche zusammenfasst. Erstellen Sie Liniendiagramme dieser Daten, sogenannte Lernkurven.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.ylim(0.4, 0.8)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## (k) Trainieren auf einem neuen Datensatzes\n",
    "\n",
    "Probieren Sie, einen anderen Tabellendatensatz zu verwenden, vielleicht aus dem UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## (l)  Das Modell anpassen\n",
    "\n",
    "Ändern Sie die Konfiguration des Modells oder den Trainingsprozess und sehen Sie, ob Sie die Leistung des Modells verbessern können, z. B. eine Genauigkeit von mehr als 76 % erreichen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachLe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
