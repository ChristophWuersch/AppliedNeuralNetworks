{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTFj8ft5dlbS"
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" height=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over- and Underfitting und Regularisierung\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U03/overfit_and_underfit_SOLUTION-PyTorch.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19rPukKZsPG6"
   },
   "source": [
    "**Overfitting** bedeutet: Unser Modell passt sich den Trainingsdaten *zu sehr* an. Es ist wichtig zu lernen, wie man mit der Überanpassung umgeht. Obwohl es oft möglich ist, eine hohe Genauigkeit im *Trainingsdatensatz* zu erreichen, wollen wir eigentlich Modelle entwickeln, die sich gut auf einen *Testdatensatz* verallgemeinern lassen (oder auf Daten, die sie noch nie gesehen haben).\n",
    "\n",
    "Das Gegenteil von Overfitting ist **Underfitting**. Underfitting liegt vor, wenn die Trainingsdaten noch verbesserungswürdig sind. Dies kann aus verschiedenen Gründen geschehen: Wenn das Modell nicht leistungsfähig genug ist, überreguliert ist oder einfach nicht lange genug trainiert wurde. Das bedeutet, dass das Netz die relevanten Muster in den Trainingsdaten nicht gelernt hat.\n",
    "\n",
    "Wenn Sie jedoch zu lange trainieren, wird das Modell anfangen, sich zu sehr anzupassen und Muster aus den Trainingsdaten zu lernen, die sich nicht auf die Testdaten übertragen lassen. Wir müssen also ein Gleichgewicht finden. Es ist nützlich zu wissen, wie man für eine angemessene Anzahl von Epochen trainiert, wie wir weiter unten erläutern werden.\n",
    "\n",
    "Um eine Überanpassung zu vermeiden, ist die beste Lösung die Verwendung vollständigerer Trainingsdaten. Der Datensatz sollte die gesamte Bandbreite der Eingaben abdecken, die das Modell verarbeiten soll. Zusätzliche Daten sind nur dann sinnvoll, wenn sie neue und interessante Fälle abdecken.\n",
    "Wir verwenden wie immer die `pytorch_lightning`-API, über die Sie mehr im [PyTorch Lightning Guide](https://pytorch-lightning.readthedocs.io/en/stable/) erfahren können.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisierung\n",
    "\n",
    "Ein Modell, das auf umfangreicheren Daten trainiert wurde, wird natürlich besser verallgemeinern. Wenn dies nicht mehr möglich ist, besteht die nächstbeste Lösung darin, Techniken wie die **Regularisierung** anzuwenden. Diese schränken die Menge und Art der Informationen ein, die Ihr Modell speichern kann.  Wenn sich ein Netzwerk nur eine kleine Anzahl von Mustern merken kann, wird es durch den Optimierungsprozess gezwungen, sich auf die auffälligsten Muster zu konzentrieren, die eine bessere Chance haben, gut zu verallgemeinern.\n",
    "\n",
    "In dieser Übungsaufgabe werden wir verschiedene *gängige Regularisierungstechniken* untersuchen und sie zur Verbesserung eines Klassifizierungsmodells einsetzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WL8UoOTmGGsL"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FklhSI0Gg9R"
   },
   "source": [
    "Bevor Sie beginnen, importieren Sie die erforderlichen Pakete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipywidgets # für interaktive Widgets und Ladebalken auskommentieren und installieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt  # Zum Erstellen von Diagrammen\n",
    "import pandas as pd  # Zum Laden und Verarbeiten von CSV-Daten\n",
    "import pytorch_lightning as pl  # Zur Vereinfachung des Trainingsprozesses\n",
    "import requests\n",
    "import torch  # Grundfunktionen und Tensors von PyTorch\n",
    "import torch.nn as nn  # Neuronale Netzschichten\n",
    "import torch.optim as optim  # Optimierer von PyTorch\n",
    "import torchmetrics\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    ")  # Callback, um das Training frühzeitig zu beenden\n",
    "from pytorch_lightning.loggers import (\n",
    "    CSVLogger,\n",
    ")  # Logger für Trainingsmetriken (z. B. in CSV-Dateien)\n",
    "from torch.utils.data import DataLoader  # Hilfsfunktionen für Datensätze\n",
    "from torch.utils.data import Dataset, TensorDataset, random_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cweoTiruj8O"
   },
   "source": [
    "## The Higgs Dataset\n",
    "\n",
    "Das Ziel dieser Übungsserie ist nicht die Teilchenphysik, daher sollten Sie sich nicht mit den Details des Datensatzes beschäftigen. Er enthält 11'000'000 Beispiele, jedes mit 28 Merkmalen und einem binären Klassenlabel.\n",
    "\n",
    "Informationen zum Datensatz:\n",
    "\n",
    "Die [Daten](http://mlphysics.ics.uci.edu/data/higgs/) wurden mit Hilfe von Monte-Carlo-Simulationen erstellt. Die ersten 21\n",
    "Merkmale (Spalten 2-22) sind kinematische Eigenschaften, die von den Teilchendetektoren im Beschleuniger gemessen wurden. Die letzten sieben Merkmale sind Funktionen der ersten 21 Merkmalen; es handelt sich dabei um hochrangige Merkmale, die von Physikern abgeleitet wurden, um zwischen den beiden Klassen zu unterscheiden. \n",
    "\n",
    "Es besteht ein grosses Interesse an der Verwendung von Deep Learning Methoden, damit die Physiker solche Merkmale nicht mehr manuell entwickeln müssen. \n",
    "\n",
    "Benchmark-Ergebnisse mit Bayes'schen Entscheidungsbäumen aus einem Standard einem Standard-Physikpaket und neuronalen Netzen mit 5 Schichten werden in der ursprünglichen Arbeit vorgestellt. [Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2,\n",
    "2014)](https://www.nature.com/articles/ncomms5308)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_if_not_exists(url, output_dir, filename):\n",
    "    \"\"\"\n",
    "    Lädt eine Datei von der angegebenen URL herunter, wenn sie noch nicht existiert.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL der herunterzuladenden Datei.\n",
    "        output_dir (str): Verzeichnis, in dem die Datei gespeichert wird.\n",
    "        filename (str): Name der Zieldatei.\n",
    "\n",
    "    Returns:\n",
    "        str: Pfad zur heruntergeladenen oder bestehenden Datei.\n",
    "    \"\"\"\n",
    "    # Erstelle das Verzeichnis, falls es nicht existiert\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Vollständiger Pfad zur Zieldatei\n",
    "    output_file = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Überprüfen, ob die Datei bereits existiert\n",
    "    if not os.path.exists(output_file):\n",
    "        print(f\"Datei wird heruntergeladen von {url}...\")\n",
    "        # Lade die Datei herunter und speichere sie\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(output_file, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:  # Leere Chunks filtern\n",
    "                    file.write(chunk)\n",
    "        print(f\"Download abgeschlossen. Datei gespeichert in: {output_file}\")\n",
    "    else:\n",
    "        print(f\"Datei existiert bereits: {output_file}\")\n",
    "\n",
    "    return output_file\n",
    "\n",
    "\n",
    "# URL der Datei\n",
    "url = \"https://mlphysics.ics.uci.edu/data/higgs/HIGGS.csv.gz\"\n",
    "\n",
    "# Zielverzeichnis und Dateiname\n",
    "output_dir = \"Daten\"\n",
    "filename = \"HIGGS.csv.gz\"\n",
    "\n",
    "# Funktion aufrufen\n",
    "downloaded_file = download_file_if_not_exists(url, output_dir, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lglk41MwvU5o"
   },
   "source": [
    "## Überanpassung\n",
    "\n",
    "Der einfachste Weg, eine Überanpassung zu verhindern, ist, mit einem kleinen Modell zu beginnen: Ein Modell mit einer kleinen Anzahl von lernbaren Parametern (die durch die Anzahl der Schichten und die Anzahl der Einheiten pro Schicht bestimmt wird). Beim Deep Learning wird die Anzahl der lernbaren Parameter in einem Modell oft als \"Kapazität\" des Modells bezeichnet.\n",
    "\n",
    "Intuitiv betrachtet hat ein Modell mit mehr Parametern eine grössere \"Speicherkapazität\" und kann daher leicht eine perfekte wörterbuchartige Zuordnung zwischen Trainingsmustern und ihren Zielen erlernen, eine Zuordnung ohne jegliche Generalisierungskraft, die jedoch nutzlos wäre, wenn es Vorhersagen für zuvor nicht gesehene Daten macht.\n",
    "\n",
    "- Denken Sie immer daran: Deep-Learning-Modelle neigen dazu, sich gut an die Trainingsdaten anzupassen, aber die eigentliche Herausforderung ist die Verallgemeinerung, nicht die Anpassung.\n",
    "\n",
    "- Wenn das Netzwerk andererseits nur über begrenzte Speicherressourcen verfügt, kann es das Mapping nicht so leicht erlernen. Um seinen Verlust zu minimieren, muss es komprimierte Darstellungen lernen, die eine höhere Vorhersagekraft haben. Wenn Sie Ihr Modell jedoch zu klein machen, wird es Schwierigkeiten haben, sich an die Trainingsdaten anzupassen. Es gibt ein Gleichgewicht zwischen \"zu viel Kapazität\" und \"nicht genug Kapazität\".\n",
    "\n",
    "- Leider gibt es keine magische Formel, um die richtige Grösse oder Architektur Ihres Modells zu bestimmen (in Bezug auf die Anzahl der Schichten oder die richtige Grösse für jede Schicht). Sie müssen mit einer Reihe von verschiedenen Architekturen experimentieren.\n",
    "\n",
    "- Um eine geeignete Modellgrösse zu finden, beginnen Sie am besten mit relativ wenigen Schichten und Parametern und beginnen dann, die Grösse der Schichten zu erhöhen oder neue Schichten hinzuzufügen, bis Sie eine Verringerung des Validierungsverlustes feststellen.\n",
    "\n",
    "Beginnen Sie mit einem einfachen Modell, das nur `nn.Linear` als Basis verwendet, erstellen Sie dann grössere Versionen und vergleichen Sie diese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ReKHdC2EgVu"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses `config_dict` ist ein Python Dictionary, das verschiedene Konfigurationsparameter für das Training eines neuronalen Netzes in PyTorch Lighning enthält. Es ist in drei Hauptabschnitte unterteilt: dataset, model und training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    \"dataset\": {\n",
    "        \"path\": \"Daten/HIGGS.csv.gz\",  # Pfad zur Datensatzdatei\n",
    "        \"n_train\": 10000,  # Anzahl der Trainingsbeispiele\n",
    "        \"n_validation\": 1000,  # Anzahl der Validierungsbeispiele\n",
    "        \"batch_size\": 50,  # Batch-Größe für Training und Validierung\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"max_epochs\": 300,  # Maximale Anzahl der Trainingsepochen\n",
    "        \"early_stopping_patience\": 50,  # Geduld für Early Stopping\n",
    "        \"learning_rate\": 0.0001,  # Lernrate für den Optimierer\n",
    "        \"use_gpu\": True,  # Flag zur Nutzung der GPU für das Training\n",
    "        \"log_dir\": \"lightning_logs\",  # Verzeichnis zum Speichern der Logs\n",
    "        \"log_name\": \"model_logs\",  # Name für die Log-Dateien\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Klasse `HiggDataset` ist eine benutzerdefinierte Datasetklasse. Sie wird verwendet, um den Higgs-Datensatz zu laden und für das Training die Validierung von neuronalen Netzen in PyTorch vorzubereiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Dataset Definition -----------------------------\n",
    "class HiggsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Die erste Spalte der CSV enthält die Labels (Zielwerte)\n",
    "        self.labels = (\n",
    "            torch.tensor(data.iloc[:, 0].values, dtype=torch.float32)\n",
    "            if not isinstance(data.iloc[:, 0].values, torch.Tensor)\n",
    "            else data.iloc[:, 0].values\n",
    "        )\n",
    "        # Die restlichen Spalten enthalten die Features\n",
    "        self.features = (\n",
    "            torch.tensor(data.iloc[:, 1:].values, dtype=torch.float32)\n",
    "            if not isinstance(data.iloc[:, 1:].values, torch.Tensor)\n",
    "            else data.iloc[:, 1:].values\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `create_model` nimmt `input_size`, `hidden_layers` und erstellt die Schichten die im `config_...`-File vorgegeben werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Modellarchitektur -----------------------------\n",
    "def create_model(input_size, hidden_layers, dropout_rate=0.0, l2_reg=0.0):\n",
    "    layers = []\n",
    "    prev_dim = input_size\n",
    "    for layer_dim in hidden_layers:\n",
    "        layers.append(nn.Linear(prev_dim, layer_dim))\n",
    "        layers.append(nn.ELU())\n",
    "        if dropout_rate > 0:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        prev_dim = layer_dim\n",
    "    layers.append(nn.Linear(prev_dim, 1))\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Klasse `LightningModel` dient zur Verwaltung von Training, Validierung und Optimierung eines neuronalen Netzwerkes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- LightningModule -----------------------------\n",
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model, lr=0.0001, l2_reg=0.0):\n",
    "        super(LightningModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.l2_reg = l2_reg\n",
    "        self.training_losses = []  # Speichert den Trainingsverlust pro Epoche\n",
    "        self.validation_losses = []  # Speichert den Validierungsverlust pro Epoche\n",
    "        self.training_accuracy = []  # Speichert die Trainingsgenauigkeit pro Epoche\n",
    "        self.validation_accuracy = []  # Speichert die Validierungsgenauigkeit pro Epoche\n",
    "        self.accuracy_metric = torchmetrics.Accuracy(task=\"binary\").to(\n",
    "            self.device\n",
    "        )  # Genauigkeitsmetrik\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)  # Vorwärtsdurchlauf\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.unsqueeze(\n",
    "            1\n",
    "        )  # Anpassen der Dimension, damit y mit den Vorhersagen übereinstimmt\n",
    "        y_hat = self(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(y_hat, y)  # Berechnung des Verlusts\n",
    "        # Berechnung der Genauigkeit\n",
    "        preds = torch.sigmoid(y_hat) > 0.5  # Schwelle bei 0.5\n",
    "        acc = self.accuracy_metric(preds, y.int())\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )  # Loggen des Trainingsverlusts\n",
    "        self.log(\n",
    "            \"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )  # Loggen der Trainingsgenauigkeit\n",
    "        return {\"loss\": loss, \"acc\": acc}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.unsqueeze(1)  # Anpassen der Dimension\n",
    "        y_hat = self(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(y_hat, y)  # Berechnung des Verlusts\n",
    "        # Berechnung der Genauigkeit\n",
    "        preds = torch.sigmoid(y_hat) > 0.5\n",
    "        acc = self.accuracy_metric(preds, y.int())\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )  # Loggen des Validierungsverlusts\n",
    "        self.log(\n",
    "            \"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )  # Loggen der Validierungsgenauigkeit\n",
    "\n",
    "        return {\"loss\": loss, \"accuracy\": acc}\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = self.trainer.callback_metrics[\n",
    "            \"train_loss\"\n",
    "        ].item()  # Durchschnittlicher Trainingsverlust\n",
    "        avg_acc = self.trainer.callback_metrics[\n",
    "            \"train_acc\"\n",
    "        ].item()  # Durchschnittliche Trainingsgenauigkeit\n",
    "        self.training_losses.append(avg_loss)\n",
    "        self.training_accuracy.append(avg_acc)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = self.trainer.callback_metrics[\n",
    "            \"val_loss\"\n",
    "        ].item()  # Durchschnittlicher Validierungsverlust\n",
    "        avg_acc = self.trainer.callback_metrics[\n",
    "            \"val_acc\"\n",
    "        ].item()  # Durchschnittliche Validierungsgenauigkeit\n",
    "        self.validation_losses.append(avg_loss)\n",
    "        self.validation_accuracy.append(avg_acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(), lr=self.lr, weight_decay=self.l2_reg\n",
    "        )  # Optimierer\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.1, patience=10\n",
    "        )  # Lernraten-Scheduler\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"},\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Trainingsfunktion -----------------------------\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    validate_loader,\n",
    "    max_epochs=config_dict[\"training\"][\"max_epochs\"],\n",
    "    logger=None,\n",
    "    callbacks=None,\n",
    "    use_gpu=True,\n",
    "):\n",
    "    # Logger festlegen (CSVLogger, falls keiner angegeben ist)\n",
    "    logger = CSVLogger(\"logs\", name=\"model_logs\") if logger is None else logger\n",
    "\n",
    "    # Trainer-Instanz von PyTorch Lightning erstellen\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,  # Maximale Anzahl der Epochen\n",
    "        logger=logger,  # Logger für Trainingsmetriken\n",
    "        callbacks=callbacks,  # Callbacks (z.B. EarlyStopping)\n",
    "        check_val_every_n_epoch=1,  # Validierung nach jeder Epoche\n",
    "        accelerator=\"gpu\"\n",
    "        if use_gpu and torch.cuda.is_available()\n",
    "        else \"cpu\",  # Beschleuniger (GPU oder CPU)\n",
    "        devices=[0]\n",
    "        if use_gpu and torch.cuda.is_available()\n",
    "        else None,  # Geräte (nur GPU 0, falls verfügbar)\n",
    "    )\n",
    "\n",
    "    # Training des Modells\n",
    "    trainer.fit(model, train_loader, validate_loader)\n",
    "\n",
    "    # Plotten des Verlaufs der Trainings- und Validierungsverluste\n",
    "    plt.figure()\n",
    "    plt.plot(model.training_losses, label=\"Training Loss\")\n",
    "    plt.plot(model.validation_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoche\")\n",
    "    plt.ylabel(\"Binary Crossentropy Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Verlauf der Verluste\")\n",
    "    plt.show()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Trainingsfunktion mit EarlyStopping -----------------------------\n",
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    validate_loader,\n",
    "    max_epochs=config_dict[\"training\"][\"max_epochs\"],\n",
    "    use_gpu=True,\n",
    "):\n",
    "    # EarlyStopping-Callback erstellen, um das Training zu beenden, wenn sich der Validierungsverlust nicht verbessert\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",  # Überwachen des Validierungsverlusts\n",
    "        patience=config_dict[\"training\"][\n",
    "            \"early_stopping_patience\"\n",
    "        ],  # Geduld, bevor das Training gestoppt wird\n",
    "        mode=\"min\",  # Minimierungsmodus, da wir den Verlust minimieren möchten\n",
    "    )\n",
    "    # Modell mit EarlyStopping-Callback trainieren\n",
    "    return train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        validate_loader,\n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[early_stopping],\n",
    "        use_gpu=use_gpu,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Datenvorbereitung -----------------------------\n",
    "gz_path = config_dict[\"dataset\"][\"path\"]  # Pfad zur komprimierten Datensatzdatei\n",
    "data = (\n",
    "    pd.read_csv(gz_path, compression=\"gzip\", header=None)  # Laden der CSV-Datei\n",
    "    .sample(frac=1)  # Zufälliges Durchmischen der Daten\n",
    "    .reset_index(drop=True)  # Zurücksetzen der Indizes\n",
    ")\n",
    "\n",
    "N_TRAIN = config_dict[\"dataset\"][\"n_train\"]  # Anzahl der Trainingsbeispiele\n",
    "N_VALIDATION = config_dict[\"dataset\"][\n",
    "    \"n_validation\"\n",
    "]  # Anzahl der Validierungsbeispiele\n",
    "BATCH_SIZE = config_dict[\"dataset\"][\"batch_size\"]  # Batch-Grösse\n",
    "\n",
    "data_size = N_TRAIN + N_VALIDATION  # Gesamtanzahl der benötigten Daten\n",
    "data = data.iloc[:data_size]  # Auswahl der benötigten Daten\n",
    "\n",
    "# Extrahieren der Features (X) und Labels (y) aus den Daten\n",
    "X = torch.tensor(data.iloc[:, 1:].values, dtype=torch.float32)\n",
    "y = torch.tensor(data.iloc[:, 0].values, dtype=torch.float32)\n",
    "\n",
    "# Erstellen des TensorDatasets und Aufteilen in Trainings- und Validierungsdatensätze\n",
    "dataset = TensorDataset(X, y)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [N_TRAIN, N_VALIDATION], generator=generator\n",
    ")\n",
    "\n",
    "# Erstellen der DataLoader für Training und Validierung\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4\n",
    ")\n",
    "validate_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxBeiLUiWHJV"
   },
   "source": [
    "### (c) Winziges Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6JDv12scLTI"
   },
   "source": [
    "Wir starten mit einem sehr kleinen Modell, das wahrscheinlich nicht an Überanpassung leidet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config file vor jedem training und gut am Anfang beschreiben, und alle helper Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_tiny = {\n",
    "    \"model\": {\n",
    "        \"input_size\": 28,  # Eingangsgröße des Modells\n",
    "        \"tiny\": {\n",
    "            \"hidden_layers\": [16],\n",
    "            \"dropout_rate\": 0.0,\n",
    "            \"l2_reg\": 0.0,\n",
    "        },  # Konfiguration für Tiny-Modell\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Basis Tiny Model...\")\n",
    "# printe die Konfiguration des Modells\n",
    "print(config_tiny[\"model\"][\"tiny\"])\n",
    "tiny_model = create_model(\n",
    "    config_tiny[\"model\"][\"input_size\"], **config_tiny[\"model\"][\"tiny\"]\n",
    ")\n",
    "tiny_lightning_model = LightningModel(tiny_model)\n",
    "tiny_trained = train_with_early_stopping(\n",
    "    tiny_lightning_model, train_loader, validate_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGxGzh_FWOJ8"
   },
   "source": [
    "### (d) Kleines Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjMb6E72f2pN"
   },
   "source": [
    "Um zu sehen, ob Sie die Leistung des kleinen Modells übertreffen können, trainieren Sie nach und nach einige grössere Modelle.\n",
    "\n",
    "Versuchen Sie es mit zwei versteckten Schichten mit je 16 Einheiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_small = {\n",
    "    \"model\": {\n",
    "        \"input_size\": 28,  # Eingangsgröße des Modells\n",
    "        \"small\": {\n",
    "            \"hidden_layers\": [16, 16],\n",
    "            \"dropout_rate\": 0.0,\n",
    "            \"l2_reg\": 0.0,\n",
    "        },  # Konfiguration für Small-Modell\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Small Model (Basis, [16,16])...\")\n",
    "# printe die Konfiguration des Modells\n",
    "print(config_small[\"model\"][\"small\"])\n",
    "small_model = create_model(\n",
    "    config_small[\"model\"][\"input_size\"], **config_small[\"model\"][\"small\"]\n",
    ")\n",
    "small_lightning_model = LightningModel(small_model)\n",
    "small_trained = train_with_early_stopping(\n",
    "    small_lightning_model, train_loader, validate_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-DGRBbGxI6G"
   },
   "source": [
    "### (e) Mittelgrosses Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrfoVQheYSO5"
   },
   "source": [
    "Testen Sie nun ein Modell mit drei versteckten Schichten (hidden layers) mit je 64 Einheiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_medium = {\n",
    "    \"model\": {\n",
    "        \"input_size\": 28,  # Eingangsgröße des Modells\n",
    "        \"medium\": {\n",
    "            \"hidden_layers\": [64, 64, 64],\n",
    "            \"dropout_rate\": 0.0,\n",
    "            \"l2_reg\": 0.0,\n",
    "        },  # Konfiguration für Medium-Modell\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Medium Model (Basis, [64,64,64])...\")\n",
    "# printe die Konfiguration des Modells\n",
    "print(config_medium[\"model\"][\"medium\"])\n",
    "medium_model = create_model(\n",
    "    config_medium[\"model\"][\"input_size\"], **config_medium[\"model\"][\"medium\"]\n",
    ")\n",
    "medium_lightning_model = LightningModel(medium_model)\n",
    "medium_trained = train_with_early_stopping(\n",
    "    medium_lightning_model, train_loader, validate_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIPuf23FFaVn"
   },
   "source": [
    "### (f) grosses Modell\n",
    "\n",
    "Zu Übungszwecken können Sie ein noch grösseres Modell erstellen und sehen, wie schnell es anfängt, sich übermässig anzupassen.  Als Nächstes fügen wir zu diesem Benchmark ein Netzwerk hinzu, das viel mehr Kapazität hat, weit mehr als das Problem rechtfertigen würde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_large = {\n",
    "    \"model\": {\n",
    "        \"input_size\": 28,  # Eingangsgröße des Modells\n",
    "        \"large\": {\n",
    "            \"hidden_layers\": [512, 512, 512, 512],\n",
    "            \"dropout_rate\": 0.0,\n",
    "            \"l2_reg\": 0.0,\n",
    "        },  # Konfiguration für Large-Modell\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Large Model (Basis, [512,512,512,512])...\")\n",
    "# printe die Konfiguration des Modells\n",
    "print(config_large[\"model\"][\"large\"])\n",
    "large_model = create_model(\n",
    "    config_large[\"model\"][\"input_size\"], **config_large[\"model\"][\"large\"]\n",
    ")\n",
    "large_lightning_model = LightningModel(large_model)\n",
    "large_trained = train_with_early_stopping(\n",
    "    large_lightning_model, train_loader, validate_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy3CMUZpzH3d"
   },
   "source": [
    "### (g) Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLhL1AszdLfM"
   },
   "source": [
    "Die Erstellung eines grösseren Modells verleiht ihm zwar mehr Leistung, aber wenn diese Leistung nicht irgendwie eingeschränkt wird, kann es leicht zu einer Überanpassung an den Trainingssatz kommen.\n",
    "\n",
    "In diesem Beispiel gelingt es typischerweise nur dem \"kleinen\" Modell, eine Überanpassung ganz zu vermeiden, und jedes der größeren Modelle passt die Daten schneller über. Für das \"große\" Modell ist dies so gravierend, dass man die Darstellung auf eine logarithmische Skala umstellen muss, um wirklich zu sehen, was passiert.\n",
    "\n",
    "Dies wird deutlich, wenn Sie die Validierungsmetriken aufzeichnen und mit den Trainingsmetriken vergleichen.\n",
    "\n",
    "* Es ist normal, dass es einen kleinen Unterschied gibt.\n",
    "* Wenn sich beide Metriken in dieselbe Richtung bewegen, ist alles in Ordnung.\n",
    "* Wenn die Validierungskennzahl zu stagnieren beginnt, während sich die Trainingskennzahl weiter verbessert, sind Sie wahrscheinlich nahe an einer Überanpassung.\n",
    "* Wenn sich die Validierungsmetrik in die falsche Richtung bewegt, ist das Modell eindeutig überangepasst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSlo1F4xHuuM"
   },
   "source": [
    "Die durchgezogenen Linien zeigen den Trainingsverlust, die gestrichelten Linien den Validierungsverlust (zur Erinnerung: ein geringerer Validierungsverlust bedeutet ein besseres Modell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UekcaQdmZxnW"
   },
   "source": [
    "Note: All the above training runs used the `callbacks.EarlyStopping` to end the training once it was clear the model was not making progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASdv7nsgEFhx"
   },
   "source": [
    "## Strategien zur Regularisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN512ksslaxJ"
   },
   "source": [
    "Bevor Sie sich mit dem Inhalt dieses Abschnitts befassen, kopieren Sie die Trainingsprotokolle des obigen Modells \"Tiny\", um sie als Vergleichsgrundlage zu verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rHoVWcswFLa"
   },
   "source": [
    "### (h) Gewichtsregularisierung (weight regularization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRxWepNawbBK"
   },
   "source": [
    "Vielleicht kennen Sie das Prinzip von **Occams Rasiermesser**: Wenn es zwei Erklärungen für etwas gibt, ist die Erklärung, die am wahrscheinlichsten richtig ist, die \"einfachste\", diejenige, die die wenigsten Annahmen enthält. Dies gilt auch für die Modelle, die von neuronalen Netzen gelernt werden: Bei bestimmten Trainingsdaten und einer Netzarchitektur gibt es mehrere Sätze von Gewichtungswerten (mehrere Modelle), die die Daten erklären könnten, und bei einfacheren Modellen ist die Wahrscheinlichkeit einer Überanpassung geringer als bei komplexen Modellen.\n",
    "\n",
    "Ein \"einfaches Modell\" ist in diesem Zusammenhang ein Modell, bei dem die Verteilung der Parameterwerte eine geringere Entropie aufweist (oder ein Modell mit insgesamt weniger Parametern, wie wir im obigen Abschnitt gesehen haben). Eine gängige Methode zur Abschwächung der Überanpassung besteht daher darin, die Komplexität eines Netzes einzuschränken, indem seine Gewichte gezwungen werden, nur kleine Werte anzunehmen, wodurch die Verteilung der Gewichtswerte \"regelmässiger\" wird. Dies wird als \"Gewichtsregulierung\" bezeichnet und erfolgt durch Hinzufügen von Kosten zur Verlustfunktion des Netzes, die mit großen Gewichten verbunden sind. Diese Kosten gibt es in zwei Varianten:\n",
    "\n",
    "* [L1-Regularisierung](https://developers.google.com/machine-learning/glossary/#L1_regularization), bei der die hinzugefügten Kosten proportional zum absoluten Wert der Gewichtskoeffizienten sind (d. h. zur so genannten \"L1-Norm\" der Gewichte).\n",
    "\n",
    "* [L2-Regularisierung](https://developers.google.com/machine-learning/glossary/#L2_regularization), bei der die zusätzlichen Kosten proportional zum Quadrat des Wertes der Gewichtungskoeffizienten sind (d.h. zu dem, was man die quadrierte \"L2-Norm\" der Gewichte nennt). Die L2-Regularisierung wird im Zusammenhang mit neuronalen Netzen auch als Gewichtsabnahme bezeichnet. Lassen Sie sich durch die unterschiedliche Bezeichnung nicht verwirren: Gewichtsabnahme ist mathematisch gesehen genau dasselbe wie L2-Regularisierung.\n",
    "\n",
    "Die L1-Regularisierung verschiebt einige Gewichte genau gegen Null, was ein spärliches Modell fördert. Die L2-Regularisierung bestraft die Parameter der Gewichte, ohne sie spärlich zu machen, da die Strafe bei kleinen Gewichten gegen Null geht - ein Grund, warum L2 häufiger verwendet wird.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUUHoXb7w-_C"
   },
   "source": [
    "`l2(0.001)` bedeutet, dass jeder Koeffizient in der Gewichtsmatrix der Schicht `0.001 * weight_coefficient_value**2` zum gesamten **Verlust** des Netzes beiträgt.\n",
    "\n",
    "Deshalb überwachen wir die `binary_crossentropy` direkt, weil sie diese Regularisierungskomponente nicht enthält.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLhG6fMSjE-J"
   },
   "source": [
    "Es gibt einen zweiten Ansatz, bei dem der Optimierer nur auf den Rohverlust angewandt wird und dann während der Anwendung des berechneten Schritts auch ein gewisser Gewichtsabbau erfolgt. Dieses \"Decoupled Weight Decay\" findet sich in Optimierern wie `torch.optim.AdamW`. (`torch.optim.Adam`-> gekoppeltes Weight Decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_L2 = {\n",
    "    \"model\": {\n",
    "        \"input_size\": 28,  # Eingangsgröße des Modells\n",
    "        \"large_l2\": {\n",
    "            \"hidden_layers\": [512, 512, 512, 512],\n",
    "            \"dropout_rate\": 0.0,\n",
    "            \"l2_reg\": 0.001,\n",
    "        },  # Tiny-Modell mit L2-Regularisierung\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Large Model mit L2-Regularisierung...\")\n",
    "# printe die Konfiguration des Modells\n",
    "print(config_L2[\"model\"][\"large_l2\"])\n",
    "large_l2_model = create_model(\n",
    "    config_L2[\"model\"][\"input_size\"], **config_L2[\"model\"][\"large_l2\"]\n",
    ")\n",
    "large_l2_lightning_model = LightningModel(\n",
    "    large_l2_model, l2_reg=config_L2[\"model\"][\"large_l2\"][\"l2_reg\"]\n",
    ")\n",
    "large_l2_trained = train_with_early_stopping(\n",
    "    large_l2_lightning_model, train_loader, validate_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmnBNOOVxiG8"
   },
   "source": [
    "### (i) Dropout hinzufügen\n",
    "\n",
    "Dropout ist eine der effektivsten und am häufigsten verwendeten Regularisierungstechniken für neuronale Netze, die von Hinton und seinen Studenten an der Universität von Toronto entwickelt wurde.\n",
    "\n",
    "Die intuitive Erklärung für Dropout ist, dass sich einzelne Knoten im Netz nicht auf die Ausgaben der anderen verlassen können, sondern dass jeder Knoten für sich selbst nützliche Merkmale ausgeben muss.\n",
    "\n",
    "Dropout, angewandt auf eine Schicht, besteht darin, dass während des Trainings eine Anzahl von Ausgangsmerkmalen der Schicht zufällig \"weggelassen\" (d. h. auf Null gesetzt) wird. Nehmen wir an, eine gegebene Schicht hätte normalerweise einen Vektor `[0.2, 0.5, 1.3, 0.8, 1.1]` für eine gegebene Eingabeprobe während des Trainings geliefert; nach Anwendung von Dropout wird dieser Vektor einige zufällig verteilte Nulleinträge haben, z. B. `[0, 0.5, 1.3, 0, 1.1]`.\n",
    "\n",
    "Die \"Dropout-Rate\" ist der Anteil der Merkmale, die mit Nullen versehen werden; sie wird normalerweise zwischen `0.2` und `0.5` festgelegt. Zur Testzeit werden keine Einheiten herausgenommen, stattdessen werden die Ausgabewerte der Schicht um den Faktor der Dropout-Rate herunterskaliert, um die Tatsache auszugleichen, dass mehr Einheiten aktiv sind als zur Trainingszeit.\n",
    "\n",
    "In `PyTorch Lightning` können Sie Dropouts in einem Netzwerk über die `nn.Dropout`-Schicht einführen, die auf die Ausgabe der Schicht direkt davor angewendet wird.\n",
    "\n",
    "Fügen wir Dropout-Schichten in unser Netzwerk ein, um zu sehen, wie gut sie das Overfitting reduzieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dropout = {\n",
    "    \"model\": {\n",
    "        \"input_size\": 28,  # Eingangsgröße des Modells\n",
    "        \"large_dropout\": {\n",
    "            \"hidden_layers\": [512, 512, 512, 512],\n",
    "            \"dropout_rate\": 0.5,\n",
    "            \"l2_reg\": 0.0,\n",
    "        },  # Tiny-Modell mit Dropout\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Large Model mit Dropout...\")\n",
    "# printe die Konfiguration des Modells\n",
    "print(config_dropout[\"model\"][\"large_dropout\"])\n",
    "large_dropout_model = create_model(\n",
    "    config_dropout[\"model\"][\"input_size\"], **config_dropout[\"model\"][\"large_dropout\"]\n",
    ")\n",
    "large_dropout_lightning_model = LightningModel(large_dropout_model)\n",
    "large_dropout_trained = train_with_early_stopping(\n",
    "    large_dropout_lightning_model, train_loader, validate_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7qMg_7Nwy5t"
   },
   "source": [
    "### (j) L2-Regularisierung und Dropout kombiniert\n",
    "\n",
    "Versuchen Sie als Nächstes beide zusammen, um zu sehen, ob das besser funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_L2_dropout = {\n",
    "    \"model\": {\n",
    "        \"input_size\": 28,  # Eingangsgröße des Modells\n",
    "        \"large_l2_dropout\": {\n",
    "            \"hidden_layers\": [512, 512, 512, 512],\n",
    "            \"dropout_rate\": 0.5,\n",
    "            \"l2_reg\": 0.001,\n",
    "        },  # Tiny-Modell mit Dropout und L2-Regularisierung\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Large Model mit Dropout und L2-Regularisierung...\")\n",
    "# printe die Konfiguration des Modells\n",
    "print(config_L2_dropout[\"model\"][\"large_l2_dropout\"])\n",
    "large_l2_dropout_model = create_model(\n",
    "    config_L2_dropout[\"model\"][\"input_size\"],\n",
    "    **config_L2_dropout[\"model\"][\"large_l2_dropout\"],\n",
    ")\n",
    "large_l2_dropout_lightning_model = LightningModel(\n",
    "    large_l2_dropout_model,\n",
    "    l2_reg=config_L2_dropout[\"model\"][\"large_l2_dropout\"][\"l2_reg\"],\n",
    ")\n",
    "large_l2_dropout_trained = train_with_early_stopping(\n",
    "    large_l2_dropout_lightning_model, train_loader, validate_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparisons(models, metrics, titles, xlabel=\"Epoche\"):\n",
    "    \"\"\"\n",
    "    Erstellt Vergleichsplots für verschiedene Modelle und Metriken als Subplots.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Dictionary mit Modellnamen als Keys und einem weiteren Dictionary mit Metriken als Werten.\n",
    "                       Beispiel:\n",
    "                       {\n",
    "                           \"Tiny Basis [16]\": tiny_trained,\n",
    "                           \"Tiny + Dropout\": tiny_dropout_trained\n",
    "                       }\n",
    "        metrics (list): Liste der Metriken als Strings, z.B. [\"training_losses\", \"validation_losses\"]\n",
    "        titles (list): Liste der Titel für die Plots in der gleichen Reihenfolge wie die Metriken.\n",
    "        xlabel (str): Bezeichnung der x-Achse (Standard: \"Epoche\").\n",
    "    \"\"\"\n",
    "    num_metrics = len(metrics)\n",
    "    fig, axes = plt.subplots(num_metrics, 1, figsize=(10, 6 * num_metrics))\n",
    "\n",
    "    for ax, metric, title in zip(axes, metrics, titles):\n",
    "        for model_name, model_data in models.items():\n",
    "            ax.plot(getattr(model_data, metric), label=model_name)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(metric.replace(\"_\", \" \").capitalize())\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------- Anwendung der Funktion -----------------------------\n",
    "\n",
    "# 1. Vergleich der Tiny-Modellvarianten\n",
    "tiny_models = {\n",
    "    \"Tiny Basis [16]\": tiny_trained,\n",
    "    \"large + Dropout\": large_dropout_trained,\n",
    "    \"large + L2\": large_l2_trained,\n",
    "    \"large + Dropout und L2\": large_l2_dropout_trained,\n",
    "}\n",
    "\n",
    "plot_model_comparisons(\n",
    "    tiny_models,\n",
    "    metrics=[\n",
    "        \"training_losses\",\n",
    "        \"validation_losses\",\n",
    "        \"training_accuracy\",\n",
    "        \"validation_accuracy\",\n",
    "    ],\n",
    "    titles=[\n",
    "        \"Vergleich der Trainingsverluste\",\n",
    "        \"Vergleich der Validierungsverluste\",\n",
    "        \"Vergleich der Trainings-Accuracy\",\n",
    "        \"Vergleich der Validierungs-Accuracy\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 2. Vergleich der Basis-Modelle\n",
    "base_models = {\n",
    "    \"Tiny Basis [16]\": tiny_trained,\n",
    "    \"Small [16,16]\": small_trained,\n",
    "    \"Medium [64,64,64]\": medium_trained,\n",
    "    \"Large [512,512,512,512]\": large_trained,\n",
    "}\n",
    "\n",
    "plot_model_comparisons(\n",
    "    base_models,\n",
    "    metrics=[\"training_losses\", \"validation_losses\"],\n",
    "    titles=[\n",
    "        \"Vergleich der Trainingsverluste: Basis-Modelle\",\n",
    "        \"Vergleich der Validierungsverluste: Basis-Modelle\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjfnkEeQyAFG"
   },
   "source": [
    "# Schlussfolgerungen\n",
    "\n",
    "Zusammenfassend lässt sich sagen, dass es die gängigsten Methoden gibt, um eine Überanpassung in neuronalen Netzen zu verhindern:\n",
    "\n",
    "* Mehr Trainingsdaten erhalten.\n",
    "* Verringern Sie die Kapazität des Netzes.\n",
    "* Hinzufügen einer Gewichtsregulierung.\n",
    "* Dropout hinzufügen.\n",
    "\n",
    "Zwei wichtige Ansätze, die in diesem Leitfaden nicht behandelt werden, sind:\n",
    "\n",
    "* Datenerweiterung\n",
    "* Batch-Normalisierung\n",
    "\n",
    "Denken Sie daran, dass jede Methode für sich genommen hilfreich sein kann, aber oft kann eine Kombination der Methoden noch effektiver sein.\n",
    "\n",
    "# Zusammenfassung Over- und Underfitting erkennen\n",
    "| Kategorie | Underfitting | Overfitting |\n",
    "|-----------|--------------|-------------|\n",
    "| **Symptome** | - Hoher Trainingsfehler<br>- Hoher Validierungsfehler<br>- Trainings- und Validierungskurven liegen nahe beieinander | - Sehr niedriger Trainingsfehler<br>- Hoher Validierungsfehler<br>- Grosse Lücke zwischen Trainings- und Validierungskurve |\n",
    "| **Ursachen** | - Modell zu einfach<br>- Zu starke Regularisierung<br>- Zu kurze Trainingszeit | - Modell zu komplex<br>- Zu langes Training<br>- Zu wenig Trainingsdaten |\n",
    "| **Lösungen** | - Komplexeres Modell verwenden<br>- Regularisierung reduzieren<br>- Mehr Trainingszeit (Epochen) | - Regularisierung hinzufügen/erhöhen (L1/L2)<br>- Dropout verwenden<br>- Data Augmentation einsetzen<br>- Earlystopping anwenden |\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "overfit_and_underfit.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
