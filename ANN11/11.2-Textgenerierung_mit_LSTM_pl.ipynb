{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN11/11.2-Textgenerierung_mit_LSTM_pl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 11. Generative Modelle\n",
    "\n",
    "Das Potenzial künstlicher Intelligenz, menschliche Denkvorgänge zu emulieren,\n",
    "geht weit über passive Aufgaben (wie Objekterkennung) oder vornehmlich reagierende\n",
    "Aufgaben (wie das Steuern eines Autos) hinaus. \n",
    "\n",
    "Es umfasst auch Tätigkeiten, die durchaus als *kreativ* bezeichnet werden können. Als François Chollet seinerzeit die Behauptung aufstellte, dass in einer gar nicht so fernen Zukunft die meisten von uns konsumierten kulturellen Inhalte mit beträchtlicher Hilfe der KI entstehen werden, schlug ihm eine Welle völligen Unglaubens entgegen – selbst von Forschern, die sich schon lange mit Machine Learning beschäftigt hatten. Das war 2014. Nur drei Jahre später hatte sich die Skepsis weitgehend gelegt – mit atemberaubender Geschwindigkeit. \n",
    "\n",
    "- Im Sommer 2015 haben wir uns darüber amüsiert, wie **Googles DeepDream**-Algorithmus Bilder in ein psychedelisches Durcheinander von Hundeaugen und pareidolischen Trugbildern verwandelte. \n",
    "- 2016 erschien die **Smartphone-App Prisma**, die Fotos in Gemälde eines bestimmten Stils umwandelt (Style Transfer).\n",
    "- Im Sommer 2016 wurde ein experimenteller Kurzfilm mit dem Titel **Sunspring** vorgestellt, bei dem nach einem Drehbuch Regie geführt wurde, das ein LSTM-Algorithmus verfasst hatte – inklusive der Dialoge. Und wer weiss – vielleicht haben Sie erst kürzlich Musik gehört, die testweise von einem neuronalen Netz erzeugt wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eine andere Art von Intelligenz\n",
    "\n",
    "- Zugegeben, die bislang von einer KI geschaffenen künstlerischen Werke sind von eher bescheidener Qualität. \n",
    "- Die KI ist gegenwärtig noch weit davon entfernt, mit menschlichen Drehbuchautoren, Malern oder Komponisten konkurrieren zu können.\n",
    "- **Aber Menschen zu ersetzen stand auch nie im Fokus**: Bei der KI geht es nicht darum, unsere eigene Intelligenz durch irgendetwas anderes abzulösen, vielmehr soll zusätzliche Intelligenz Einzug in unseren Alltag halten – eine andere Art von Intelligenz. \n",
    "- In vielen Bereichen, insbesondere in den kreativ geprägten, werden Menschen die KI als ein Werkzeug nutzen, das ihre eigenen Fähigkeiten verbessert – es handelt sich also eher um erweiterte Intelligenz als um künstliche Intelligenz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Was ist Kunst?\n",
    "\n",
    "Künstlerisches Schaffen ist im Wesentlichen ein *Zusammenspiel einfacher Mustererkennung\n",
    "und technischer Fähigkeiten*. Und genau diese Aspekte sind es, die von vielen Künstlern als weniger attraktiv oder sogar als entbehrlich erachtet werden. Hier kommt die KI ins Spiel. **Die Art unserer Wahrnehmung, unsere Sprache und unsere Kunstwerke sind von der Statistik geprägt**. \n",
    "- Deep-Learning-Algorithmen können diese Strukturen besonders gut erlernen. \n",
    "- Machine-Learning-Modelle können diesen *latenten Raum* von Bildern, Musik und Erzählungen statistisch erfassen und sich der Inhalte bedienen, um neue Kunstwerke mit Eigenschaften zu erschaffen, die denen ähnlich sind, die das Modell in den Trainingsdaten entdeckt hat. \n",
    "- Dabei handelt es sich natürlich kaum um einen Akt künstlerischen Schaffens, sondern vielmehr um eine mathematische Operation: Der Algorithmus weiss nichts von unserem Leben, menschlichen Emotionen oder unserer Sichtweise der Welt, sondern lernt aus Erfahrungen, die nur wenig mit den unsrigen gemeinsam haben. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kunstschaffen bereichern und erleichtern\n",
    "\n",
    "- Ein fähiger Künstler ist jedoch in der Lage, *Algorithmen einzusetzen, um Bedeutungsvolles und sogar Kunstvolles zu schaffen*.\n",
    "- Sich der Inhalte dieses latenten Raums zu bedienen, kann dem Künstler neue *Möglichkeiten eröffnen, unser kreatives Denken erweitern und unser Vorstellungsvermögen vergrössern*. \n",
    "- Darüber hinaus kann es den *Vorgang des künstlerischen Schaffens erleichtern*, weil Übung und technische Fähigkeiten nicht mehr erforderlich sind, und so eine neue Ausdrucksmöglichkeit bereitstellen, die Kunst und handwerkliches Geschick voneinander trennt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Iannes Xenakis**, der visionäre Pionier elektronischer und algorithmischer Musik,\n",
    "hat diese Idee schon in den 1960er-Jahren im Zusammenhang mit der Anwendung\n",
    "von Automatisierungstechnologien auf die Komposition von Musik sehr\n",
    "schön formuliert:[1]\n",
    "\n",
    "*Der lästigen Berechnungen ledig, kann sich der Komponist voll und ganz den eigentlichen\n",
    "Herausforderungen widmen, die neue musikalische Formen darstellen, eine neue\n",
    "Form bis in den letzten Winkel erkunden und gleichzeitig die Eingabedaten verändern.\n",
    "So kann er beispielsweise vom Solisten über das Kammerorchester bis zum Sinfonieorchester\n",
    "alle erdenklichen Kombinationen von Instrumenten ausprobieren. Dank des\n",
    "Computers wird der Komponist zu einer Art Pilot: Er drückt Knöpfe, gibt Koordinaten\n",
    "ein und überwacht die Anzeigen auf dem Armaturenbrett eines Raumschiffs, das in\n",
    "einem Klanguniversum durch Schallkonstellationen und Tongalaxien gleitet, von denen\n",
    "er früher nur im Traum einen kurzen Blick erhaschen konnte.*\n",
    "\n",
    "[1] Iannis Xenakis, *Musiques formelles: nouveaux principes formels de composition musicale*, Spezialausgabe\n",
    "von La Revue musicale, 253–254 (1963)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir werden folgende Themen streifen:\n",
    "* **Texterzeugung** mit einem LSTM-Layer\n",
    "* Implementierung von **DeepDream**\n",
    "* Stilübertragung mit dem **Neural-Style-Algorithmus**\n",
    "* **Variational Autoencoders**\n",
    "* **Generative Adversarial Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 11.1 Texterzeugung mit LSTM-Modellen\n",
    "\n",
    "This notebook contains the code samples found in Chapter 8, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). \n",
    "\n",
    "\n",
    "\n",
    "In diesem Abschnitt werden wir erkunden, wie sich rekurrente neuronale Netze\n",
    "zum Erzeugen sequenzieller Daten verwenden lassen. \n",
    "- Als Beispiel dient das **Erzeugen von Text**, das vorgestellte Verfahren lässt sich aber verallgemeinern, um beliebige sequenzielle Daten zu erzeugen. \n",
    "- Sie könnten es auch auf eine Folge von Noten anwenden, um **Musik zu erzeugen**, oder auf Zeitreihen von Pinselstrichen (die z.B. beim Malen eines Künstlers auf einem iPad aufgezeichnet wurden), um Strich für Strich Gemälde zu erzeugen usw.\n",
    "- Das Erzeugen sequenzieller Daten ist natürlich nicht auf künstlerische Inhalte beschränkt. Es wurde auch erfolgreich zur **Sprachsynthese** und zur **Dialogerzeugung für Chatbots** eingesetzt. \n",
    "- Das 2016 von Google vorgestellte Feature namens Smart Reply beruht auf einem vergleichbaren Verfahren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 11.2 Eine kurze Geschichte generativer RNNs\n",
    "\n",
    "#### Jürgen Schmidhuber und Sepp Hochreiter\n",
    "\n",
    "- Bis Ende 2014 kannte kaum ein Mensch die Buchstabenkombination LSTM, auch in der Machine-Learning-Community nicht. \n",
    "- Die erfolgreiche Anwendung der Erzeugung sequenzieller Daten mit rekurrenten neuronalen Netzen fand erst Mitte 2016 allmählich Verbreitung. \n",
    "- Allerdings hat dieses Verfahren eine ziemlich lange Vorgeschichte, die 1997 mit der Entwicklung des LSTM-Algorithmus ihren Anfang nahm [2]. Der neue Algorithmus wurde ursprünglich zur zeichenweisen Erzeugung von Texten verwendet.\n",
    "- Im Jahr 2002 nutzte *Douglas Eck*, der damals in *Schmidhubers Labor* in der Schweiz arbeitete, LSTM erstmals zum Erzeugen von Musik – mit vielversprechenden Ergebnissen. Eck ist inzwischen als Forscher bei Google Brain tätig und gründete dort 2016 eine Forschungsgruppe namens **Magenta**, die sich insbesondere mit der Anwendung von Deep-Learning-Verfahren auf die Erzeugung von Unterhaltungsmusik befasst. Manchmal dauert es eben 15 Jahre, bis eine gute Idee in die Tat umgesetzt wird.\n",
    "\n",
    "[2] Sepp Hochreiter und Jürgen Schmidhuber, *Long Short-Term Memory*, Neural Computation 9,\n",
    "Nr. 8 (1997)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Alex Graves\n",
    "\n",
    "Ende der 2000er- und Anfang der 2010er-Jahre leistete *Alex Graves* wichtige Pionierarbeit bei der Verwendung rekurrenter neuronaler Netze zur Erzeugung sequenzieller Daten. \n",
    "\n",
    "- Insbesondere seine Arbeit aus dem Jahr 2013 über rekurrente neuronale Netze mit Mischverteilung zur Erzeugung von handschriftähnlichen Bildern unter Verwendung von Zeitreihen der Stiftposition beim Schreiben kann als Wendepunkt betrachtet werden [3]. \n",
    "- Diese eine Anwendung neuronaler Netze zu genau diesem Zeitpunkt löste bei Francois Chollet die Vorstellung von träumenden Maschinen aus und hat ihn massgeblich inspiriert, als er mit der Entwicklung von Keras begann. \n",
    "- Versteckt in einer LaTeX-Datei, die Graves 2013 auf den Dokumentenserver für Preprints arXiv hochgeladen hatte, findet sich eine ähnliche auskommentierte Bemerkung: **Computer können dem Träumen nicht näherkommen als beim Erzeugen sequenzieller Daten.** \n",
    "- Inzwischen, einige Jahre später, halten wir viele dieser Entwicklungen für selbstverständlich, aber damals war es kaum möglich, Graves Experimente zu betrachten, ohne von den Möglichkeiten tief beeindruckt zu sein.\n",
    "\n",
    "[3] [Alex Graves, Generating Sequences With Recurrent Neural Networks, arXiv (2013)](https://arxiv.org/abs/1308.0850)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Seitdem sind rekurrente neuronale Netze erfolgreich zum\n",
    "- **Erzeugen von Musik**,\n",
    "- für **Sprachdialogsysteme**,\n",
    "- zur **Bilderzeugung**, \n",
    "- zur **Sprachsynthese** und \n",
    "- zum **Molekulardesign**\n",
    "eingesetzt worden. \n",
    "\n",
    "Sie wurden sogar dazu verwendet, ein Drehbuch für einen Film zu schreiben, dessen Rollen mit richtigen Schauspielern besetzt wurden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 11.3 Wie erzeugt man sequenzielle Daten?\n",
    "\n",
    "- Die allgemeine Methode, beim Deep Learning sequenzielle Daten zu erzeugen, ist das Trainieren eines neuronalen Netzes (für gewöhnlich ein RNN oder ein CNN), das ein Token oder die nächsten paar Tokens einer Sequenz vorhersagt, wobei die vorangegangenen Tokens als Eingabe dienen. \n",
    "- Bei der Eingabe »the cat is on the ma« wird das neuronale Netz beispielsweise darauf trainiert, den Zielwert »t« vorherzusagen, also den nächsten Buchstaben. Wie bei Textdaten üblich, sind die Tokens typischerweise Wörter oder Zeichen.\n",
    "- **Ein neuronales Netz, das die Wahrscheinlichkeitsverteilung für das nächste Token anhand der vorangegangenen Tokens vorhersagen kann, wird als Sprachmodell bezeichnet**. \n",
    "- Ein Sprachmodell erfasst den latenten Raum der Sprache: ihre statistische Struktur.\n",
    "\n",
    "\n",
    "$$ p(x_{t+1} \\vert x_t,x_{t-1}, \\dots x_{t-n})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sobald ein solches trainiertes Sprachmodell vorliegt, können Sie Stichproben entnehmen\n",
    "(neue Sequenzen erzeugen, das sogenannte **Sampling**): \n",
    "- Sie übergeben dem Modell einen Text die **Konditionierungsdaten**, weisen es an, das nächste Zeichen\n",
    "oder Wort zu erzeugen (Sie können sogar mehrere Tokens gleichzeitig erzeugen), fügen die erzeugte Ausgabe den Eingabedaten hinzu und wiederholen diesen Vorgang viele Male:\n",
    "\n",
    "\n",
    "<img src=\"Bilder/TextGeneration_LSTM.jpg\" width=\"640\"  align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ein neuronales Sprachmodell für Zeichen\n",
    "\n",
    "Diese Schleife ermöglicht es, Sequenzen beliebiger Länge zu erzeugen, die die Struktur der Daten widerspiegeln,\n",
    "mit denen das Modell trainiert wurde: Sequenzen, die fast wie von Menschen verfasste Sätze aussehen. \n",
    "\n",
    "- In dem hier vorgestellten Beispiel verwenden wir einen LSTM-Layer, dem `N` Zeichen übergeben werden, die einem Textkorpus entstammen.\n",
    "- Anschließend wird das Modell darauf trainiert, das Zeichen `N + 1` vorherzusagen.\n",
    "- Die Ausgabe des Modells ist eine `softmax`-Funktion aller möglichen Zeichen: eine Wahrscheinlichkeitsverteilung des nächsten Zeichens. Dieses LSTM-Modell bezeichnen wir als **neuronales Sprachmodell für Zeichen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 11.4 Die Bedeutung der Sampling-Strategie\n",
    "\n",
    "Bei der Erzeugung von Text ist die Methode zur Auswahl des nächsten Zeichens von entscheidender Bedeutung. \n",
    "- Das sogenannte **Greedy Sampling** ist ein naiver Ansatz, bei dem stets das Zeichen mit der höchsten Wahrscheinlichkeit ausgewählt wird. Allerdings führt ein solcher Ansatz zu vorhersagbaren Zeichenfolgen, die sich wiederholen und nicht wie zusammenhängende Sprache aussehen. \n",
    "- Ein interessanterer Ansatz trifft eine etwas überraschendere Auswahl und bringt beim Sampling den Zufall ins Spiel, indem die Wahrscheinlichkeitsverteilung des nachfolgenden Zeichens verwendet wird. Man bezeichnet das als **stochastisches\n",
    "Sampling** (stochastisch bedeutet in diesem Zusammenhang einfach nur zufallsabhängig).\n",
    "- Wenn also das nächste Zeichen dem Modell zufolge mit 30%iger Wahrscheinlichkeit ein `e` ist, wählen Sie es in 30% der Fälle aus. \n",
    "- Greedy Sampling kann ebenfalls als eine Wahrscheinlichkeitsverteilung aufgefasst werden, bei der ein bestimmtes Zeichen mit der Wahrscheinlichkeit 1 auftritt und alle anderen die Wahrscheinlichkeit 0 besitzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Die softmax-Funktion\n",
    "\n",
    "In der Wahrscheinlichkeitstheorie kann die Ausgabe der Softmax-Funktion genutzt werden, um eine kategoriale Verteilung – also eine Wahrscheinlichkeitsverteilung über K unterschiedliche mögliche Ereignisse – darzustellen. \n",
    "\n",
    "Die Softmax-Funktion wird in verschiedenen Methoden der Multiklassen-Klassifikation verwendet, wie bspw. bei der *multinomialen logistischen Regression* (auch bekannt als *Softmax-Regression*), der *multiklassen-bezogenen linearen Diskriminantenanalyse*, bei *Naïve-Bayes-Klassifikatoren* und *künstlichen neuronalen Netzen*. Insbesondere in der multinomialen logistischen Regression sowie der linearen Diskriminantenanalyse entspricht die Eingabe der Funktion dem Ergebnis von $K$ distinkten linearen Funktionen, und die ermittelte Wahrscheinlichkeit für die $j$-te Klasse gegeben ein Stichprobenvektor $\\mathbf{x}$ und einem Gewichtsvektor $\\mathbf{w}$ entspricht: \n",
    "\n",
    "$$ P(y=j\\mid \\mathbf {x} )={\\frac {e^{\\mathbf {x} ^{\\mathsf {T}}\\mathbf {w} _{j}}}{\\sum _{k=1}^{K}e^{\\mathbf {x} ^{\\mathsf {T}}\\mathbf {w} _{k}}}}$$\n",
    "\n",
    "Dies kann angesehen werden als Komposition von $K$ linearen Funktionen $\\mathbf{x} \\mapsto \\mathbf{x}^T \\mathbf{w}_{1},\\ldots ,\\mathbf{x} \\mapsto \\mathbf{x}^T \\mathbf{w} _{K}$ und der Softmax-Funktion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Das Sampling der Ausgabe der `softmax`-Funktion des Modells ist praktisch: Es ermöglicht, dass auch eigentlich unwahrscheinliche Zeichen hin und wieder ausgewählt werden. \n",
    "\n",
    "$$\\mathrm{softmax}(\\mathbf{x})_i=\\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "- Es erzeugt interessant aussehende Sätze und zeigt mitunter durch neue, realistisch klingende Wörter, die in den Trainingsdaten gar nicht vorkommen, sogar etwas Kreativität. \n",
    "- Diese Strategie hat jedoch den Nachteil, dass es nicht möglich ist, beim Sampling das *Ausmass der Zufälligkeit festzulegen*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Entropie: Warum sollte man sich mehr oder weniger Zufälligkeit wünschen? \n",
    "\n",
    "Betrachten Sie einen extremen Fall: **vollkommen zufälliges Sampling**, bei dem das nächste\n",
    "Zeichen gemäss einer gleichförmigen Wahrscheinlichkeitsverteilung ausgewählt\n",
    "wird und alle Zeichen gleich wahrscheinlich sind. Dieses Verfahren ist maximal\n",
    "zufällig – oder in anderen Worten: *Die Wahrscheinlichkeitsverteilung besitzt\n",
    "maximale Entropie*. Es liegt in der Natur der Sache, dass dabei nichts Interessantes\n",
    "herauskommen wird. \n",
    "\n",
    "Das andere Extrem ist **Greedy Sampling**, das ebenfalls nichts Interessantes liefert und keine Zufälligkeit aufweist: Die dazugehörige Wahrscheinlichkeitsverteilung besitzt *minimale Entropie*. \n",
    "\n",
    "Das Sampling mit der »echten« Wahrscheinlichkeitsverteilung, also mit der von der softmax-Funktion\n",
    "des Modells ausgegebenen, stellt einen *Mittelweg zwischen diesen beiden Extremen*\n",
    "dar. \n",
    "\n",
    "Es gibt allerdings noch viele weitere Möglichkeiten für Mittelwege mit\n",
    "höherer oder niedrigerer Entropie, die von Interesse sein können. Geringere Entropie\n",
    "verleiht der erzeugten Sequenz eine besser vorhersagbare Struktur (und deshalb wird diese Sequenz vermutlich realistischer erscheinen). Grössere Entropie\n",
    "hingegen führt zu überraschenderen und kreativeren Sequenzen. Beim\n",
    "Sampling generativer Modelle ist es immer sinnvoll, bei der Erzeugung von Sequenzen\n",
    "mit der Zufälligkeit zu experimentieren. Da letztendlich wir Menschen\n",
    "beurteilen, wie interessant die erzeugten Daten sind, ist diese Entscheidung natürlich\n",
    "äusserst subjektiv, und es lässt sich nicht vorhersagen, wie gross die optimale\n",
    "Entropie ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Softmax-Temperatur\n",
    "\n",
    "Um das Ausmass der Zufälligkeit beim Sampling festlegen zu können, führen wir\n",
    "einen Parameter namens Softmax-Temperatur ein, der die Entropie der für das\n",
    "Sampling verwendeten Wahrscheinlichkeitsverteilung angibt. Er ist ein Mass dafür,\n",
    "wie überraschend oder wie vorhersagbar die Auswahl des nächsten Zeichens\n",
    "sein wird. Aus einem temperature-Wert und der ursprünglichen Wahrscheinlichkeitsverteilung\n",
    "(der Ausgabe der softmax-Funktion des Modells) wird durch\n",
    "eine Neugewichtung eine neue Wahrscheinlichkeitsverteilung folgendermassen\n",
    "berechnet:\n",
    "\n",
    "$$\\mathrm{softmax}(\\mathbf{x},T)_i=\\frac{e^{\\frac{x_i}{T}}}{\\sum e^{\\frac{x_j}{T}}}$$\n",
    "\n",
    "Höhere Temperaturen führen zu Sampling-Verteilungen mit grösserer Entropie,\n",
    "die überraschendere und unstrukturiertere Daten erzeugen. Niedrigere Temperaturen\n",
    "hingegen führen zu weniger zufälligen, besser vorhersagbaren Daten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    return distribution / np.sum(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "T = [0.1, 1, 3, 100]\n",
    "samples = [3, 5, 2, 15, 6, 4, 21, 55, 43]\n",
    "original_distribution = samples / np.sum(samples)\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    reweight = reweight_distribution(original_distribution, temperature=T[i])\n",
    "    ax.bar(x=np.arange(len(samples)), height=reweight)\n",
    "    ax.grid(True)\n",
    "    ax.set_title(\"T=%f\" % T[i])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In diesem Beispiel verwenden wir ins Englische übertragene Texte des deutschen Philosophen\n",
    "*Friedrich Wilhelm Nietzsche* aus dem späten 19. Jahrhundert. Das erlernte\n",
    "Sprachmodell spiegelt also *Nietzsches Schreibstil* und die Themen seiner Wahl\n",
    "wider, ist also kein allgemeines Modell der englischen Sprache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeichenweise Textgenerierung – Erklärung der Datenvorverarbeitung\n",
    "\n",
    "In diesem Abschnitt geht es um die Vorbereitung der Daten für ein LSTM-Modell zur zeichenweisen Textgenerierung. Der Datensatz basiert auf den Schriften von Friedrich Nietzsche.\n",
    "\n",
    "\n",
    "## 1. Herunterladen des Nietzsche-Textes\n",
    "\n",
    "Zunächst wird geprüft, ob der Text lokal bereits vorhanden ist. Falls nicht, wird er aus dem Internet geladen:\n",
    "\n",
    "- Es wird ein Ordner `data` angelegt, wenn er noch nicht existiert.\n",
    "- Die Datei `nietzsche.txt` wird aus einer öffentlichen URL heruntergeladen.\n",
    "- Der Text wird lokal gespeichert, um mehrfaches Herunterladen zu vermeiden.\n",
    "\n",
    "**Ziel:** Sicherstellen, dass der Trainingsdatensatz lokal zur Verfügung steht.\n",
    "\n",
    "\n",
    "## 2. Verarbeitung des Textes in Sequenzen\n",
    "\n",
    "Der geladene Text wird wie folgt vorbereitet:\n",
    "\n",
    "### a. Umwandlung in Kleinbuchstaben\n",
    "Der gesamte Text wird in Kleinbuchstaben umgewandelt, um die Anzahl der verschiedenen Zeichen zu verringern.\n",
    "\n",
    "### b. Erstellung von Trainingssequenzen\n",
    "Aus dem Text werden sich überlappende Teilsequenzen erzeugt:\n",
    "- Jede Sequenz ist `maxlen` (z. B. 60) Zeichen lang.\n",
    "- Nach jeder Sequenz wird ein Zielzeichen extrahiert – das ist das Zeichen, das als nächstes vorhergesagt werden soll.\n",
    "- Mit dem Parameter `step` (z. B. 3) wird die Schrittweite gesteuert, mit der der Text durchlaufen wird.\n",
    "\n",
    "Beispiel bei `maxlen=10` und `step=3`:\n",
    "- Sequenz: `\"hallihallo\"`\n",
    "- Zielzeichen: `\"!\"` (das Zeichen nach dieser Sequenz)\n",
    "\n",
    "### c. Zeicheninventar (Vokabular)\n",
    "Es wird ein Alphabet aller im Text vorkommenden Zeichen erstellt:\n",
    "- `chars`: Liste aller eindeutigen Zeichen im Text.\n",
    "- `char_indices`: Dictionary zur Zuordnung jedes Zeichens zu einem eindeutigen Index (für One-Hot-Encoding).\n",
    "- `indices_char`: Umgekehrtes Dictionary zur Rückübersetzung von Indizes in Zeichen.\n",
    "\n",
    "**Ziel:** Vorbereitung der Daten in einer für das Modell geeigneten Form:\n",
    "- Sequenzen von Zeichen → Eingaben\n",
    "- Zielzeichen → Vorhersageziel\n",
    "\n",
    "\n",
    "## 3. Erstellung des PyTorch-Datasets\n",
    "\n",
    "Für das Training mit PyTorch wird ein benutzerdefiniertes `Dataset` erstellt. Dieses liefert bei jedem Aufruf ein Trainingsbeispiel in folgendem Format:\n",
    "\n",
    "### a. Eingabesequenz (`x`)\n",
    "- Größe: `[maxlen, vocab_size]`\n",
    "- Jede Zeile ist ein **One-Hot-Vektor**, der das jeweilige Zeichen repräsentiert.\n",
    "- Beispiel: Das Zeichen `\"a\"` an Position 3 wird als Vektor dargestellt, in dem nur die Stelle für `\"a\"` auf 1 steht, alle anderen auf 0.\n",
    "\n",
    "### b. Zielzeichen (`y`)\n",
    "- Integer-Wert (Index im Alphabet), der das **nächste Zeichen** repräsentiert.\n",
    "- Dieser wird später beim Training in eine Wahrscheinlichkeitsverteilung umgewandelt (via Softmax).\n",
    "\n",
    "**Ziel:** Jede Eingabesequenz mit ihrer erwarteten Zielausgabe (nächstes Zeichen) bereitstellen, um damit das LSTM-Modell zu trainieren.\n",
    "\n",
    "\n",
    "## Fazit\n",
    "\n",
    "Die Datenvorverarbeitung sorgt dafür, dass:\n",
    "- Ein zusammenhängender Text in eine große Anzahl überlappender Trainingsbeispiele zerlegt wird.\n",
    "- Jedes Beispiel als One-Hot-kodierter Tensor vorliegt.\n",
    "- Für jedes Beispiel das Zielzeichen eindeutig bestimmt ist.\n",
    "\n",
    "Diese Form ist optimal geeignet für zeichenweise Textgenerierung mit rekurrenten neuronalen Netzen wie LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 1. DATEN -----\n",
    "def download_nietzsche_text():\n",
    "    # verschiedene Texte und Autoren\n",
    "    url = r\"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/678ae4873e82f52ae1b563e32c12c6837fd5ae78/data/Gutenberg/nietzsche.txt\"\n",
    "    # url = r\"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/678ae4873e82f52ae1b563e32c12c6837fd5ae78/data/Gutenberg/Kant.txt\"\n",
    "    # url = r\"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/678ae4873e82f52ae1b563e32c12c6837fd5ae78/data/Gutenberg/shakespeare.txt\"\n",
    "    # url = r\"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/678ae4873e82f52ae1b563e32c12c6837fd5ae78/data/Gutenberg/Faust1.txt\"\n",
    "\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    path = \"data/nietzsche.txt\"\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Downloading text...\")\n",
    "        response = requests.get(url)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "    else:\n",
    "        print(\"Text already downloaded.\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def load_text(path, maxlen=60, step=3):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        text = f.read().lower()\n",
    "    print(\"Corpus length:\", len(text))\n",
    "\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i : i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    print(\"Number of sequences:\", len(sentences))\n",
    "\n",
    "    chars = sorted(list(set(text)))\n",
    "    print(\"Unique characters:\", len(chars))\n",
    "    char_indices = {char: idx for idx, char in enumerate(chars)}\n",
    "    indices_char = {idx: char for char, idx in char_indices.items()}\n",
    "\n",
    "    return text, sentences, next_chars, chars, char_indices, indices_char\n",
    "\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sentences, next_chars, char_indices, maxlen):\n",
    "        self.sentences = sentences\n",
    "        self.next_chars = next_chars\n",
    "        self.char_indices = char_indices\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = len(char_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.zeros(self.maxlen, self.vocab_size)\n",
    "        for t, char in enumerate(self.sentences[idx]):\n",
    "            x[t, self.char_indices[char]] = 1.0\n",
    "        y = self.char_indices[self.next_chars[idx]]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeichenweise Textgenerierung – Erklärung des LSTM-Modells\n",
    "\n",
    "In diesem Abschnitt wird das LSTM-Modell zur zeichenweisen Vorhersage mithilfe von PyTorch Lightning erklärt. Das Modell nimmt eine Sequenz von Zeichen (in One-Hot-Form) entgegen und sagt das nächste Zeichen vorher.\n",
    "\n",
    "\n",
    "## 1. Modellklasse `CharLSTM`\n",
    "\n",
    "Die Klasse erbt von `pl.LightningModule`, was typische Trainingsfunktionen kapselt und strukturiert.\n",
    "\n",
    "### Konstruktor `__init__`\n",
    "\n",
    "- **Parameter**:\n",
    "  - `vocab_size`: Anzahl einzigartiger Zeichen im Vokabular.\n",
    "  - `maxlen`: Länge der Eingabesequenzen (informativ).\n",
    "  - `lr`: Lernrate für den Optimierer.\n",
    "\n",
    "- **Layer**:\n",
    "  - `lstm1`: Erste LSTM-Schicht mit 32 versteckten Einheiten.\n",
    "  - `lstm2`: Zweite LSTM-Schicht mit denselben Dimensionen.\n",
    "  - `fc`: Voll verbundene Schicht, die auf die Vokabulargröße projiziert.\n",
    "\n",
    "- `self.save_hyperparameters()` speichert alle übergebenen Hyperparameter für Logging und Zugriff.\n",
    "\n",
    "\n",
    "## 2. Vorwärtsdurchlauf `forward`\n",
    "\n",
    "Der Forward-Pass verarbeitet die Eingabesequenz und gibt Vorhersagen (Logits) aus:\n",
    "\n",
    "- **Eingabe**: Tensor der Form `[batch_size, seq_len, vocab_size]` mit One-Hot-kodierten Zeichen.\n",
    "- Die Sequenz durchläuft zwei LSTM-Schichten.\n",
    "- Es wird **nur das letzte Zeitschritt-Output** `x[:, -1, :]` verwendet – also der finale Kontext der gesamten Sequenz.\n",
    "- Die finale lineare Schicht `fc` berechnet die Logits für jedes mögliche nächste Zeichen im Vokabular.\n",
    "\n",
    "\n",
    "## 3. Trainingsschritt `training_step`\n",
    "\n",
    "Dieser Schritt wird bei jedem Batch während des Trainings ausgeführt:\n",
    "\n",
    "- **Input `batch`**: Tupel `(x, y)`:\n",
    "  - `x`: Eingabesequenz (One-Hot-kodiert).\n",
    "  - `y`: Zielzeichen (als Integer).\n",
    "- Das Modell berechnet die Vorhersage `logits = self.forward(x)`.\n",
    "- Der **Kreuzentropieverlust** wird zwischen den Logits und den Ziel-Labels berechnet.\n",
    "- Der Verlust wird geloggt (`self.log(...)`) und zurückgegeben.\n",
    "\n",
    "Ziel: Das Modell lernt, Wahrscheinlichkeiten für das korrekte nächste Zeichen zu maximieren.\n",
    "\n",
    "\n",
    "## 4. Optimierer `configure_optimizers`\n",
    "\n",
    "- Der RMSprop-Optimierer wird verwendet, da er sich besonders gut für rekurrente Netze eignet.\n",
    "- Die Lernrate wird aus den gespeicherten Hyperparametern (`self.hparams.lr`) bezogen.\n",
    "\n",
    "\n",
    "## Fazit\n",
    "\n",
    "Das Modell `CharLSTM` ist ein einfaches, aber leistungsfähiges LSTM-Netzwerk zur Zeichen-Vorhersage:\n",
    "\n",
    "- **Zwei LSTM-Schichten** verarbeiten eine Sequenz aus One-Hot-Zeichen.\n",
    "- **Nur das letzte Zeitfenster** wird zur Vorhersage verwendet.\n",
    "- Eine **vollverbundene Schicht** berechnet Wahrscheinlichkeiten über alle Zeichen.\n",
    "- **PyTorch Lightning** sorgt für saubere Modularisierung und einfaches Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 2. MODELL -----\n",
    "class CharLSTM(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, maxlen, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lstm1 = torch.nn.LSTM(vocab_size, 32, batch_first=True)\n",
    "        self.lstm2 = torch.nn.LSTM(32, 32, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(32, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]  # nur letztes Time-Step\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=self.hparams.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeichenweise Textgenerierung – Erklärung des Textsamplings\n",
    "\n",
    "Nachdem das LSTM-Modell trainiert wurde, kann es genutzt werden, um neuen Text zu generieren. Dieser Abschnitt beschreibt die Sampling-Logik – also, wie aus Modellvorhersagen neue Zeichen entstehen – und wie ein zusammenhängender Text erzeugt wird.\n",
    "\n",
    "\n",
    "## 1. Sampling-Funktion `sample(...)`\n",
    "\n",
    "Diese Funktion wählt das nächste Zeichen auf Basis der Modellvorhersage aus.\n",
    "\n",
    "### Schrittweise Erklärung:\n",
    "\n",
    "- **Input**: \n",
    "  - `preds`: Wahrscheinlichkeiten (Softmax-Ausgabe) für jedes Zeichen im Vokabular.\n",
    "  - `temperature`: Kontrolliert die Kreativität bzw. Zufälligkeit der Auswahl.\n",
    "\n",
    "### Funktionsweise:\n",
    "\n",
    "1. Die Wahrscheinlichkeiten werden zuerst in `float64` konvertiert (für numerische Stabilität).\n",
    "2. Ein **Temperatur-Parameter** verändert die Wahrscheinlichkeitsverteilung:\n",
    "   - `temperature < 1.0`: konservativer, deterministischer (nur die wahrscheinlichsten Zeichen).\n",
    "   - `temperature > 1.0`: kreativer, zufälliger (auch weniger wahrscheinliche Zeichen möglich).\n",
    "3. `np.log(preds + 1e-8)`: Logarithmieren der Wahrscheinlichkeiten (Numerik-Tweak).\n",
    "4. Die Wahrscheinlichkeiten werden erneut normalisiert (Softmax-Schritt).\n",
    "5. `np.random.choice(...)`: Wählt ein Zeichen zufällig entsprechend der Wahrscheinlichkeitsverteilung.\n",
    "\n",
    "**Ziel:** Statt immer das wahrscheinlichste Zeichen zu nehmen, erlaubt Sampling auch kreative und abwechslungsreiche Ausgaben.\n",
    "\n",
    "\n",
    "## 2. Textgenerierungsfunktion `generate_text(...)`\n",
    "\n",
    "Diese Funktion erzeugt fortlaufend Zeichen, basierend auf einem Start-Seed.\n",
    "\n",
    "### Parameter:\n",
    "\n",
    "- `model`: Das trainierte LSTM-Modell.\n",
    "- `seed`: Startsequenz (Initialtext).\n",
    "- `char_indices`: Mapping Zeichen → Index.\n",
    "- `indices_char`: Mapping Index → Zeichen.\n",
    "- `chars`: Alle Zeichen im Vokabular.\n",
    "- `maxlen`: Länge der Eingabesequenzen.\n",
    "- `length`: Wie viele Zeichen sollen generiert werden?\n",
    "- `temperature`: Sampling-Temperatur (siehe oben).\n",
    "\n",
    "### Ablauf:\n",
    "\n",
    "1. **Initialisierung**:\n",
    "   - Modell wird in Evaluationsmodus (`model.eval()`) versetzt.\n",
    "   - Der `generated`-Text beginnt mit dem Seed.\n",
    "   - Die aktuelle Position des Modells wird auf `device` gesetzt (CPU oder GPU).\n",
    "\n",
    "2. **Iterative Zeichen-Vorhersage**:\n",
    "   Für jedes neue Zeichen (insgesamt `length` Wiederholungen):\n",
    "   - Ein Tensor `x_pred` der Größe `[1, maxlen, vocab_size]` wird erstellt.\n",
    "   - Die letzten `maxlen` Zeichen aus dem bisherigen Text werden **One-Hot-codiert** in `x_pred` geschrieben.\n",
    "   - Das Modell gibt eine Vorhersage für das nächste Zeichen aus.\n",
    "   - Diese Vorhersage wird via Softmax in Wahrscheinlichkeiten umgewandelt.\n",
    "   - Die Funktion `sample(...)` wählt das nächste Zeichen.\n",
    "   - Dieses Zeichen wird zur laufenden Zeichenkette `generated` hinzugefügt und ausgegeben.\n",
    "\n",
    "### Ausgabe:\n",
    "\n",
    "- Der neu erzeugte Text wird zeichenweise direkt im Terminal ausgegeben.\n",
    "- Jede Generierung beginnt mit dem Seed und wächst durch neue Vorhersagen des Modells.\n",
    "\n",
    "\n",
    "## Fazit\n",
    "\n",
    "Das Textsampling ermöglicht es, einen beliebig langen, neuen Text im Stil der Trainingsdaten zu erzeugen:\n",
    "\n",
    "- Mithilfe des **Temperaturwerts** kann gesteuert werden, ob der Text eher **konservativ** oder **kreativ** sein soll.\n",
    "- Die Sampling-Strategie vermeidet starres deterministisches Verhalten und erlaubt **vielfältigere Ausdrucksweise**.\n",
    "- Durch sequentielle Einspeisung der Vorhersagen entsteht ein fortlaufender Text.\n",
    "\n",
    "Diese Methode ist zentral für alle Anwendungen der **kreativen Textgenerierung mit neuronalen Netzen**, z. B. für Lyrik, Prosa, Chatbots oder Musiknotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 3. TEXTSAMPLING -----\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return np.random.choice(len(preds), p=preds)\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    model,\n",
    "    seed,\n",
    "    char_indices,\n",
    "    indices_char,\n",
    "    chars,\n",
    "    maxlen=60,\n",
    "    length=400,\n",
    "    temperature=1.0,\n",
    "):\n",
    "    model.eval()\n",
    "    generated = seed\n",
    "    print(f\"\\n------ temperature: {temperature}\")\n",
    "    print(f'Seed: \"{seed}\"')\n",
    "    print(seed, end=\"\")\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    for k in range(length):\n",
    "        x_pred = torch.zeros(1, maxlen, len(chars)).to(device)\n",
    "        for t, char in enumerate(generated[-maxlen:]):\n",
    "            if char in char_indices:\n",
    "                x_pred[0, t, char_indices[char]] = 1.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = F.softmax(model(x_pred), dim=-1).cpu().numpy()[0]\n",
    "\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        print(next_char, end=\"\", flush=True)\n",
    "        if k % 100 == 0:\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeichenweise Textgenerierung\n",
    "\n",
    "Dieses Hauptskript führt den gesamten Trainings- und Textgenerierungsprozess durch.\n",
    "\n",
    "\n",
    "## Ablaufübersicht:\n",
    "\n",
    "1. **Gerätewahl**\n",
    "   - Ermittelt, ob CUDA (GPU) verfügbar ist, und gibt das verwendete Gerät aus.\n",
    "\n",
    "2. **Datendownload und -vorbereitung**\n",
    "   - Lädt den Nietzsche-Text herunter.\n",
    "   - Schneidet den Text in Sequenzen (`maxlen = 40`, `step = 5`).\n",
    "   - Erstellt ein `CharDataset` und einen `DataLoader` für das Training.\n",
    "\n",
    "3. **Modellerstellung**\n",
    "   - Initialisiert das `CharLSTM`-Modell mit der passenden Vokabulargröße und Sequenzlänge.\n",
    "\n",
    "4. **Trainer-Konfiguration**\n",
    "   - Verwendet PyTorch Lightning's `Trainer` mit folgenden Einstellungen:\n",
    "     - `max_epochs=10`: Anzahl der Trainingsdurchläufe.\n",
    "     - `log_every_n_steps=10`: Logging-Intervall.\n",
    "     - `gradient_clip_val=1.0`: Verhindert explodierende Gradienten.\n",
    "     - `accelerator=\"auto\"`: Automatische Gerätewahl (CPU/GPU).\n",
    "     - `devices=1`: Trainingslauf auf einem Gerät.\n",
    "\n",
    "5. **Training**\n",
    "   - Das Modell wird mit dem vorbereiteten `DataLoader` trainiert.\n",
    "\n",
    "6. **Textgenerierung**\n",
    "   - Eine zufällige Startsequenz (Seed) wird aus dem Originaltext gewählt.\n",
    "   - Für vier verschiedene Temperaturwerte (`0.2`, `0.5`, `1.0`, `1.2`) wird Text generiert:\n",
    "     - Niedrige Temperatur → konservative Vorhersagen.\n",
    "     - Hohe Temperatur → kreative, unvorhersehbare Vorhersagen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "path = download_nietzsche_text()\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "text, sentences, next_chars, chars, char_indices, indices_char = load_text(\n",
    "    path, maxlen, step\n",
    ")\n",
    "dataset = CharDataset(sentences, next_chars, char_indices, maxlen)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "model = CharLSTM(vocab_size=len(chars), maxlen=maxlen)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    log_every_n_steps=10,\n",
    "    gradient_clip_val=1.0,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "trainer.fit(model, dataloader)\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "seed = text[start_index : start_index + maxlen]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temperature in [0.01, 0.2, 0.5, 1.0]:\n",
    "    generate_text(\n",
    "        model,\n",
    "        seed,\n",
    "        char_indices,\n",
    "        indices_char,\n",
    "        chars,\n",
    "        maxlen,\n",
    "        length=600,\n",
    "        temperature=temperature,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
