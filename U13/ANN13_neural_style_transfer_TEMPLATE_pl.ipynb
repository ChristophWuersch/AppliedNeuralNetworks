{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U13/ANN13_neural_style_transfer_TEMPLATE_pl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "**Neural Style Transfer (NST)** ist ein Verfahren im Bereich des Deep Learning, das es ermöglicht, den **Inhalt eines Bildes** mit dem **künstlerischen Stil eines anderen Bildes** zu kombinieren. Ziel ist es, ein neues Bild zu erzeugen, das die semantische Struktur des einen Bildes (z. B. eine Landschaft) mit der künstlerischen Ästhetik eines anderen Bildes (z. B. ein Van-Gogh-Gemälde) vereint.\n",
    "## Ziel\n",
    "\n",
    "Ein neues Bild generieren, das:\n",
    "\n",
    "- **Inhaltlich dem Content-Bild** entspricht  \n",
    "- **Stilistisch dem Style-Bild** ähnelt\n",
    "## Funktionsweise\n",
    "\n",
    "NST verwendet ein **vortrainiertes Convolutional Neural Network (CNN)** – typischerweise **VGG19** – um Bilder in sogenannte **Feature-Repräsentationen** zu überführen. Dabei wird unterschieden zwischen:\n",
    "\n",
    "- **Content-Features**: erfassen die räumliche Struktur und Anordnung von Objekten  \n",
    "- **Style-Features**: erfassen die Textur, Farbverteilung und Muster – über sogenannte **Gram-Matrizen**\n",
    "\n",
    "Das **generierte Bild** wird schrittweise angepasst, sodass es ähnliche Feature-Repräsentationen wie die Eingabebilder aufweist.\n",
    "\n",
    "\n",
    "## Komponenten im Detail\n",
    "\n",
    "| Komponente       | Beschreibung                                                                 |\n",
    "|------------------|------------------------------------------------------------------------------|\n",
    "| **Content Loss** | Misst die Abweichung der Content-Features zwischen generiertem und Content-Bild |\n",
    "| **Style Loss**   | Misst die Abweichung der Gram-Matrizen zwischen generiertem und Style-Bild     |\n",
    "| **TV Loss**      | (optional) Glättet das Bild, um visuelle Artefakte zu reduzieren               |\n",
    "| **Optimierung**  | Das Bild selbst wird trainiert – nicht das Netzwerk!                          |\n",
    "\n",
    "\n",
    "## Technischer Ablauf\n",
    "\n",
    "1. **Initialisierung**: Starte mit einem Rauschbild oder einer Kopie des Content-Bildes  \n",
    "2. **Feature Extraktion**: VGG-Netz extrahiert Features aus Content-, Style- und generiertem Bild  \n",
    "3. **Loss-Berechnung**: Vergleiche die Features und berechne Content- und Style-Loss  \n",
    "4. **Backpropagation**: Optimiere das Bild so, dass der kombinierte Loss minimiert wird  \n",
    "5. **Visualisierung**: Das Bild entwickelt sich mit jeder Iteration in Richtung „stilisiertes Ergebnis“\n",
    "\n",
    "\n",
    "## Anwendung in dieser Übung\n",
    "\n",
    "In dieser Übung wirst du:\n",
    "\n",
    "- Ein VGG19-Modell als Feature-Extractor nutzen  \n",
    "- Content- und Style-Features aus Beispielbildern extrahieren  \n",
    "- Ein neues Bild durch Optimierung erzeugen  \n",
    "- Verlaufsplots zur Analyse der Verluste über die Iterationen erstellen\n",
    "\n",
    "## Typische Beispiele\n",
    "\n",
    "| Content-Bild                | Style-Bild                   | Ergebnis                              |\n",
    "|-----------------------------|------------------------------|----------------------------------------|\n",
    "| Landschaftsfoto             | Monet-Gemälde                | Landschaft im impressionistischen Stil |\n",
    "| Porträtfoto                 | Picasso-Zeichnung            | Abstraktes Porträt                     |\n",
    "| Stadtbild                   | Van Gogh (Sternennacht)      | Leuchtende, wirbelnde Stadtansicht     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Setup der Umgebung\n",
    "\n",
    "Die Funktion `setup_environment()` prüft, ob alle benötigten Dateien für den Neural Style Transfer vorhanden sind – insbesondere das vortrainierte VGG19-Modell und zwei Beispielbilder (Content & Style). Falls die Dateien nicht vorhanden sind, werden sie automatisch aus dem Internet heruntergeladen und an die richtigen Stellen gespeichert.\n",
    "\n",
    "### Was genau passiert:\n",
    "\n",
    "1. **VGG19-Modell laden:**\n",
    "   - Prüft, ob die Datei `models/vgg19-d01eb7cb.pth` vorhanden ist\n",
    "   - Wenn nicht, wird das vortrainierte VGG19-Modell von der Universität Michigan heruntergeladen und in den Ordner `models/` gespeichert\n",
    "\n",
    "2. **Beispielbilder laden:**\n",
    "   - Prüft, ob `images/1-content.png` und `images/1-style.jpg` existieren\n",
    "   - Falls nicht:\n",
    "     - Wird ein ZIP-Archiv von GitHub heruntergeladen (`master.zip`)\n",
    "     - Es wird entpackt\n",
    "     - Die beiden Beispielbilder werden aus dem entpackten Ordner in den Ordner `images/` verschoben\n",
    "\n",
    "3. Am Ende wird bestätigt, dass das Setup erfolgreich abgeschlossen wurde\n",
    "\n",
    "> Diese Funktion erleichtert den Einstieg und stellt sicher, dass das Notebook direkt lauffähig ist – ohne manuelles Herunterladen von Dateien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "\n",
    "def setup_environment():\n",
    "    # Basisverzeichnis\n",
    "    base_dir = \"data\"\n",
    "    model_dir = os.path.join(base_dir, \"models\")\n",
    "    image_dir = os.path.join(base_dir, \"images\")\n",
    "\n",
    "    # Check if VGG-Modell existiert\n",
    "    vgg_path = os.path.join(model_dir, \"vgg19-d01eb7cb.pth\")\n",
    "    if not os.path.exists(vgg_path):\n",
    "        print(\"Lade VGG19-Gewichte herunter ...\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        url = \"https://web.eecs.umich.edu/~justincj/models/vgg19-d01eb7cb.pth\"\n",
    "        urllib.request.urlretrieve(url, vgg_path)\n",
    "\n",
    "    # Check ob Bilder existieren\n",
    "    content_img = os.path.join(image_dir, \"1-content.png\")\n",
    "    style_img = os.path.join(image_dir, \"1-style.jpg\")\n",
    "    if not os.path.exists(content_img) or not os.path.exists(style_img):\n",
    "        print(\"Lade Beispielbilder herunter ...\")\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        zip_path = os.path.join(base_dir, \"master.zip\")\n",
    "        if not os.path.exists(zip_path):\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://github.com/iamRusty/neural-style-pytorch/archive/master.zip\",\n",
    "                zip_path,\n",
    "            )\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(base_dir)\n",
    "\n",
    "        extracted_path = os.path.join(base_dir, \"neural-style-pytorch-master\", \"images\")\n",
    "        os.rename(os.path.join(extracted_path, \"1-content.png\"), content_img)\n",
    "        os.rename(os.path.join(extracted_path, \"1-style.jpg\"), style_img)\n",
    "\n",
    "    print(\"✅ Setup abgeschlossen. Dateien sind im 'data/' Ordner gespeichert.\")\n",
    "\n",
    "\n",
    "# Setup die Umgebung\n",
    "setup_environment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noch mehr Bilder um selber umzuschalten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zielordner\n",
    "target_dir = \"Bilder/StyleTransfer\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Bildliste aus Screenshot (alle .jpg + .png außer readme.md)\n",
    "filenames = [\n",
    "    \"Klimt2.jpg\",\n",
    "    \"Kopenhagen.jpg\",\n",
    "    \"Mosaic2.jpg\",\n",
    "    \"Sonnenblumen.jpg\",\n",
    "    \"Van_Gogh.jpg\",\n",
    "    \"Vassily_Kandinsky7.jpg\",\n",
    "    \"candy.jpg\",\n",
    "    \"composition-with-figures_popova.jpg\",\n",
    "    \"composition.jpg\",\n",
    "    \"graffiti.jpg\",\n",
    "    \"popova.png\",\n",
    "    \"portrait.jpg\",\n",
    "    \"portrait_mann.jpg\",\n",
    "    \"portrait_women.jpg\",\n",
    "    \"simpsons.jpg\",\n",
    "    \"udnie.jpg\",\n",
    "]\n",
    "\n",
    "# Basis-URL\n",
    "base_url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/data/StyleTransfer/\"\n",
    "\n",
    "# Download-Schleife\n",
    "for name in filenames:\n",
    "    save_path = os.path.join(target_dir, name)\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"✅ Bereits vorhanden: {name}\")\n",
    "        continue\n",
    "    url = base_url + name\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, save_path)\n",
    "        print(f\"✅ Heruntergeladen: {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fehler bei {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt werden alle benötigten Bibliotheken für den Neural Style Transfer geladen. Sie ermöglichen Bildverarbeitung, Modellhandling, Visualisierung und numerische Optimierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Neural Style Transfer in PyTorch\n",
    "# Ziel: Inhalt eines Bildes mit dem Stil eines anderen kombinieren\n",
    "# -----------------------------------------------\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import plotly.subplots as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from torchvision import models, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Hyperparameter und Einstellungen\n",
    "\n",
    "In diesem Abschnitt werden alle zentralen Konfigurationsparameter für den Style Transfer definiert. Sie bestimmen, **wie stark Inhalt und Stil gewichtet werden**, wie das Bild initialisiert wird und welche Optimierungsmethode verwendet wird.\n",
    "\n",
    "### Modell- und Optimierungsparameter\n",
    "\n",
    "| Parameter         | Bedeutung                                                                 |\n",
    "|-------------------|---------------------------------------------------------------------------|\n",
    "| `MAX_IMAGE_SIZE`  | Maximale Kantenlänge der Eingabebilder (Skalierung)                       |\n",
    "| `OPTIMIZER`       | Optimierer: `adam` (schnell, stabil) oder `lbfgs` (klassisch, präzise)    |\n",
    "| `ADAM_LR`         | Lernrate für den Adam-Optimierer                                          |\n",
    "| `CONTENT_WEIGHT`  | Gewichtung des Inhaltsverlustes (Content Loss)                            |\n",
    "| `STYLE_WEIGHT`    | Gewichtung des Stilverlustes (Style Loss)                                 |\n",
    "| `TV_WEIGHT`       | Gewichtung des Glättungsfaktors (Total Variation Loss)                    |\n",
    "| `NUM_ITER`        | Anzahl der Optimierungsschritte                                           |\n",
    "| `SHOW_ITER`       | Nach wie vielen Iterationen ein Zwischenbild angezeigt wird               |\n",
    "| `INIT_IMAGE`      | Startbild: `\"random\"` oder `\"content\"`                                    |\n",
    "| `PRESERVE_COLOR`  | Soll die Farbgebung des Content-Bildes erhalten bleiben? (`True/False`)   |\n",
    "| `PIXEL_CLIP`      | Soll das Bild nach jeder Iteration auf gültige Pixelwerte begrenzt werden?|\n",
    "\n",
    "### Pfade zu Ressourcen\n",
    "\n",
    "| Variable       | Beschreibung                                 |\n",
    "|----------------|----------------------------------------------|\n",
    "| `CONTENT_PATH` | Pfad zum Content-Bild                        |\n",
    "| `STYLE_PATH`   | Pfad zum Style-Bild                          |\n",
    "| `VGG19_PATH`   | Pfad zu den vortrainierten VGG19-Gewichten   |\n",
    "| `POOL`         | Art des Poolings im Netzwerk (`max` oder `avg`) |\n",
    "\n",
    "### Gerät wählen\n",
    "\n",
    "```python\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "```\n",
    "\n",
    "> Diese Zeile prüft automatisch, ob eine GPU (CUDA) verfügbar ist, und nutzt sie – andernfalls wird die CPU verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Hyperparameter & Einstellungen\n",
    "# -----------------------------------------------\n",
    "MAX_IMAGE_SIZE = 512\n",
    "OPTIMIZER = \"adam\"  # 'adam' oder 'lbfgs'\n",
    "ADAM_LR = 10  # Lernrate\n",
    "CONTENT_WEIGHT = 5e0  # Gewichtung des Inhaltsverlustes\n",
    "STYLE_WEIGHT = 1e2  # Gewichtung des Stilverlustes\n",
    "TV_WEIGHT = 1e-3  # Gewichtung des Total Variation Loss\n",
    "NUM_ITER = 500  # Anzahl der Optimierungsiterationen\n",
    "SHOW_ITER = 100  # Anzeigeintervall\n",
    "INIT_IMAGE = \"random\"  # 'random' oder 'content'\n",
    "PRESERVE_COLOR = \"True\"  # Soll die Farbe des Content-Bildes beibehalten werden?\n",
    "PIXEL_CLIP = \"True\"  # Pixel nach jeder Iteration clippen\n",
    "\n",
    "# Pfade zu Bildern\n",
    "# =========== CONTENT PATHS =========== Bitte nur jeweils einen Pfad aktivieren\n",
    "CONTENT_PATH = \"data/images/1-content.png\"\n",
    "# CONTENT_PATH = \"Bilder/StyleTransfer/Kopenhagen.jpg\"\n",
    "# CONTENT_PATH = \"Bilder/StyleTransfer/portrait_mann.jpg\"\n",
    "# CONTENT_PATH = \"Bilder/StyleTransfer/portrait_women.jpg\"\n",
    "# CONTENT_PATH = \"Bilder/StyleTransfer/portrait.jpg\"\n",
    "# CONTENT_PATH = \"Bilder/StyleTransfer/simpsons.jpg\"\n",
    "# =========== STYLE PATHS =========== Bitte nur jeweils einen Pfad aktivieren\n",
    "STYLE_PATH = \"data/images/1-style.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/candy.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/Van_Gogh.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/udnie.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/Vassily_Kandinsky7.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/composition.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/composition-with-figures_popova.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/graffiti.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/Mosaic2.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/Sonnenblumen.jpg\"\n",
    "# =========== VGG PATH ===========\n",
    "VGG19_PATH = \"data/models/vgg19-d01eb7cb.pth\"\n",
    "POOL = \"max\"\n",
    "\n",
    "# Gerät festlegen\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Verwendetes Gerät:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Hilfsfunktionen für die Bildverarbeitung\n",
    "\n",
    "Diese Funktionen unterstützen das Laden, Anzeigen, Speichern und die Farbübertragung von Bildern, was zentral für die Visualisierung und Verarbeitung im Neural Style Transfer ist.\n",
    "\n",
    "### Funktionsübersicht: Bildverarbeitung\n",
    "\n",
    "| Funktion           | Zweck                                                  | Besonderheiten                                       |\n",
    "|--------------------|---------------------------------------------------------|------------------------------------------------------|\n",
    "| `load_image(path)` | Bild aus Datei laden (BGR-Format, OpenCV)              | Nutzt `cv2.imread()`                                 |\n",
    "| `show(img)`        | Bild anzeigen (matplotlib, RGB-Konvertierung)          | Skaliert Pixel auf [0, 1], wandelt BGR → RGB         |\n",
    "| `saveimg(img, iters)` | Stilisiertes Bild speichern                           | Erstellt Ordner `style_transfer_pictures/`, wandelt zu `uint8` |\n",
    "| `transfer_color(src, dest)` | Farbübertragung vom Content-Bild auf das Stilbild | Arbeitet mit YCrCb-Farbraum, ersetzt Helligkeit      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Hilfsfunktionen für Bildverarbeitung\n",
    "# -----------------------------------------------\n",
    "def load_image(path):\n",
    "    return cv2.imread(path)  # BGR Format\n",
    "\n",
    "\n",
    "def show(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.array(img / 255).clip(0, 1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def saveimg(img, iters):\n",
    "    # Erstelle den Ordner 'style_transfer_pictures', falls er nicht existiert\n",
    "    output_dir = \"data/style_transfer_pictures\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    if PIXEL_CLIP == \"True\":\n",
    "        img = img.clip(0, 255)\n",
    "    img_uint8 = img.astype(np.uint8)\n",
    "    # Speichere das Bild im Ordner 'style_transfer_pictures'\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"out{iters}.png\"), img_uint8)\n",
    "\n",
    "\n",
    "def transfer_color(src, dest):\n",
    "    if PIXEL_CLIP == \"True\":\n",
    "        src, dest = src.clip(0, 255), dest.clip(0, 255)\n",
    "    H, W, _ = src.shape\n",
    "    dest = cv2.resize(dest, dsize=(W, H), interpolation=cv2.INTER_CUBIC)\n",
    "    dest_gray = cv2.cvtColor(dest, cv2.COLOR_BGR2GRAY)\n",
    "    src_yiq = cv2.cvtColor(src, cv2.COLOR_BGR2YCrCb)\n",
    "    src_yiq[..., 0] = dest_gray\n",
    "    return cv2.cvtColor(src_yiq, cv2.COLOR_YCrCb2BGR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Tensor-Bild Konvertierung\n",
    "\n",
    "Für das Training und die Optimierung im Neural Style Transfer müssen Bilder als **Tensors** verarbeitet werden. Diese beiden Funktionen übernehmen die Konvertierung zwischen **NumPy-Bilddaten (OpenCV)** und **PyTorch-Tensoren**, wobei sie auch das **Preprocessing bzw. Denormalisieren** mit den VGG19-spezifischen Mittelwerten übernehmen.\n",
    "\n",
    "\n",
    "### Funktionen im Detail\n",
    "\n",
    "#### `itot(img)`\n",
    "**Image → Tensor**\n",
    "\n",
    "- Wandelt ein Bild (`np.ndarray`, BGR) in einen PyTorch-Tensor um\n",
    "- Skaliert das Bild auf die maximale Größe `MAX_IMAGE_SIZE`\n",
    "- Normalisiert die Farbkanäle mit den VGG19-Trainingsmittelwerten:\n",
    "  `[103.939, 116.779, 123.68]` (BGR-Reihenfolge)\n",
    "- Multipliziert mit 255, da `ToTensor()` standardmäßig Werte in `[0, 1]` gibt\n",
    "- Fügt Batch-Dimension `[1, C, H, W]` hinzu\n",
    "\n",
    "#### `ttoi(tensor)`\n",
    "**Tensor → Image**\n",
    "\n",
    "- Entfernt die Batch-Dimension\n",
    "- Revertiert die Normalisierung mit den negativen VGG-Mittelwerten\n",
    "- Wandelt den Tensor wieder in ein NumPy-Bild `[H, W, C]` um (RGB)\n",
    "- Gibt ein Bild-Array mit Werten im Bereich `[0, 255]` (float) zurück\n",
    "\n",
    "\n",
    "### Funktionsübersicht\n",
    "\n",
    "| Funktion     | Richtung         | Zweck                                         | Besonderheiten                               |\n",
    "|--------------|------------------|-----------------------------------------------|----------------------------------------------|\n",
    "| `itot(img)`  | Bild → Tensor     | Vorverarbeitung & Normalisierung für VGG19    | Skaliert Bild, wendet VGG-Mean-Norm an       |\n",
    "| `ttoi(tensor)` | Tensor → Bild   | Rücktransformation zum Bildformat             | Macht Bild darstellbar, entfernt VGG-Norm    |\n",
    "\n",
    "\n",
    "> Diese Funktionen sorgen dafür, dass deine Bilder korrekt als Netzwerk-Input verarbeitet werden können – und später auch wieder als visuell interpretierbare Ausgaben vorliegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Tensor-Image Konvertierung\n",
    "# -----------------------------------------------\n",
    "def itot(img):\n",
    "    H, W, _ = img.shape\n",
    "    image_size = tuple([int((MAX_IMAGE_SIZE / max(H, W)) * x) for x in [H, W]])\n",
    "    itot_t = transforms.Compose(\n",
    "        [transforms.ToPILImage(), transforms.Resize(image_size), transforms.ToTensor()]\n",
    "    )\n",
    "    normalize_t = transforms.Normalize([103.939, 116.779, 123.68], [1, 1, 1])\n",
    "    tensor = normalize_t(itot_t(img) * 255).unsqueeze(0)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def ttoi(tensor):\n",
    "    ttoi_t = transforms.Compose(\n",
    "        [transforms.Normalize([-103.939, -116.779, -123.68], [1, 1, 1])]\n",
    "    )\n",
    "    tensor = tensor.squeeze()\n",
    "    img = ttoi_t(tensor).cpu().numpy().transpose(1, 2, 0)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Bilder vorbereiten\n",
    "\n",
    "In diesem Schritt werden das **Content-Bild** und das **Style-Bild** geladen und visualisiert. Diese beiden Bilder sind die Grundlage für den späteren Stiltransfer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Bilder vorbereiten\n",
    "# -----------------------------------------------\n",
    "content_img = load_image(CONTENT_PATH)\n",
    "style_img = load_image(STYLE_PATH)\n",
    "print(\"Content Image Shape:\", content_img.shape, \"-------------\")\n",
    "show(content_img)\n",
    "print(\"Style Image Shape:\", style_img.shape, \"-------------\")\n",
    "show(style_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) VGG19 Feature-Extractor vorbereiten\n",
    "\n",
    "In diesem Abschnitt wird ein **vortrainiertes VGG19-Netzwerk** als **Feature-Extractor** geladen und vorbereitet. Dieses CNN wird **nicht trainiert**, sondern lediglich verwendet, um **semantische Inhalte und stilistische Merkmale** aus den Bildern zu extrahieren.\n",
    "\n",
    "\n",
    "\n",
    "### Was passiert hier?\n",
    "\n",
    "1. **Laden des VGG19-Modells**\n",
    "   ```python\n",
    "   vgg = models.vgg19(pretrained=False)\n",
    "   vgg.load_state_dict(torch.load(VGG19_PATH), strict=False)\n",
    "   ```\n",
    "   - Das Modell wird **nicht automatisch** von `torchvision` heruntergeladen (`pretrained=False`)\n",
    "   - Stattdessen werden manuell geladene Gewichte verwendet (`VGG19_PATH`)\n",
    "   - Es handelt sich um ein **auf ImageNet vortrainiertes Modell**\n",
    "\n",
    "2. **Extraktion des Feature-Teils**\n",
    "   ```python\n",
    "   model = copy.deepcopy(vgg.features).to(device)\n",
    "   ```\n",
    "   - Nur der **Feature-Teil des VGG19-Netzes** wird verwendet\n",
    "   - Der Klassifikationskopf (`classifier`) wird entfernt\n",
    "\n",
    "3. **Deaktivieren der Gradientenberechnung**\n",
    "   ```python\n",
    "   for param in model.parameters():\n",
    "       param.requires_grad = False\n",
    "   ```\n",
    "   - Das Modell bleibt **eingefroren** während des Style Transfers\n",
    "   - Es dient nur zur **Berechnung der Verluste**, nicht zur Optimierung\n",
    "\n",
    "\n",
    "### Warum VGG19?\n",
    "\n",
    "- VGG19 ist ein tiefes CNN mit vielen kleinen Faltungen\n",
    "- Seine Zwischenlayer repräsentieren sehr gut:\n",
    "  - **Inhalt** (mittlere Layer)\n",
    "  - **Stil** (frühe + tiefe Layer, über Gram-Matrizen)\n",
    "- Durch das Einfrieren bleiben diese Repräsentationen stabil und zuverlässig\n",
    "\n",
    "\n",
    "> Das Modell extrahiert wichtige Merkmale aus Content- und Style-Bild und ermöglicht die **Berechnung der entsprechenden Verluste**, die das generierte Bild formen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# VGG19 Feature-Extractor vorbereiten\n",
    "# -----------------------------------------------\n",
    "vgg = models.vgg19(weights=None)\n",
    "vgg.load_state_dict(torch.load(VGG19_PATH, weights_only=False), strict=False)\n",
    "model = copy.deepcopy(vgg.features).to(device)\n",
    "\n",
    "# Gradienten ausschalten\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Parameter nicht trainierbar machen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g) Loss-Funktionen\n",
    "\n",
    "Beim Neural Style Transfer basiert die Optimierung auf verschiedenen Verlustfunktionen, die sicherstellen, dass das generierte Bild sowohl inhaltlich dem Content-Bild als auch stilistisch dem Style-Bild ähnelt. Zusätzlich wird ein Glättungsterm verwendet, um visuelle Artefakte zu vermeiden.\n",
    "\n",
    "\n",
    "\n",
    "### Übersicht der verwendeten Loss-Funktionen\n",
    "\n",
    "| Funktion        | Zweck                                                         |\n",
    "|-----------------|---------------------------------------------------------------|\n",
    "| **Content Loss** | Misst Ähnlichkeit zum Content-Bild anhand von VGG-Features     |\n",
    "| **Style Loss**   | Misst Ähnlichkeit zum Style-Bild über Gram-Matrizen           |\n",
    "| **TV Loss**      | Total Variation Loss – sorgt für glatte Übergänge im Bild      |\n",
    "\n",
    "\n",
    "\n",
    "### Definitionen im Code\n",
    "\n",
    "#### Mean Squared Error Loss (Grundlage)\n",
    "```python\n",
    "mse_loss = nn.MSELoss()\n",
    "```\n",
    "Der mittlere quadratische Fehler bildet die Basis aller Verluste.\n",
    "\n",
    "\n",
    "\n",
    "#### Gram-Matrix: Stil-Repräsentation\n",
    "```python\n",
    "def gram(tensor):\n",
    "    B, C, H, W = tensor.shape\n",
    "    x = tensor.view(C, H * W)\n",
    "    return torch.mm(x, x.t())\n",
    "```\n",
    "- Die Gram-Matrix misst die **Korrelation zwischen Feature-Kanälen**\n",
    "- Je ähnlicher zwei Kanäle feuern, desto höher ihr Wert\n",
    "- Sie beschreibt **Textur und Stil**, unabhängig von der Position im Bild\n",
    "\n",
    "\n",
    "\n",
    "#### Content Loss\n",
    "```python\n",
    "def content_loss(g, c):\n",
    "    return mse_loss(g, c)\n",
    "```\n",
    "- Misst die Differenz der Features zwischen dem generierten Bild `g` und dem Content-Bild `c`\n",
    "- Nutzt eine mittlere Schicht (z. B. `relu4_2`) aus dem VGG-Netz\n",
    "\n",
    "\n",
    "\n",
    "#### Style Loss\n",
    "```python\n",
    "def style_loss(g, s):\n",
    "    c1, _ = g.shape\n",
    "    return mse_loss(g, s) / (c1**2)\n",
    "```\n",
    "- Nutzt Gram-Matrizen aus mehreren VGG-Schichten (z. B. `relu1_2`, `relu3_3`, ...)\n",
    "- Die Division durch `c1²` (Kanalanzahl²) normalisiert die Verlustgröße\n",
    "\n",
    "\n",
    "\n",
    "#### Total Variation Loss\n",
    "```python\n",
    "def tv_loss(c):\n",
    "    x = c[:, :, 1:, :] - c[:, :, :-1, :]\n",
    "    y = c[:, :, :, 1:] - c[:, :, :, :-1]\n",
    "    return torch.sum(torch.abs(x)) + torch.sum(torch.abs(y))\n",
    "```\n",
    "- Belohnt **räumlich glatte Strukturen**\n",
    "- Hilft, **visuelles Rauschen** im Bild zu vermeiden\n",
    "- Optional, aber häufig mit großem Einfluss auf das Ergebnis\n",
    "\n",
    "\n",
    "\n",
    "> Diese Verluste werden gewichtet kombiniert und bilden die Zielfunktion, nach der das generierte Bild optimiert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Loss-Funktionen\n",
    "# -----------------------------------------------\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "def gram(tensor):\n",
    "    B, C, H, W = tensor.shape\n",
    "    x = tensor.view(C, H * W)\n",
    "    return torch.mm(x, x.t())\n",
    "\n",
    "\n",
    "def content_loss(g, c):\n",
    "    return mse_loss(g, c)\n",
    "\n",
    "\n",
    "def style_loss(g, s):\n",
    "    c1, _ = g.shape\n",
    "    return mse_loss(g, s) / (c1**2)\n",
    "\n",
    "\n",
    "def tv_loss(c):\n",
    "    x = c[:, :, 1:, :] - c[:, :, :-1, :]\n",
    "    y = c[:, :, :, 1:] - c[:, :, :, :-1]\n",
    "    return torch.sum(torch.abs(x)) + torch.sum(torch.abs(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (h) Feature Extraction aus VGG-Schichten\n",
    "\n",
    "Diese Funktion extrahiert **Content- und Style-Features** aus einem Eingabetensor mit Hilfe des VGG19-Modells. Dabei werden bestimmte Layer gezielt abgefragt, um die Repräsentationen für Inhalt und Stil zu erhalten.\n",
    "\n",
    "\n",
    "### Was macht `get_features(model, tensor)`?\n",
    "\n",
    "- Führt eine **Forward-Pass** durch das VGG-Modell aus\n",
    "- Speichert die **Aktivierungen bestimmter Schichten** (jeweils als Feature-Maps)\n",
    "- Verwendet:\n",
    "  - **`relu4_2`** für den **Inhalt**\n",
    "  - **Mehrere Schichten** (`relu1_2`, `relu2_2`, etc.) für den **Stil** (Gram-Matrizen)\n",
    "- Berechnet Gram-Matrizen für Style-Features direkt im Funktionsverlauf\n",
    "\n",
    "\n",
    "### Verwendete Layer im VGG19\n",
    "\n",
    "| Layer-ID | Name im VGG      | Verwendung         |\n",
    "|----------|------------------|--------------------|\n",
    "| `\"3\"`    | `relu1_2`        | Stil-Feature       |\n",
    "| `\"8\"`    | `relu2_2`        | Stil-Feature       |\n",
    "| `\"17\"`   | `relu3_3`        | Stil-Feature       |\n",
    "| `\"26\"`   | `relu4_3`        | Stil-Feature       |\n",
    "| `\"35\"`   | `relu5_3`        | Stil-Feature       |\n",
    "| `\"22\"`   | `relu4_2`        | Content-Feature    |\n",
    "\n",
    "> Diese Layer wurden so gewählt, da sie sich im Originalpaper von Gatys et al. als besonders geeignet für die Darstellung von Stil- und Inhaltsinformationen erwiesen haben.\n",
    "\n",
    "\n",
    "### Besonderheiten im Code\n",
    "\n",
    "```python\n",
    "if name == \"22\":\n",
    "    features[layers[name]] = x\n",
    "```\n",
    "- Der Inhalt wird **direkt als Feature-Map gespeichert**\n",
    "\n",
    "```python\n",
    "features[layers[name]] = gram(x) / (H * W)\n",
    "```\n",
    "- Style-Features werden als **normierte Gram-Matrizen** gespeichert (nach Fläche)\n",
    "\n",
    "```python\n",
    "if name == \"35\":\n",
    "    break\n",
    "```\n",
    "- Stoppt die Verarbeitung nach der letzten benötigten Schicht → spart Rechenzeit\n",
    "\n",
    "\n",
    "> Diese extrahierten Features werden später für die Berechnung des Content- und Style-Loss verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Feature Extraction aus VGG Layers\n",
    "# -----------------------------------------------\n",
    "def get_features(model, tensor):\n",
    "    layers = {\n",
    "        \"3\": \"relu1_2\",\n",
    "        \"8\": \"relu2_2\",\n",
    "        \"17\": \"relu3_3\",\n",
    "        \"26\": \"relu4_3\",\n",
    "        \"35\": \"relu5_3\",\n",
    "        \"22\": \"relu4_2\",  # Content Layer\n",
    "    }\n",
    "    features = {}\n",
    "    x = tensor\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            if name == \"22\":\n",
    "                features[layers[name]] = x\n",
    "            else:\n",
    "                B, C, H, W = x.shape\n",
    "                features[layers[name]] = gram(x) / (H * W)\n",
    "        if name == \"35\":\n",
    "            break\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Startbild erzeugen\n",
    "\n",
    "Bevor das stilisierte Bild optimiert werden kann, muss ein **Startbild** definiert werden. Dieses Bild ist der Ausgangspunkt für die iterative Optimierung, bei der sich das Bild schrittweise an Inhalt und Stil annähert.\n",
    "\n",
    "\n",
    "### Funktion: `initial(content_tensor, init_image=\"random\")`\n",
    "\n",
    "Diese Funktion erzeugt das Startbild auf Basis des gewählten Initialisierungstyps.\n",
    "\n",
    "#### Zwei Möglichkeiten der Initialisierung:\n",
    "\n",
    "| Modus         | Beschreibung                                                                 |\n",
    "|---------------|-------------------------------------------------------------------------------|\n",
    "| `\"random\"`    | Erzeugt ein zufälliges Bild mit leichtem Rauschen                            |\n",
    "| `\"content\"`   | Verwendet eine Kopie des Content-Bildes als Startbild                        |\n",
    "\n",
    "\n",
    "### Was passiert im Code?\n",
    "\n",
    "```python\n",
    "if init_image == \"random\":\n",
    "    tensor = torch.randn(C, H, W).mul(0.001).unsqueeze(0)\n",
    "```\n",
    "- Zufälliges Bild mit minimaler Varianz – verhindert zu starken Startbias  \n",
    "- `.mul(0.001)` → sehr kleine Zufallswerte  \n",
    "- `.unsqueeze(0)` → fügt Batch-Dimension `[1, C, H, W]` hinzu\n",
    "\n",
    "```python\n",
    "else:\n",
    "    tensor = content_tensor.clone().detach()\n",
    "```\n",
    "- Wenn `\"content\"` gewählt ist, wird das Content-Bild als Basis genommen  \n",
    "- `.clone().detach()` vermeidet unbeabsichtigte Verlinkung zum Original-Tensor\n",
    "\n",
    "\n",
    "### Warum ist die Initialisierung wichtig?\n",
    "\n",
    "- **Random** führt häufig zu interessanteren, aber unvorhersehbaren Ergebnissen\n",
    "- **Content** sorgt für eine schnellere Konvergenz und stabilere Ergebnisse\n",
    "\n",
    "> Die Wahl der Initialisierung beeinflusst also sowohl den Look als auch die Trainingsdynamik deines Style Transfers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Startbild erzeugen\n",
    "# -----------------------------------------------\n",
    "def initial(content_tensor, init_image=\"random\"):\n",
    "    B, C, H, W = content_tensor.shape\n",
    "    if init_image == \"random\":\n",
    "        tensor = torch.randn(C, H, W).mul(0.001).unsqueeze(0)\n",
    "    else:\n",
    "        tensor = content_tensor.clone().detach()\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (k) Stylization-Funktion: `stylize()`\n",
    "\n",
    "Diese Funktion führt den eigentlichen **Style Transfer** durch. Dabei wird ein Bild so lange optimiert, bis es den Inhalt des Content-Bildes und den Stil des Style-Bildes kombiniert.\n",
    "\n",
    "### Ablauf:\n",
    "\n",
    "1. **Feature-Extraktion**:\n",
    "   - Content-Features aus `relu4_2`\n",
    "   - Style-Features aus mehreren VGG-Schichten (`relu1_2` bis `relu5_3`)\n",
    "\n",
    "2. **Iterative Optimierung**:\n",
    "   - In jeder Iteration wird das Bild `g` angepasst\n",
    "   - Berechnet werden:\n",
    "     - **Content Loss**\n",
    "     - **Style Loss**\n",
    "     - **TV Loss** (Glättung)\n",
    "\n",
    "3. **Backpropagation**:\n",
    "   - Der Gesamtverlust wird minimiert\n",
    "   - Das Bild `g` wird direkt optimiert (nicht das Modell)\n",
    "\n",
    "4. **Verlauf speichern**:\n",
    "   - Alle Loss-Werte werden für spätere Visualisierung mitgeloggt\n",
    "\n",
    "5. **Ausgabe und Visualisierung**:\n",
    "   - Bild wird regelmäßig angezeigt und gespeichert\n",
    "   - Optional mit Farbübertragung (`PRESERVE_COLOR`)\n",
    "\n",
    "### Rückgabe:\n",
    "- Das stilisierte Bild `g`\n",
    "- Der komplette Verlustverlauf als Dictionary `loss_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Stylization-Funktion\n",
    "# -----------------------------------------------\n",
    "def stylize(iteration=NUM_ITER):\n",
    "    content_layers = [\"relu4_2\"]\n",
    "    content_weights = {\"relu4_2\": 1.0}\n",
    "    style_layers = [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\", \"relu5_3\"]\n",
    "    style_weights = {layer: 0.2 for layer in style_layers}\n",
    "\n",
    "    c_feat = get_features(model, content_tensor)\n",
    "    s_feat = get_features(model, style_tensor)\n",
    "\n",
    "    # ===> Neu: Verlaufs-Listen für Plot\n",
    "    loss_history = {\"total\": [], \"content\": [], \"style\": [], \"tv\": []}\n",
    "\n",
    "    i = [0]\n",
    "    while i[0] < iteration:\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            g_feat = get_features(model, g)\n",
    "\n",
    "            c_loss = sum(\n",
    "                content_weights[j] * content_loss(g_feat[j], c_feat[j])\n",
    "                for j in content_layers\n",
    "            )\n",
    "            s_loss = sum(\n",
    "                style_weights[j] * style_loss(g_feat[j], s_feat[j])\n",
    "                for j in style_layers\n",
    "            )\n",
    "\n",
    "            c_loss *= CONTENT_WEIGHT\n",
    "            s_loss *= STYLE_WEIGHT\n",
    "            t_loss = TV_WEIGHT * tv_loss(g.clone().detach())\n",
    "            total_loss = c_loss + s_loss + t_loss\n",
    "\n",
    "            total_loss.backward(retain_graph=True)\n",
    "\n",
    "            # ===> Verluste aufzeichnen\n",
    "            loss_history[\"content\"].append(c_loss.item())\n",
    "            loss_history[\"style\"].append(s_loss.item())\n",
    "            loss_history[\"tv\"].append(t_loss.item())\n",
    "            loss_history[\"total\"].append(total_loss.item())\n",
    "\n",
    "            i[0] += 1\n",
    "            if i[0] % SHOW_ITER == 1 or i[0] == NUM_ITER:\n",
    "                print(\n",
    "                    f\"Iteration {i[0]}: Style {s_loss.item():.2f}, Content {c_loss.item():.2f}, TV {t_loss.item():.2f}, Total {total_loss.item():.2f}\"\n",
    "                )\n",
    "                g_ = (\n",
    "                    transfer_color(\n",
    "                        ttoi(content_tensor.clone().detach()), ttoi(g.clone().detach())\n",
    "                    )\n",
    "                    if PRESERVE_COLOR == \"True\"\n",
    "                    else ttoi(g.clone().detach())\n",
    "                )\n",
    "                show(g_)\n",
    "                saveimg(g_, i[0] - 1)\n",
    "\n",
    "            return total_loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    return g, loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (l) Verlustverlauf visualisieren: `plot_losses()`\n",
    "\n",
    "Diese Funktion visualisiert den Verlauf der verschiedenen Verlustkomponenten während des Style Transfers. So kannst du nachvollziehen, wie sich das Training entwickelt hat.\n",
    "\n",
    "### Was wird geplottet?\n",
    "\n",
    "- **Content Loss**\n",
    "- **Style Loss**\n",
    "- **Total Variation Loss (TV)**\n",
    "- **Gesamtverlust (Total Loss)**\n",
    "\n",
    "### Darstellung:\n",
    "\n",
    "- Jeder Loss-Typ wird in einem eigenen **Subplot (2×2 Raster)** dargestellt\n",
    "- Gemeinsame x-Achse: Iterationen\n",
    "- Automatisches Styling mit `seaborn`\n",
    "\n",
    "### Optional:\n",
    "- Mit `save_path` kann der Plot als PNG gespeichert werden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Interaktive Plots mit Plotly\n",
    "# -----------------------------------------------\n",
    "def plot_losses_interactive(\n",
    "    losses,\n",
    "    save_path=\"data/loss_plot_interactive.html\",\n",
    "    dark_mode=False,\n",
    "    show_legend=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Erstellt einen interaktiven 2x2 Plot der Loss-Verläufe mit Plotly und speichert als HTML.\n",
    "\n",
    "    Args:\n",
    "        losses (dict): Dictionary mit Keys \"total\", \"content\", \"style\", \"tv\"\n",
    "        save_path (str): Zielpfad zum Speichern\n",
    "        dark_mode (bool): Ob Dark Mode verwendet werden soll\n",
    "        show_legend (bool): Ob die Legende angezeigt wird\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Iteration\": list(range(1, len(losses[\"total\"]) + 1)),\n",
    "            \"Total Loss\": losses[\"total\"],\n",
    "            \"Content Loss\": losses[\"content\"],\n",
    "            \"Style Loss\": losses[\"style\"],\n",
    "            \"TV Loss\": losses[\"tv\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig = sp.make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        subplot_titles=(\"Total Loss\", \"Content Loss\", \"Style Loss\", \"TV Loss\"),\n",
    "        horizontal_spacing=0.12,\n",
    "        vertical_spacing=0.15,\n",
    "    )\n",
    "\n",
    "    # Farben + Setup\n",
    "    colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\"]\n",
    "    keys = [\"Total Loss\", \"Content Loss\", \"Style Loss\", \"TV Loss\"]\n",
    "    positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "\n",
    "    for key, color, pos in zip(keys, colors, positions):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"Iteration\"],\n",
    "                y=df[key],\n",
    "                mode=\"lines\",\n",
    "                name=key,\n",
    "                line=dict(color=color, width=2),\n",
    "                hovertemplate=f\"<b>{key}</b><br>Iteration: %{{x}}<br>Loss: %{{y:.2e}}<extra></extra>\",\n",
    "            ),\n",
    "            row=pos[0],\n",
    "            col=pos[1],\n",
    "        )\n",
    "\n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=1200,\n",
    "        title_text=\"📈 <b>Interaktiver Loss-Verlauf pro Komponente</b>\",\n",
    "        title_x=0.5,\n",
    "        showlegend=show_legend,\n",
    "        template=\"plotly_dark\" if dark_mode else \"plotly_white\",\n",
    "        font=dict(size=14),\n",
    "        margin=dict(t=80, b=40),\n",
    "    )\n",
    "\n",
    "    # Achsentitel einheitlich\n",
    "    for i in range(1, 5):\n",
    "        fig[\"layout\"][f\"yaxis{i}\"].title = \"Loss\"\n",
    "        fig[\"layout\"][f\"xaxis{i}\"].title = \"Iteration\"\n",
    "\n",
    "    fig.write_html(save_path)\n",
    "    print(f\"✅ Interaktiver Plot gespeichert unter: {save_path}\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (m) Ausführung: Style Transfer starten\n",
    "\n",
    "In diesem Abschnitt wird der vollständige Style Transfer ausgeführt.\n",
    "\n",
    "### Schritte:\n",
    "\n",
    "1. **Bild-zu-Tensor-Konvertierung**\n",
    "   ```python\n",
    "   content_tensor = itot(content_img).to(device)\n",
    "   style_tensor = itot(style_img).to(device)\n",
    "   ```\n",
    "   - Die geladenen Bilder werden für das Modell vorbereitet\n",
    "\n",
    "2. **Startbild erzeugen**\n",
    "   ```python\n",
    "   g = initial(content_tensor, init_image=INIT_IMAGE).to(device).requires_grad_(True)\n",
    "   ```\n",
    "   - Entweder zufälliges Rauschen oder das Content-Bild als Ausgangspunkt\n",
    "\n",
    "3. **Optimierer festlegen**\n",
    "   ```python\n",
    "   optimizer = optim.Adam([g], lr=ADAM_LR)\n",
    "   ```\n",
    "   - Wahl zwischen `adam` (standardmäßig) oder `lbfgs`\n",
    "\n",
    "4. **Style Transfer starten**\n",
    "   ```python\n",
    "   out, losses = stylize(iteration=NUM_ITER)\n",
    "   ```\n",
    "   - Das Bild `g` wird über mehrere Iterationen so angepasst, dass es Stil und Inhalt kombiniert\n",
    "\n",
    "\n",
    "> Nach Abschluss enthält `out` das finale stilisierte Bild und `losses` die Entwicklung der Verlustfunktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Ausfuehrung\n",
    "# -----------------------------------------------\n",
    "content_tensor = itot(content_img).to(device)\n",
    "style_tensor = itot(style_img).to(device)\n",
    "g = initial(content_tensor, init_image=INIT_IMAGE).to(device).requires_grad_(True)\n",
    "\n",
    "if OPTIMIZER == \"lbfgs\":\n",
    "    optimizer = optim.LBFGS([g])\n",
    "elif OPTIMIZER == \"adam\":\n",
    "    optimizer = optim.Adam([g], lr=ADAM_LR)\n",
    "\n",
    "# Stylize!\n",
    "out, losses = stylize(iteration=NUM_ITER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (n) Loss-Verlauf anzeigen\n",
    "\n",
    "Nach dem Style Transfer wird der Verlauf der einzelnen Verlustkomponenten (Content, Style, TV, Total) visuell dargestellt.\n",
    "\n",
    "```python\n",
    "plot_losses(losses)\n",
    "```\n",
    "\n",
    "### Ziel:\n",
    "- **Verständnis für den Optimierungsverlauf** entwickeln\n",
    "- **Überwachung** der Konvergenz und Stabilität\n",
    "- **Identifikation von Ungleichgewichten** in der Gewichtung der Loss-Funktionen\n",
    "\n",
    "> Die Visualisierung hilft dir, das Verhalten des Modells über die Trainingszeit zu interpretieren und ggf. Hyperparameter anzupassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_interactive(losses, dark_mode=True, show_legend=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung: Was passiert hier?\n",
    "\n",
    "Dieser Code implementiert den klassischen **Neural Style Transfer** (nach Gatys et al., 2015).\n",
    "\n",
    "### Was wird gemacht?\n",
    "\n",
    "- **VGG19** wird als Feature-Extractor verwendet (nicht trainiert!)\n",
    "- Das **generierte Bild wird direkt optimiert**\n",
    "- Verluste:\n",
    "  - **Content Loss** (zwischen Feature-Maps)\n",
    "  - **Style Loss** (über Gram-Matrizen)\n",
    "  - **TV Loss** (Bildglättung)\n",
    "- Bild wird iterativ angepasst (nicht das Modell!)\n",
    "- Ergebnis: Ein stilisiertes Bild + Loss-Verlauf\n",
    "\n",
    "\n",
    "## Wie trainiert man stattdessen ein eigenes Modell?\n",
    "\n",
    "Beim **Fast Style Transfer** wird ein **CNN-Modell** trainiert, das beliebige Bilder im gewünschten Stil in **Echtzeit** umwandeln kann.\n",
    "\n",
    "### Dafür braucht man:\n",
    "\n",
    "1. **Datensatz** mit vielen Content-Bildern\n",
    "2. **Feedforward-Stilnetz** (z. B. ResNet oder Encoder–Decoder)\n",
    "3. **VGG19 (eingefroren)** für die Loss-Berechnung\n",
    "4. **Loss-Funktionen**: Content + Style + TV Loss\n",
    "5. **Training-Loop**: Optimierung des Netzwerks, nicht des Bildes\n",
    "\n",
    "\n",
    "### Vergleich\n",
    "\n",
    "| Klassisch (hier)        | Fast Style Transfer         |\n",
    "|-------------------------|-----------------------------|\n",
    "| Optimiert Bild direkt   | Trainiert CNN-Modell        |\n",
    "| Flexibel, aber langsam  | Schnell, aber stilgebunden  |\n",
    "| Kein Training nötig     | Modelltraining erforderlich |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "neural_style_transfer_SOLUTION",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
