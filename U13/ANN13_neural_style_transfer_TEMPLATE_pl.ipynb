{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> Â© Christoph WÃ¼rsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U13/ANN13_neural_style_transfer_TEMPLATE_pl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "**Neural Style Transfer (NST)** ist ein Verfahren im Bereich des Deep Learning, das es ermÃ¶glicht, den **Inhalt eines Bildes** mit dem **kÃ¼nstlerischen Stil eines anderen Bildes** zu kombinieren. Ziel ist es, ein neues Bild zu erzeugen, das die semantische Struktur des einen Bildes (z.â€¯B. eine Landschaft) mit der kÃ¼nstlerischen Ã„sthetik eines anderen Bildes (z.â€¯B. ein Van-Gogh-GemÃ¤lde) vereint.\n",
    "## Ziel\n",
    "\n",
    "Ein neues Bild generieren, das:\n",
    "\n",
    "- **Inhaltlich dem Content-Bild** entspricht  \n",
    "- **Stilistisch dem Style-Bild** Ã¤hnelt\n",
    "## Funktionsweise\n",
    "\n",
    "NST verwendet ein **vortrainiertes Convolutional Neural Network (CNN)** â€“ typischerweise **VGG19** â€“ um Bilder in sogenannte **Feature-ReprÃ¤sentationen** zu Ã¼berfÃ¼hren. Dabei wird unterschieden zwischen:\n",
    "\n",
    "- **Content-Features**: erfassen die rÃ¤umliche Struktur und Anordnung von Objekten  \n",
    "- **Style-Features**: erfassen die Textur, Farbverteilung und Muster â€“ Ã¼ber sogenannte **Gram-Matrizen**\n",
    "\n",
    "Das **generierte Bild** wird schrittweise angepasst, sodass es Ã¤hnliche Feature-ReprÃ¤sentationen wie die Eingabebilder aufweist.\n",
    "\n",
    "\n",
    "## Komponenten im Detail\n",
    "\n",
    "| Komponente       | Beschreibung                                                                 |\n",
    "|------------------|------------------------------------------------------------------------------|\n",
    "| **Content Loss** | Misst die Abweichung der Content-Features zwischen generiertem und Content-Bild |\n",
    "| **Style Loss**   | Misst die Abweichung der Gram-Matrizen zwischen generiertem und Style-Bild     |\n",
    "| **TV Loss**      | (optional) GlÃ¤ttet das Bild, um visuelle Artefakte zu reduzieren               |\n",
    "| **Optimierung**  | Das Bild selbst wird trainiert â€“ nicht das Netzwerk!                          |\n",
    "\n",
    "\n",
    "## Technischer Ablauf\n",
    "\n",
    "1. **Initialisierung**: Starte mit einem Rauschbild oder einer Kopie des Content-Bildes  \n",
    "2. **Feature Extraktion**: VGG-Netz extrahiert Features aus Content-, Style- und generiertem Bild  \n",
    "3. **Loss-Berechnung**: Vergleiche die Features und berechne Content- und Style-Loss  \n",
    "4. **Backpropagation**: Optimiere das Bild so, dass der kombinierte Loss minimiert wird  \n",
    "5. **Visualisierung**: Das Bild entwickelt sich mit jeder Iteration in Richtung â€žstilisiertes Ergebnisâ€œ\n",
    "\n",
    "\n",
    "## Anwendung in dieser Ãœbung\n",
    "\n",
    "In dieser Ãœbung wirst du:\n",
    "\n",
    "- Ein VGG19-Modell als Feature-Extractor nutzen  \n",
    "- Content- und Style-Features aus Beispielbildern extrahieren  \n",
    "- Ein neues Bild durch Optimierung erzeugen  \n",
    "- Verlaufsplots zur Analyse der Verluste Ã¼ber die Iterationen erstellen\n",
    "\n",
    "## Typische Beispiele\n",
    "\n",
    "| Content-Bild                | Style-Bild                   | Ergebnis                              |\n",
    "|-----------------------------|------------------------------|----------------------------------------|\n",
    "| Landschaftsfoto             | Monet-GemÃ¤lde                | Landschaft im impressionistischen Stil |\n",
    "| PortrÃ¤tfoto                 | Picasso-Zeichnung            | Abstraktes PortrÃ¤t                     |\n",
    "| Stadtbild                   | Van Gogh (Sternennacht)      | Leuchtende, wirbelnde Stadtansicht     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Setup der Umgebung\n",
    "\n",
    "Die Funktion `setup_environment()` prÃ¼ft, ob alle benÃ¶tigten Dateien fÃ¼r den Neural Style Transfer vorhanden sind â€“ insbesondere das vortrainierte VGG19-Modell und zwei Beispielbilder (Content & Style). Falls die Dateien nicht vorhanden sind, werden sie automatisch aus dem Internet heruntergeladen und an die richtigen Stellen gespeichert.\n",
    "\n",
    "### Was genau passiert:\n",
    "\n",
    "1. **VGG19-Modell laden:**\n",
    "   - PrÃ¼ft, ob die Datei `models/vgg19-d01eb7cb.pth` vorhanden ist\n",
    "   - Wenn nicht, wird das vortrainierte VGG19-Modell von der UniversitÃ¤t Michigan heruntergeladen und in den Ordner `models/` gespeichert\n",
    "\n",
    "2. **Beispielbilder laden:**\n",
    "   - PrÃ¼ft, ob `images/1-content.png` und `images/1-style.jpg` existieren\n",
    "   - Falls nicht:\n",
    "     - Wird ein ZIP-Archiv von GitHub heruntergeladen (`master.zip`)\n",
    "     - Es wird entpackt\n",
    "     - Die beiden Beispielbilder werden aus dem entpackten Ordner in den Ordner `images/` verschoben\n",
    "\n",
    "3. Am Ende wird bestÃ¤tigt, dass das Setup erfolgreich abgeschlossen wurde\n",
    "\n",
    "> Diese Funktion erleichtert den Einstieg und stellt sicher, dass das Notebook direkt lauffÃ¤hig ist â€“ ohne manuelles Herunterladen von Dateien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "\n",
    "def setup_environment():\n",
    "    # Basisverzeichnis\n",
    "    base_dir = \"data\"\n",
    "    model_dir = os.path.join(base_dir, \"models\")\n",
    "    image_dir = os.path.join(base_dir, \"images\")\n",
    "\n",
    "    # Check if VGG-Modell existiert\n",
    "    vgg_path = os.path.join(model_dir, \"vgg19-d01eb7cb.pth\")\n",
    "    if not os.path.exists(vgg_path):\n",
    "        print(\"Lade VGG19-Gewichte herunter ...\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        url = \"https://web.eecs.umich.edu/~justincj/models/vgg19-d01eb7cb.pth\"\n",
    "        urllib.request.urlretrieve(url, vgg_path)\n",
    "\n",
    "    # Check ob Bilder existieren\n",
    "    content_img = os.path.join(image_dir, \"1-content.png\")\n",
    "    style_img = os.path.join(image_dir, \"1-style.jpg\")\n",
    "    if not os.path.exists(content_img) or not os.path.exists(style_img):\n",
    "        print(\"Lade Beispielbilder herunter ...\")\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        zip_path = os.path.join(base_dir, \"master.zip\")\n",
    "        if not os.path.exists(zip_path):\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://github.com/iamRusty/neural-style-pytorch/archive/master.zip\",\n",
    "                zip_path,\n",
    "            )\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(base_dir)\n",
    "\n",
    "        extracted_path = os.path.join(base_dir, \"neural-style-pytorch-master\", \"images\")\n",
    "        os.rename(os.path.join(extracted_path, \"1-content.png\"), content_img)\n",
    "        os.rename(os.path.join(extracted_path, \"1-style.jpg\"), style_img)\n",
    "\n",
    "    print(\"âœ… Setup abgeschlossen. Dateien sind im 'data/' Ordner gespeichert.\")\n",
    "\n",
    "\n",
    "# Setup die Umgebung\n",
    "setup_environment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noch mehr Bilder um selber umzuschalten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zielordner\n",
    "target_dir = \"Bilder/StyleTransfer\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Bildliste aus Screenshot (alle .jpg + .png auÃŸer readme.md)\n",
    "filenames = [\n",
    "    \"Klimt2.jpg\",\n",
    "    \"Kopenhagen.jpg\",\n",
    "    \"Mosaic2.jpg\",\n",
    "    \"Sonnenblumen.jpg\",\n",
    "    \"Van_Gogh.jpg\",\n",
    "    \"Vassily_Kandinsky7.jpg\",\n",
    "    \"candy.jpg\",\n",
    "    \"composition-with-figures_popova.jpg\",\n",
    "    \"composition.jpg\",\n",
    "    \"graffiti.jpg\",\n",
    "    \"popova.png\",\n",
    "    \"portrait.jpg\",\n",
    "    \"portrait_mann.jpg\",\n",
    "    \"portrait_women.jpg\",\n",
    "    \"simpsons.jpg\",\n",
    "    \"udnie.jpg\",\n",
    "]\n",
    "\n",
    "# Basis-URL\n",
    "base_url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/data/StyleTransfer/\"\n",
    "\n",
    "# Download-Schleife\n",
    "for name in filenames:\n",
    "    save_path = os.path.join(target_dir, name)\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"âœ… Bereits vorhanden: {name}\")\n",
    "        continue\n",
    "    url = base_url + name\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, save_path)\n",
    "        print(f\"âœ… Heruntergeladen: {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fehler bei {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt werden alle benÃ¶tigten Bibliotheken fÃ¼r den Neural Style Transfer geladen. Sie ermÃ¶glichen Bildverarbeitung, Modellhandling, Visualisierung und numerische Optimierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Neural Style Transfer in PyTorch\n",
    "# Ziel: Inhalt eines Bildes mit dem Stil eines anderen kombinieren\n",
    "# -----------------------------------------------\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import plotly.subplots as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from torchvision import models, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Hyperparameter und Einstellungen\n",
    "\n",
    "In diesem Abschnitt werden alle zentralen Konfigurationsparameter fÃ¼r den Style Transfer definiert. Sie bestimmen, **wie stark Inhalt und Stil gewichtet werden**, wie das Bild initialisiert wird und welche Optimierungsmethode verwendet wird.\n",
    "\n",
    "### Modell- und Optimierungsparameter\n",
    "\n",
    "| Parameter         | Bedeutung                                                                 |\n",
    "|-------------------|---------------------------------------------------------------------------|\n",
    "| `MAX_IMAGE_SIZE`  | Maximale KantenlÃ¤nge der Eingabebilder (Skalierung)                       |\n",
    "| `OPTIMIZER`       | Optimierer: `adam` (schnell, stabil) oder `lbfgs` (klassisch, prÃ¤zise)    |\n",
    "| `ADAM_LR`         | Lernrate fÃ¼r den Adam-Optimierer                                          |\n",
    "| `CONTENT_WEIGHT`  | Gewichtung des Inhaltsverlustes (Content Loss)                            |\n",
    "| `STYLE_WEIGHT`    | Gewichtung des Stilverlustes (Style Loss)                                 |\n",
    "| `TV_WEIGHT`       | Gewichtung des GlÃ¤ttungsfaktors (Total Variation Loss)                    |\n",
    "| `NUM_ITER`        | Anzahl der Optimierungsschritte                                           |\n",
    "| `SHOW_ITER`       | Nach wie vielen Iterationen ein Zwischenbild angezeigt wird               |\n",
    "| `INIT_IMAGE`      | Startbild: `\"random\"` oder `\"content\"`                                    |\n",
    "| `PRESERVE_COLOR`  | Soll die Farbgebung des Content-Bildes erhalten bleiben? (`True/False`)   |\n",
    "| `PIXEL_CLIP`      | Soll das Bild nach jeder Iteration auf gÃ¼ltige Pixelwerte begrenzt werden?|\n",
    "\n",
    "### Pfade zu Ressourcen\n",
    "\n",
    "| Variable       | Beschreibung                                 |\n",
    "|----------------|----------------------------------------------|\n",
    "| `CONTENT_PATH` | Pfad zum Content-Bild                        |\n",
    "| `STYLE_PATH`   | Pfad zum Style-Bild                          |\n",
    "| `VGG19_PATH`   | Pfad zu den vortrainierten VGG19-Gewichten   |\n",
    "| `POOL`         | Art des Poolings im Netzwerk (`max` oder `avg`) |\n",
    "\n",
    "### GerÃ¤t wÃ¤hlen\n",
    "\n",
    "```python\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "```\n",
    "\n",
    "> Diese Zeile prÃ¼ft automatisch, ob eine GPU (CUDA) verfÃ¼gbar ist, und nutzt sie â€“ andernfalls wird die CPU verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Hyperparameter & Einstellungen\n",
    "# -----------------------------------------------\n",
    "MAX_IMAGE_SIZE = 512\n",
    "OPTIMIZER = \"adam\"  # 'adam' oder 'lbfgs'\n",
    "ADAM_LR = 10  # Lernrate\n",
    "CONTENT_WEIGHT = 5e0  # Gewichtung des Inhaltsverlustes\n",
    "STYLE_WEIGHT = 1e2  # Gewichtung des Stilverlustes\n",
    "TV_WEIGHT = 1e-3  # Gewichtung des Total Variation Loss\n",
    "NUM_ITER = 500  # Anzahl der Optimierungsiterationen\n",
    "SHOW_ITER = 100  # Anzeigeintervall\n",
    "INIT_IMAGE = \"random\"  # 'random' oder 'content'\n",
    "PRESERVE_COLOR = \"True\"  # Soll die Farbe des Content-Bildes beibehalten werden?\n",
    "PIXEL_CLIP = \"True\"  # Pixel nach jeder Iteration clippen\n",
    "\n",
    "# Pfade zu Bildern\n",
    "# =========== CONTENT PATHS =========== Bitte nur jeweils einen Pfad aktivieren\n",
    "CONTENT_PATH = \"data/images/1-content.png\"\n",
    "# CONTENT_PATH = \"Bilder/StyleTransfer/Kopenhagen.jpg\"\n",
    "# CONTENT_PATH = \"Bilder/StyleTransfer/portrait_mann.jpg\"\n",
    "# CONTENT_PATH = \"Bilder/StyleTransfer/portrait_women.jpg\"\n",
    "# CONTENT_PATH = \"Bilder/StyleTransfer/portrait.jpg\"\n",
    "# CONTENT_PATH = \"Bilder/StyleTransfer/simpsons.jpg\"\n",
    "# =========== STYLE PATHS =========== Bitte nur jeweils einen Pfad aktivieren\n",
    "STYLE_PATH = \"data/images/1-style.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/candy.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/Van_Gogh.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/udnie.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/Vassily_Kandinsky7.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/composition.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/composition-with-figures_popova.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/graffiti.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/Mosaic2.jpg\"\n",
    "# STYLE_PATH = \"Bilder/StyleTransfer/Sonnenblumen.jpg\"\n",
    "# =========== VGG PATH ===========\n",
    "VGG19_PATH = \"data/models/vgg19-d01eb7cb.pth\"\n",
    "POOL = \"max\"\n",
    "\n",
    "# GerÃ¤t festlegen\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Verwendetes GerÃ¤t:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Hilfsfunktionen fÃ¼r die Bildverarbeitung\n",
    "\n",
    "Diese Funktionen unterstÃ¼tzen das Laden, Anzeigen, Speichern und die FarbÃ¼bertragung von Bildern, was zentral fÃ¼r die Visualisierung und Verarbeitung im Neural Style Transfer ist.\n",
    "\n",
    "### FunktionsÃ¼bersicht: Bildverarbeitung\n",
    "\n",
    "| Funktion           | Zweck                                                  | Besonderheiten                                       |\n",
    "|--------------------|---------------------------------------------------------|------------------------------------------------------|\n",
    "| `load_image(path)` | Bild aus Datei laden (BGR-Format, OpenCV)              | Nutzt `cv2.imread()`                                 |\n",
    "| `show(img)`        | Bild anzeigen (matplotlib, RGB-Konvertierung)          | Skaliert Pixel auf [0,â€¯1], wandelt BGR â†’ RGB         |\n",
    "| `saveimg(img, iters)` | Stilisiertes Bild speichern                           | Erstellt Ordner `style_transfer_pictures/`, wandelt zu `uint8` |\n",
    "| `transfer_color(src, dest)` | FarbÃ¼bertragung vom Content-Bild auf das Stilbild | Arbeitet mit YCrCb-Farbraum, ersetzt Helligkeit      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Hilfsfunktionen fÃ¼r Bildverarbeitung\n",
    "# -----------------------------------------------\n",
    "def load_image(path):\n",
    "    return cv2.imread(path)  # BGR Format\n",
    "\n",
    "\n",
    "def show(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.array(img / 255).clip(0, 1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def saveimg(img, iters):\n",
    "    # Erstelle den Ordner 'style_transfer_pictures', falls er nicht existiert\n",
    "    output_dir = \"data/style_transfer_pictures\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    if PIXEL_CLIP == \"True\":\n",
    "        img = img.clip(0, 255)\n",
    "    img_uint8 = img.astype(np.uint8)\n",
    "    # Speichere das Bild im Ordner 'style_transfer_pictures'\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"out{iters}.png\"), img_uint8)\n",
    "\n",
    "\n",
    "def transfer_color(src, dest):\n",
    "    if PIXEL_CLIP == \"True\":\n",
    "        src, dest = src.clip(0, 255), dest.clip(0, 255)\n",
    "    H, W, _ = src.shape\n",
    "    dest = cv2.resize(dest, dsize=(W, H), interpolation=cv2.INTER_CUBIC)\n",
    "    dest_gray = cv2.cvtColor(dest, cv2.COLOR_BGR2GRAY)\n",
    "    src_yiq = cv2.cvtColor(src, cv2.COLOR_BGR2YCrCb)\n",
    "    src_yiq[..., 0] = dest_gray\n",
    "    return cv2.cvtColor(src_yiq, cv2.COLOR_YCrCb2BGR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Tensor-Bild Konvertierung\n",
    "\n",
    "FÃ¼r das Training und die Optimierung im Neural Style Transfer mÃ¼ssen Bilder als **Tensors** verarbeitet werden. Diese beiden Funktionen Ã¼bernehmen die Konvertierung zwischen **NumPy-Bilddaten (OpenCV)** und **PyTorch-Tensoren**, wobei sie auch das **Preprocessing bzw. Denormalisieren** mit den VGG19-spezifischen Mittelwerten Ã¼bernehmen.\n",
    "\n",
    "\n",
    "### Funktionen im Detail\n",
    "\n",
    "#### `itot(img)`\n",
    "**Image â†’ Tensor**\n",
    "\n",
    "- Wandelt ein Bild (`np.ndarray`, BGR) in einen PyTorch-Tensor um\n",
    "- Skaliert das Bild auf die maximale GrÃ¶ÃŸe `MAX_IMAGE_SIZE`\n",
    "- Normalisiert die FarbkanÃ¤le mit den VGG19-Trainingsmittelwerten:\n",
    "  `[103.939, 116.779, 123.68]` (BGR-Reihenfolge)\n",
    "- Multipliziert mit 255, da `ToTensor()` standardmÃ¤ÃŸig Werte in `[0, 1]` gibt\n",
    "- FÃ¼gt Batch-Dimension `[1, C, H, W]` hinzu\n",
    "\n",
    "#### `ttoi(tensor)`\n",
    "**Tensor â†’ Image**\n",
    "\n",
    "- Entfernt die Batch-Dimension\n",
    "- Revertiert die Normalisierung mit den negativen VGG-Mittelwerten\n",
    "- Wandelt den Tensor wieder in ein NumPy-Bild `[H, W, C]` um (RGB)\n",
    "- Gibt ein Bild-Array mit Werten im Bereich `[0, 255]` (float) zurÃ¼ck\n",
    "\n",
    "\n",
    "### FunktionsÃ¼bersicht\n",
    "\n",
    "| Funktion     | Richtung         | Zweck                                         | Besonderheiten                               |\n",
    "|--------------|------------------|-----------------------------------------------|----------------------------------------------|\n",
    "| `itot(img)`  | Bild â†’ Tensor     | Vorverarbeitung & Normalisierung fÃ¼r VGG19    | Skaliert Bild, wendet VGG-Mean-Norm an       |\n",
    "| `ttoi(tensor)` | Tensor â†’ Bild   | RÃ¼cktransformation zum Bildformat             | Macht Bild darstellbar, entfernt VGG-Norm    |\n",
    "\n",
    "\n",
    "> Diese Funktionen sorgen dafÃ¼r, dass deine Bilder korrekt als Netzwerk-Input verarbeitet werden kÃ¶nnen â€“ und spÃ¤ter auch wieder als visuell interpretierbare Ausgaben vorliegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Tensor-Image Konvertierung\n",
    "# -----------------------------------------------\n",
    "def itot(img):\n",
    "    H, W, _ = img.shape\n",
    "    image_size = tuple([int((MAX_IMAGE_SIZE / max(H, W)) * x) for x in [H, W]])\n",
    "    itot_t = transforms.Compose(\n",
    "        [transforms.ToPILImage(), transforms.Resize(image_size), transforms.ToTensor()]\n",
    "    )\n",
    "    normalize_t = transforms.Normalize([103.939, 116.779, 123.68], [1, 1, 1])\n",
    "    tensor = normalize_t(itot_t(img) * 255).unsqueeze(0)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def ttoi(tensor):\n",
    "    ttoi_t = transforms.Compose(\n",
    "        [transforms.Normalize([-103.939, -116.779, -123.68], [1, 1, 1])]\n",
    "    )\n",
    "    tensor = tensor.squeeze()\n",
    "    img = ttoi_t(tensor).cpu().numpy().transpose(1, 2, 0)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Bilder vorbereiten\n",
    "\n",
    "In diesem Schritt werden das **Content-Bild** und das **Style-Bild** geladen und visualisiert. Diese beiden Bilder sind die Grundlage fÃ¼r den spÃ¤teren Stiltransfer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Bilder vorbereiten\n",
    "# -----------------------------------------------\n",
    "content_img = load_image(CONTENT_PATH)\n",
    "style_img = load_image(STYLE_PATH)\n",
    "print(\"Content Image Shape:\", content_img.shape, \"-------------\")\n",
    "show(content_img)\n",
    "print(\"Style Image Shape:\", style_img.shape, \"-------------\")\n",
    "show(style_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) VGG19 Feature-Extractor vorbereiten\n",
    "\n",
    "In diesem Abschnitt wird ein **vortrainiertes VGG19-Netzwerk** als **Feature-Extractor** geladen und vorbereitet. Dieses CNN wird **nicht trainiert**, sondern lediglich verwendet, um **semantische Inhalte und stilistische Merkmale** aus den Bildern zu extrahieren.\n",
    "\n",
    "\n",
    "\n",
    "### Was passiert hier?\n",
    "\n",
    "1. **Laden des VGG19-Modells**\n",
    "   ```python\n",
    "   vgg = models.vgg19(pretrained=False)\n",
    "   vgg.load_state_dict(torch.load(VGG19_PATH), strict=False)\n",
    "   ```\n",
    "   - Das Modell wird **nicht automatisch** von `torchvision` heruntergeladen (`pretrained=False`)\n",
    "   - Stattdessen werden manuell geladene Gewichte verwendet (`VGG19_PATH`)\n",
    "   - Es handelt sich um ein **auf ImageNet vortrainiertes Modell**\n",
    "\n",
    "2. **Extraktion des Feature-Teils**\n",
    "   ```python\n",
    "   model = copy.deepcopy(vgg.features).to(device)\n",
    "   ```\n",
    "   - Nur der **Feature-Teil des VGG19-Netzes** wird verwendet\n",
    "   - Der Klassifikationskopf (`classifier`) wird entfernt\n",
    "\n",
    "3. **Deaktivieren der Gradientenberechnung**\n",
    "   ```python\n",
    "   for param in model.parameters():\n",
    "       param.requires_grad = False\n",
    "   ```\n",
    "   - Das Modell bleibt **eingefroren** wÃ¤hrend des Style Transfers\n",
    "   - Es dient nur zur **Berechnung der Verluste**, nicht zur Optimierung\n",
    "\n",
    "\n",
    "### Warum VGG19?\n",
    "\n",
    "- VGG19 ist ein tiefes CNN mit vielen kleinen Faltungen\n",
    "- Seine Zwischenlayer reprÃ¤sentieren sehr gut:\n",
    "  - **Inhalt** (mittlere Layer)\n",
    "  - **Stil** (frÃ¼he + tiefe Layer, Ã¼ber Gram-Matrizen)\n",
    "- Durch das Einfrieren bleiben diese ReprÃ¤sentationen stabil und zuverlÃ¤ssig\n",
    "\n",
    "\n",
    "> Das Modell extrahiert wichtige Merkmale aus Content- und Style-Bild und ermÃ¶glicht die **Berechnung der entsprechenden Verluste**, die das generierte Bild formen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# VGG19 Feature-Extractor vorbereiten\n",
    "# -----------------------------------------------\n",
    "vgg = models.vgg19(weights=None)\n",
    "vgg.load_state_dict(torch.load(VGG19_PATH, weights_only=False), strict=False)\n",
    "model = copy.deepcopy(vgg.features).to(device)\n",
    "\n",
    "# Gradienten ausschalten\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Parameter nicht trainierbar machen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g) Loss-Funktionen\n",
    "\n",
    "Beim Neural Style Transfer basiert die Optimierung auf verschiedenen Verlustfunktionen, die sicherstellen, dass das generierte Bild sowohl inhaltlich dem Content-Bild als auch stilistisch dem Style-Bild Ã¤hnelt. ZusÃ¤tzlich wird ein GlÃ¤ttungsterm verwendet, um visuelle Artefakte zu vermeiden.\n",
    "\n",
    "\n",
    "\n",
    "### Ãœbersicht der verwendeten Loss-Funktionen\n",
    "\n",
    "| Funktion        | Zweck                                                         |\n",
    "|-----------------|---------------------------------------------------------------|\n",
    "| **Content Loss** | Misst Ã„hnlichkeit zum Content-Bild anhand von VGG-Features     |\n",
    "| **Style Loss**   | Misst Ã„hnlichkeit zum Style-Bild Ã¼ber Gram-Matrizen           |\n",
    "| **TV Loss**      | Total Variation Loss â€“ sorgt fÃ¼r glatte ÃœbergÃ¤nge im Bild      |\n",
    "\n",
    "\n",
    "\n",
    "### Definitionen im Code\n",
    "\n",
    "#### Mean Squared Error Loss (Grundlage)\n",
    "```python\n",
    "mse_loss = nn.MSELoss()\n",
    "```\n",
    "Der mittlere quadratische Fehler bildet die Basis aller Verluste.\n",
    "\n",
    "\n",
    "\n",
    "#### Gram-Matrix: Stil-ReprÃ¤sentation\n",
    "```python\n",
    "def gram(tensor):\n",
    "    B, C, H, W = tensor.shape\n",
    "    x = tensor.view(C, H * W)\n",
    "    return torch.mm(x, x.t())\n",
    "```\n",
    "- Die Gram-Matrix misst die **Korrelation zwischen Feature-KanÃ¤len**\n",
    "- Je Ã¤hnlicher zwei KanÃ¤le feuern, desto hÃ¶her ihr Wert\n",
    "- Sie beschreibt **Textur und Stil**, unabhÃ¤ngig von der Position im Bild\n",
    "\n",
    "\n",
    "\n",
    "#### Content Loss\n",
    "```python\n",
    "def content_loss(g, c):\n",
    "    return mse_loss(g, c)\n",
    "```\n",
    "- Misst die Differenz der Features zwischen dem generierten Bild `g` und dem Content-Bild `c`\n",
    "- Nutzt eine mittlere Schicht (z.â€¯B. `relu4_2`) aus dem VGG-Netz\n",
    "\n",
    "\n",
    "\n",
    "#### Style Loss\n",
    "```python\n",
    "def style_loss(g, s):\n",
    "    c1, _ = g.shape\n",
    "    return mse_loss(g, s) / (c1**2)\n",
    "```\n",
    "- Nutzt Gram-Matrizen aus mehreren VGG-Schichten (z.â€¯B. `relu1_2`, `relu3_3`, ...)\n",
    "- Die Division durch `c1Â²` (KanalanzahlÂ²) normalisiert die VerlustgrÃ¶ÃŸe\n",
    "\n",
    "\n",
    "\n",
    "#### Total Variation Loss\n",
    "```python\n",
    "def tv_loss(c):\n",
    "    x = c[:, :, 1:, :] - c[:, :, :-1, :]\n",
    "    y = c[:, :, :, 1:] - c[:, :, :, :-1]\n",
    "    return torch.sum(torch.abs(x)) + torch.sum(torch.abs(y))\n",
    "```\n",
    "- Belohnt **rÃ¤umlich glatte Strukturen**\n",
    "- Hilft, **visuelles Rauschen** im Bild zu vermeiden\n",
    "- Optional, aber hÃ¤ufig mit groÃŸem Einfluss auf das Ergebnis\n",
    "\n",
    "\n",
    "\n",
    "> Diese Verluste werden gewichtet kombiniert und bilden die Zielfunktion, nach der das generierte Bild optimiert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Loss-Funktionen\n",
    "# -----------------------------------------------\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "def gram(tensor):\n",
    "    B, C, H, W = tensor.shape\n",
    "    x = tensor.view(C, H * W)\n",
    "    return torch.mm(x, x.t())\n",
    "\n",
    "\n",
    "def content_loss(g, c):\n",
    "    return mse_loss(g, c)\n",
    "\n",
    "\n",
    "def style_loss(g, s):\n",
    "    c1, _ = g.shape\n",
    "    return mse_loss(g, s) / (c1**2)\n",
    "\n",
    "\n",
    "def tv_loss(c):\n",
    "    x = c[:, :, 1:, :] - c[:, :, :-1, :]\n",
    "    y = c[:, :, :, 1:] - c[:, :, :, :-1]\n",
    "    return torch.sum(torch.abs(x)) + torch.sum(torch.abs(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (h) Feature Extraction aus VGG-Schichten\n",
    "\n",
    "Diese Funktion extrahiert **Content- und Style-Features** aus einem Eingabetensor mit Hilfe des VGG19-Modells. Dabei werden bestimmte Layer gezielt abgefragt, um die ReprÃ¤sentationen fÃ¼r Inhalt und Stil zu erhalten.\n",
    "\n",
    "\n",
    "### Was macht `get_features(model, tensor)`?\n",
    "\n",
    "- FÃ¼hrt eine **Forward-Pass** durch das VGG-Modell aus\n",
    "- Speichert die **Aktivierungen bestimmter Schichten** (jeweils als Feature-Maps)\n",
    "- Verwendet:\n",
    "  - **`relu4_2`** fÃ¼r den **Inhalt**\n",
    "  - **Mehrere Schichten** (`relu1_2`, `relu2_2`, etc.) fÃ¼r den **Stil** (Gram-Matrizen)\n",
    "- Berechnet Gram-Matrizen fÃ¼r Style-Features direkt im Funktionsverlauf\n",
    "\n",
    "\n",
    "### Verwendete Layer im VGG19\n",
    "\n",
    "| Layer-ID | Name im VGG      | Verwendung         |\n",
    "|----------|------------------|--------------------|\n",
    "| `\"3\"`    | `relu1_2`        | Stil-Feature       |\n",
    "| `\"8\"`    | `relu2_2`        | Stil-Feature       |\n",
    "| `\"17\"`   | `relu3_3`        | Stil-Feature       |\n",
    "| `\"26\"`   | `relu4_3`        | Stil-Feature       |\n",
    "| `\"35\"`   | `relu5_3`        | Stil-Feature       |\n",
    "| `\"22\"`   | `relu4_2`        | Content-Feature    |\n",
    "\n",
    "> Diese Layer wurden so gewÃ¤hlt, da sie sich im Originalpaper von Gatys et al. als besonders geeignet fÃ¼r die Darstellung von Stil- und Inhaltsinformationen erwiesen haben.\n",
    "\n",
    "\n",
    "### Besonderheiten im Code\n",
    "\n",
    "```python\n",
    "if name == \"22\":\n",
    "    features[layers[name]] = x\n",
    "```\n",
    "- Der Inhalt wird **direkt als Feature-Map gespeichert**\n",
    "\n",
    "```python\n",
    "features[layers[name]] = gram(x) / (H * W)\n",
    "```\n",
    "- Style-Features werden als **normierte Gram-Matrizen** gespeichert (nach FlÃ¤che)\n",
    "\n",
    "```python\n",
    "if name == \"35\":\n",
    "    break\n",
    "```\n",
    "- Stoppt die Verarbeitung nach der letzten benÃ¶tigten Schicht â†’ spart Rechenzeit\n",
    "\n",
    "\n",
    "> Diese extrahierten Features werden spÃ¤ter fÃ¼r die Berechnung des Content- und Style-Loss verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Feature Extraction aus VGG Layers\n",
    "# -----------------------------------------------\n",
    "def get_features(model, tensor):\n",
    "    layers = {\n",
    "        \"3\": \"relu1_2\",\n",
    "        \"8\": \"relu2_2\",\n",
    "        \"17\": \"relu3_3\",\n",
    "        \"26\": \"relu4_3\",\n",
    "        \"35\": \"relu5_3\",\n",
    "        \"22\": \"relu4_2\",  # Content Layer\n",
    "    }\n",
    "    features = {}\n",
    "    x = tensor\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            if name == \"22\":\n",
    "                features[layers[name]] = x\n",
    "            else:\n",
    "                B, C, H, W = x.shape\n",
    "                features[layers[name]] = gram(x) / (H * W)\n",
    "        if name == \"35\":\n",
    "            break\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Startbild erzeugen\n",
    "\n",
    "Bevor das stilisierte Bild optimiert werden kann, muss ein **Startbild** definiert werden. Dieses Bild ist der Ausgangspunkt fÃ¼r die iterative Optimierung, bei der sich das Bild schrittweise an Inhalt und Stil annÃ¤hert.\n",
    "\n",
    "\n",
    "### Funktion: `initial(content_tensor, init_image=\"random\")`\n",
    "\n",
    "Diese Funktion erzeugt das Startbild auf Basis des gewÃ¤hlten Initialisierungstyps.\n",
    "\n",
    "#### Zwei MÃ¶glichkeiten der Initialisierung:\n",
    "\n",
    "| Modus         | Beschreibung                                                                 |\n",
    "|---------------|-------------------------------------------------------------------------------|\n",
    "| `\"random\"`    | Erzeugt ein zufÃ¤lliges Bild mit leichtem Rauschen                            |\n",
    "| `\"content\"`   | Verwendet eine Kopie des Content-Bildes als Startbild                        |\n",
    "\n",
    "\n",
    "### Was passiert im Code?\n",
    "\n",
    "```python\n",
    "if init_image == \"random\":\n",
    "    tensor = torch.randn(C, H, W).mul(0.001).unsqueeze(0)\n",
    "```\n",
    "- ZufÃ¤lliges Bild mit minimaler Varianz â€“ verhindert zu starken Startbias  \n",
    "- `.mul(0.001)` â†’ sehr kleine Zufallswerte  \n",
    "- `.unsqueeze(0)` â†’ fÃ¼gt Batch-Dimension `[1, C, H, W]` hinzu\n",
    "\n",
    "```python\n",
    "else:\n",
    "    tensor = content_tensor.clone().detach()\n",
    "```\n",
    "- Wenn `\"content\"` gewÃ¤hlt ist, wird das Content-Bild als Basis genommen  \n",
    "- `.clone().detach()` vermeidet unbeabsichtigte Verlinkung zum Original-Tensor\n",
    "\n",
    "\n",
    "### Warum ist die Initialisierung wichtig?\n",
    "\n",
    "- **Random** fÃ¼hrt hÃ¤ufig zu interessanteren, aber unvorhersehbaren Ergebnissen\n",
    "- **Content** sorgt fÃ¼r eine schnellere Konvergenz und stabilere Ergebnisse\n",
    "\n",
    "> Die Wahl der Initialisierung beeinflusst also sowohl den Look als auch die Trainingsdynamik deines Style Transfers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Startbild erzeugen\n",
    "# -----------------------------------------------\n",
    "def initial(content_tensor, init_image=\"random\"):\n",
    "    B, C, H, W = content_tensor.shape\n",
    "    if init_image == \"random\":\n",
    "        tensor = torch.randn(C, H, W).mul(0.001).unsqueeze(0)\n",
    "    else:\n",
    "        tensor = content_tensor.clone().detach()\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (k) Stylization-Funktion: `stylize()`\n",
    "\n",
    "Diese Funktion fÃ¼hrt den eigentlichen **Style Transfer** durch. Dabei wird ein Bild so lange optimiert, bis es den Inhalt des Content-Bildes und den Stil des Style-Bildes kombiniert.\n",
    "\n",
    "### Ablauf:\n",
    "\n",
    "1. **Feature-Extraktion**:\n",
    "   - Content-Features aus `relu4_2`\n",
    "   - Style-Features aus mehreren VGG-Schichten (`relu1_2` bis `relu5_3`)\n",
    "\n",
    "2. **Iterative Optimierung**:\n",
    "   - In jeder Iteration wird das Bild `g` angepasst\n",
    "   - Berechnet werden:\n",
    "     - **Content Loss**\n",
    "     - **Style Loss**\n",
    "     - **TV Loss** (GlÃ¤ttung)\n",
    "\n",
    "3. **Backpropagation**:\n",
    "   - Der Gesamtverlust wird minimiert\n",
    "   - Das Bild `g` wird direkt optimiert (nicht das Modell)\n",
    "\n",
    "4. **Verlauf speichern**:\n",
    "   - Alle Loss-Werte werden fÃ¼r spÃ¤tere Visualisierung mitgeloggt\n",
    "\n",
    "5. **Ausgabe und Visualisierung**:\n",
    "   - Bild wird regelmÃ¤ÃŸig angezeigt und gespeichert\n",
    "   - Optional mit FarbÃ¼bertragung (`PRESERVE_COLOR`)\n",
    "\n",
    "### RÃ¼ckgabe:\n",
    "- Das stilisierte Bild `g`\n",
    "- Der komplette Verlustverlauf als Dictionary `loss_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Stylization-Funktion\n",
    "# -----------------------------------------------\n",
    "def stylize(iteration=NUM_ITER):\n",
    "    content_layers = [\"relu4_2\"]\n",
    "    content_weights = {\"relu4_2\": 1.0}\n",
    "    style_layers = [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\", \"relu5_3\"]\n",
    "    style_weights = {layer: 0.2 for layer in style_layers}\n",
    "\n",
    "    c_feat = get_features(model, content_tensor)\n",
    "    s_feat = get_features(model, style_tensor)\n",
    "\n",
    "    # ===> Neu: Verlaufs-Listen fÃ¼r Plot\n",
    "    loss_history = {\"total\": [], \"content\": [], \"style\": [], \"tv\": []}\n",
    "\n",
    "    i = [0]\n",
    "    while i[0] < iteration:\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            g_feat = get_features(model, g)\n",
    "\n",
    "            c_loss = sum(\n",
    "                content_weights[j] * content_loss(g_feat[j], c_feat[j])\n",
    "                for j in content_layers\n",
    "            )\n",
    "            s_loss = sum(\n",
    "                style_weights[j] * style_loss(g_feat[j], s_feat[j])\n",
    "                for j in style_layers\n",
    "            )\n",
    "\n",
    "            c_loss *= CONTENT_WEIGHT\n",
    "            s_loss *= STYLE_WEIGHT\n",
    "            t_loss = TV_WEIGHT * tv_loss(g.clone().detach())\n",
    "            total_loss = c_loss + s_loss + t_loss\n",
    "\n",
    "            total_loss.backward(retain_graph=True)\n",
    "\n",
    "            # ===> Verluste aufzeichnen\n",
    "            loss_history[\"content\"].append(c_loss.item())\n",
    "            loss_history[\"style\"].append(s_loss.item())\n",
    "            loss_history[\"tv\"].append(t_loss.item())\n",
    "            loss_history[\"total\"].append(total_loss.item())\n",
    "\n",
    "            i[0] += 1\n",
    "            if i[0] % SHOW_ITER == 1 or i[0] == NUM_ITER:\n",
    "                print(\n",
    "                    f\"Iteration {i[0]}: Style {s_loss.item():.2f}, Content {c_loss.item():.2f}, TV {t_loss.item():.2f}, Total {total_loss.item():.2f}\"\n",
    "                )\n",
    "                g_ = (\n",
    "                    transfer_color(\n",
    "                        ttoi(content_tensor.clone().detach()), ttoi(g.clone().detach())\n",
    "                    )\n",
    "                    if PRESERVE_COLOR == \"True\"\n",
    "                    else ttoi(g.clone().detach())\n",
    "                )\n",
    "                show(g_)\n",
    "                saveimg(g_, i[0] - 1)\n",
    "\n",
    "            return total_loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    return g, loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (l) Verlustverlauf visualisieren: `plot_losses()`\n",
    "\n",
    "Diese Funktion visualisiert den Verlauf der verschiedenen Verlustkomponenten wÃ¤hrend des Style Transfers. So kannst du nachvollziehen, wie sich das Training entwickelt hat.\n",
    "\n",
    "### Was wird geplottet?\n",
    "\n",
    "- **Content Loss**\n",
    "- **Style Loss**\n",
    "- **Total Variation Loss (TV)**\n",
    "- **Gesamtverlust (Total Loss)**\n",
    "\n",
    "### Darstellung:\n",
    "\n",
    "- Jeder Loss-Typ wird in einem eigenen **Subplot (2Ã—2 Raster)** dargestellt\n",
    "- Gemeinsame x-Achse: Iterationen\n",
    "- Automatisches Styling mit `seaborn`\n",
    "\n",
    "### Optional:\n",
    "- Mit `save_path` kann der Plot als PNG gespeichert werden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Interaktive Plots mit Plotly\n",
    "# -----------------------------------------------\n",
    "def plot_losses_interactive(\n",
    "    losses,\n",
    "    save_path=\"data/loss_plot_interactive.html\",\n",
    "    dark_mode=False,\n",
    "    show_legend=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Erstellt einen interaktiven 2x2 Plot der Loss-VerlÃ¤ufe mit Plotly und speichert als HTML.\n",
    "\n",
    "    Args:\n",
    "        losses (dict): Dictionary mit Keys \"total\", \"content\", \"style\", \"tv\"\n",
    "        save_path (str): Zielpfad zum Speichern\n",
    "        dark_mode (bool): Ob Dark Mode verwendet werden soll\n",
    "        show_legend (bool): Ob die Legende angezeigt wird\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Iteration\": list(range(1, len(losses[\"total\"]) + 1)),\n",
    "            \"Total Loss\": losses[\"total\"],\n",
    "            \"Content Loss\": losses[\"content\"],\n",
    "            \"Style Loss\": losses[\"style\"],\n",
    "            \"TV Loss\": losses[\"tv\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig = sp.make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        subplot_titles=(\"Total Loss\", \"Content Loss\", \"Style Loss\", \"TV Loss\"),\n",
    "        horizontal_spacing=0.12,\n",
    "        vertical_spacing=0.15,\n",
    "    )\n",
    "\n",
    "    # Farben + Setup\n",
    "    colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\"]\n",
    "    keys = [\"Total Loss\", \"Content Loss\", \"Style Loss\", \"TV Loss\"]\n",
    "    positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "\n",
    "    for key, color, pos in zip(keys, colors, positions):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"Iteration\"],\n",
    "                y=df[key],\n",
    "                mode=\"lines\",\n",
    "                name=key,\n",
    "                line=dict(color=color, width=2),\n",
    "                hovertemplate=f\"<b>{key}</b><br>Iteration: %{{x}}<br>Loss: %{{y:.2e}}<extra></extra>\",\n",
    "            ),\n",
    "            row=pos[0],\n",
    "            col=pos[1],\n",
    "        )\n",
    "\n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=1200,\n",
    "        title_text=\"ðŸ“ˆ <b>Interaktiver Loss-Verlauf pro Komponente</b>\",\n",
    "        title_x=0.5,\n",
    "        showlegend=show_legend,\n",
    "        template=\"plotly_dark\" if dark_mode else \"plotly_white\",\n",
    "        font=dict(size=14),\n",
    "        margin=dict(t=80, b=40),\n",
    "    )\n",
    "\n",
    "    # Achsentitel einheitlich\n",
    "    for i in range(1, 5):\n",
    "        fig[\"layout\"][f\"yaxis{i}\"].title = \"Loss\"\n",
    "        fig[\"layout\"][f\"xaxis{i}\"].title = \"Iteration\"\n",
    "\n",
    "    fig.write_html(save_path)\n",
    "    print(f\"âœ… Interaktiver Plot gespeichert unter: {save_path}\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (m) AusfÃ¼hrung: Style Transfer starten\n",
    "\n",
    "In diesem Abschnitt wird der vollstÃ¤ndige Style Transfer ausgefÃ¼hrt.\n",
    "\n",
    "### Schritte:\n",
    "\n",
    "1. **Bild-zu-Tensor-Konvertierung**\n",
    "   ```python\n",
    "   content_tensor = itot(content_img).to(device)\n",
    "   style_tensor = itot(style_img).to(device)\n",
    "   ```\n",
    "   - Die geladenen Bilder werden fÃ¼r das Modell vorbereitet\n",
    "\n",
    "2. **Startbild erzeugen**\n",
    "   ```python\n",
    "   g = initial(content_tensor, init_image=INIT_IMAGE).to(device).requires_grad_(True)\n",
    "   ```\n",
    "   - Entweder zufÃ¤lliges Rauschen oder das Content-Bild als Ausgangspunkt\n",
    "\n",
    "3. **Optimierer festlegen**\n",
    "   ```python\n",
    "   optimizer = optim.Adam([g], lr=ADAM_LR)\n",
    "   ```\n",
    "   - Wahl zwischen `adam` (standardmÃ¤ÃŸig) oder `lbfgs`\n",
    "\n",
    "4. **Style Transfer starten**\n",
    "   ```python\n",
    "   out, losses = stylize(iteration=NUM_ITER)\n",
    "   ```\n",
    "   - Das Bild `g` wird Ã¼ber mehrere Iterationen so angepasst, dass es Stil und Inhalt kombiniert\n",
    "\n",
    "\n",
    "> Nach Abschluss enthÃ¤lt `out` das finale stilisierte Bild und `losses` die Entwicklung der Verlustfunktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Ausfuehrung\n",
    "# -----------------------------------------------\n",
    "content_tensor = itot(content_img).to(device)\n",
    "style_tensor = itot(style_img).to(device)\n",
    "g = initial(content_tensor, init_image=INIT_IMAGE).to(device).requires_grad_(True)\n",
    "\n",
    "if OPTIMIZER == \"lbfgs\":\n",
    "    optimizer = optim.LBFGS([g])\n",
    "elif OPTIMIZER == \"adam\":\n",
    "    optimizer = optim.Adam([g], lr=ADAM_LR)\n",
    "\n",
    "# Stylize!\n",
    "out, losses = stylize(iteration=NUM_ITER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (n) Loss-Verlauf anzeigen\n",
    "\n",
    "Nach dem Style Transfer wird der Verlauf der einzelnen Verlustkomponenten (Content, Style, TV, Total) visuell dargestellt.\n",
    "\n",
    "```python\n",
    "plot_losses(losses)\n",
    "```\n",
    "\n",
    "### Ziel:\n",
    "- **VerstÃ¤ndnis fÃ¼r den Optimierungsverlauf** entwickeln\n",
    "- **Ãœberwachung** der Konvergenz und StabilitÃ¤t\n",
    "- **Identifikation von Ungleichgewichten** in der Gewichtung der Loss-Funktionen\n",
    "\n",
    "> Die Visualisierung hilft dir, das Verhalten des Modells Ã¼ber die Trainingszeit zu interpretieren und ggf. Hyperparameter anzupassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_interactive(losses, dark_mode=True, show_legend=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung: Was passiert hier?\n",
    "\n",
    "Dieser Code implementiert den klassischen **Neural Style Transfer** (nach Gatys et al., 2015).\n",
    "\n",
    "### Was wird gemacht?\n",
    "\n",
    "- **VGG19** wird als Feature-Extractor verwendet (nicht trainiert!)\n",
    "- Das **generierte Bild wird direkt optimiert**\n",
    "- Verluste:\n",
    "  - **Content Loss** (zwischen Feature-Maps)\n",
    "  - **Style Loss** (Ã¼ber Gram-Matrizen)\n",
    "  - **TV Loss** (BildglÃ¤ttung)\n",
    "- Bild wird iterativ angepasst (nicht das Modell!)\n",
    "- Ergebnis: Ein stilisiertes Bild + Loss-Verlauf\n",
    "\n",
    "\n",
    "## Wie trainiert man stattdessen ein eigenes Modell?\n",
    "\n",
    "Beim **Fast Style Transfer** wird ein **CNN-Modell** trainiert, das beliebige Bilder im gewÃ¼nschten Stil in **Echtzeit** umwandeln kann.\n",
    "\n",
    "### DafÃ¼r braucht man:\n",
    "\n",
    "1. **Datensatz** mit vielen Content-Bildern\n",
    "2. **Feedforward-Stilnetz** (z.â€¯B. ResNet oder Encoderâ€“Decoder)\n",
    "3. **VGG19 (eingefroren)** fÃ¼r die Loss-Berechnung\n",
    "4. **Loss-Funktionen**: Content + Style + TV Loss\n",
    "5. **Training-Loop**: Optimierung des Netzwerks, nicht des Bildes\n",
    "\n",
    "\n",
    "### Vergleich\n",
    "\n",
    "| Klassisch (hier)        | Fast Style Transfer         |\n",
    "|-------------------------|-----------------------------|\n",
    "| Optimiert Bild direkt   | Trainiert CNN-Modell        |\n",
    "| Flexibel, aber langsam  | Schnell, aber stilgebunden  |\n",
    "| Kein Training nÃ¶tig     | Modelltraining erforderlich |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "neural_style_transfer_SOLUTION",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
