{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U02/BinaryClassification_HeartDataset_SOLUTION-PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" height=\"120\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ3fzvovEJcF"
   },
   "source": [
    "# Binäre Klassifikation mit kontinuierlichen und kategorischen Merkmalen\n",
    "\n",
    "**Author:** \n",
    "- Christoph Würsch, Eastern Switzerland University of Applied Science OST\n",
    "- [Francois Chollet](https://twitter.com/fchollet)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Übungsserie zeigt, wie eine strukturierte Datenklassifizierung ausgehend von einer rohen\n",
    "CSV-Datei mit keras vorgenommen werden kann. Die verwendeten Daten enthalten sowohl numerische als auch kategorische Merkmale. Wir verwenden Keras Vorverarbeitungsschichten zur Normalisierung der numerischen Merkmale und zur Vektorisierung (one-hot-coding) der kategorischen Merkmale.\n",
    "\n",
    "### Der Datensatz\n",
    "\n",
    "[Unser Datensatz](https://archive.ics.uci.edu/ml/datasets/heart+Disease) wird von der Cleveland Clinic Foundation für Herzkrankheiten zur Verfügung gestellt. Es handelt sich um eine CSV-Datei mit 303 Zeilen. Jede Zeile enthält Informationen über einen Patienten (eine **Stichprobe**), und jede Spalte beschreibt ein Attribut des Patienten (ein **Merkmal**). Wir verwenden die Merkmale, um vorherzusagen, ob ein Patient eine Herzerkrankung hat (**binäre Klassifizierung**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a11dwWcfEJcK"
   },
   "source": [
    "\n",
    "Hier ist eine Zusammenfassung der Merkmale:\n",
    "\n",
    "Column| Description| Feature Type\n",
    "------------|--------------------|----------------------\n",
    "Age | Age in years | Numerical\n",
    "Sex | (1 = male; 0 = female) | Categorical\n",
    "CP | Chest pain type (0, 1, 2, 3, 4) | Categorical\n",
    "Trestbpd | Resting blood pressure (in mm Hg on admission) | Numerical\n",
    "Chol | Serum cholesterol in mg/dl | Numerical\n",
    "FBS | fasting blood sugar in 120 mg/dl (1 = true; 0 = false) | Categorical\n",
    "RestECG | Resting electrocardiogram results (0, 1, 2) | Categorical\n",
    "Thalach | Maximum heart rate achieved | Numerical\n",
    "Exang | Exercise induced angina (1 = yes; 0 = no) | Categorical\n",
    "Oldpeak | ST depression induced by exercise relative to rest | Numerical\n",
    "Slope | Slope of the peak exercise ST segment | Numerical\n",
    "CA | Number of major vessels (0-3) colored by fluoroscopy | Both numerical & categorical\n",
    "Thal | 3 = normal; 6 = fixed defect; 7 = reversible defect | Categorical\n",
    "Target | Diagnosis of heart disease (1 = true; 0 = false) | Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aGD4lDjEJcL"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8c3sKxCEJcM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Print versions in a compact form\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment this line, if pytorch-lightning is already installed\n",
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N2fL3ZyEJcN"
   },
   "source": [
    "## (a) Datensatz laden\n",
    "\n",
    "Wir laden wir die Daten herunter und speichern diese in einen Pandas-Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsnBkHs9EJcN"
   },
   "outputs": [],
   "source": [
    "\n",
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "df = pd.read_csv(file_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCvmAtqZEJcO"
   },
   "source": [
    "Der Datensatz umfasst 303 Proben mit 14 Spalten pro Probe (13 Merkmale, plus die Zielbezeichnung Bezeichnung):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRU0h1hwEJcO"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die letzte Spalte, `target`, gibt an, ob der Patient eine Herzerkrankung hat (`1`) oder nicht (`0`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ur0_3TxEJcP"
   },
   "source": [
    "Hier ist ein kurzer Einblick in die Daten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOk0TWXDEJcP"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_features= df.select_dtypes(include=['number']).columns.tolist()\n",
    "print(numeric_features)\n",
    "\n",
    "corr=df[numeric_features].corr()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool_)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, vmin=0.0, center=0, annot=True,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=df.columns[0:-1]\n",
    "response=df.columns[-1]\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x5 grid of subplots\n",
    "fig, axes = plt.subplots(3, 5, figsize=(20, 12))  # Adjust figsize for readability\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through features and create boxplots\n",
    "for i, feature in enumerate(features):\n",
    "    sns.boxplot(data=df, x='target', y=feature, ax=axes[i])\n",
    "    axes[i].set_title(f'Boxplot of {feature}')\n",
    "    axes[i].set_xlabel('Target')\n",
    "    axes[i].set_ylabel(feature)\n",
    "\n",
    "# Hide any unused subplots (if features < 15)\n",
    "for i in range(len(features), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Standardisierung und One-hot-Encoding\n",
    "\n",
    "Die folgenden Merkmale sind kategorische Merkmale, die als ganze Zahlen kodiert sind:\n",
    "\n",
    "- `Geschlecht`\n",
    "- `cp` \n",
    "- `fbs`\n",
    "- `restecg`\n",
    "- `exang`\n",
    "- `ca`\n",
    "\n",
    "Wir kodieren diese Merkmale mit **one-hot encoding**. Wir haben zwei Optionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical= ['sex', 'cp', 'fbs', 'restecg', 'exang', 'ca', 'thal']\n",
    "\n",
    "df_onehot=pd.get_dummies(data=df.iloc[:,1:-1],columns=categorical)\n",
    "\n",
    "features=df_onehot.columns\n",
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Aufteilen in einen Trainings- und Validierungsdatensatz\n",
    "\n",
    "Wir teilen den Datensatz auf in einen Trainings- und Validierungsdatensatz. Hierfür verwenden wir direkt die Methoden `df.sample` und `df.drop()` eines Pandas-Datenframes `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Dataframe with correct encodiding\n",
    "numericFeatures=features[0:5]\n",
    "categoricFeatures=features[5:]\n",
    "\n",
    "print(numericFeatures)\n",
    "print(categoricFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot[features].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_onehot[features].astype(np.float32).to_numpy()\n",
    "y=df['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h69QGLyqEJcQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg=pd.DataFrame(data=X_train, columns=features)\n",
    "dg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Standardisieren der quantitativen, kontinuierlichen Merkmale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "myScaler=StandardScaler()\n",
    "Xtrain= np.hstack((myScaler.fit_transform(X_train[:,0:5]),X_train[:,5:]))\n",
    "Xtest = np.hstack((myScaler.fit_transform(X_test[:,0:5]),X_test[:,5:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Erstellen der Daten-Klasse (`Dataset`) und des `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = CustomDataset(Xtrain, y_train)\n",
    "test_dataset  = CustomDataset(Xtest, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Move data to device (optional, for demonstration purposes)\n",
    "for X_batch, y_batch in train_loader:\n",
    "    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "    print(f\"Batch X shape: {X_batch.shape}, Batch y shape: {y_batch.shape}\")\n",
    "    break  # Just to show one batch, remove in actual training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData=np.hstack((Xtrain,y_train.reshape(-1,1)))\n",
    "valData =np.hstack((Xtest,y_test.reshape(-1,1)))\n",
    "fullFeatures=list(features)\n",
    "fullFeatures.append('target')\n",
    "print(fullFeatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können aber auch mit Hilfe einer helper-Funktion direkt aus den `pandas`-Dataframes die `DataLoader`erzeugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe=pd.DataFrame(data=trainData, columns=fullFeatures)\n",
    "train_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataframe=pd.DataFrame(data=valData, columns=fullFeatures)\n",
    "val_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert DataFrame to tensors\n",
    "def dataframe_to_dataloader(df, target_column, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Converts a pandas DataFrame into a PyTorch DataLoader.\n",
    "    Args:\n",
    "    - df: pandas DataFrame with features and target.\n",
    "    - target_column: Name of the column containing the target variable.\n",
    "    - batch_size: Size of batches in the DataLoader.\n",
    "    - shuffle: Whether to shuffle the data.\n",
    "    \"\"\"\n",
    "    features = torch.tensor(df.drop(columns=[target_column]).values, dtype=torch.float32)\n",
    "    target   = torch.tensor(df[target_column].values, dtype=torch.float32)\n",
    "    \n",
    "    dataset = TensorDataset(features, target)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "batch_size = 32\n",
    "train_loader = dataframe_to_dataloader(train_dataframe, target_column='target', batch_size=16)\n",
    "val_loader   = dataframe_to_dataloader(val_dataframe, target_column='target', batch_size=16)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch_features, batch_targets in train_loader:\n",
    "    print(batch_features.shape, batch_targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g) Erstellen der Modellarchitektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "class ClassifierNet(pl.LightningModule):\n",
    "    def __init__(self, n_hidden=32):\n",
    "        super(ClassifierNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output = nn.Linear(n_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # Initialize metric storage\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(acc.item())\n",
    "        return {'loss': loss, 'train_acc': acc}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_accuracies.append(acc.item())\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (h) Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "model = ClassifierNet(n_hidden=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Example input size: (batch_size, input_features)\n",
    "summary(model, input_size=(batch_size,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer setup\n",
    "max_epochs=50\n",
    "trainer = pl.Trainer(max_epochs=40, log_every_n_steps=1)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Lernkurven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot learning curves\n",
    "def plot_learning_curves(model):\n",
    "    epochs_train = np.array(range(1, len(model.train_losses) + 1)) / len(model.train_losses)*max_epochs \n",
    "    epochs_val = np.array(range(1, len(model.val_losses) + 1)) / len(model.val_losses)*max_epochs\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_train, model.train_losses, 'b.-', label='Training Loss')\n",
    "    plt.plot(epochs_val, model.val_losses, 'r.-', label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_train, model.train_accuracies, 'b.-', label='Training Accuracy')\n",
    "    plt.plot(epochs_val, model.val_accuracies, 'r.-', label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (j) Eine Funktion für Training und Darstellung\n",
    "\n",
    "Nun führen wir alles in einer einzigen Funktion zusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainAndPlot(n_hidden=32,max_epochs=50):\n",
    "    # Model instantiation\n",
    "    model = ClassifierNet(n_hidden=n_hidden)\n",
    "    # Trainer setup\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs, log_every_n_steps=1)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    plot_learning_curves(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (j) Ein etwas anderes Netzwerk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainAndPlot(n_hidden=32,max_epochs=50)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Variable Anzahl an Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierNet(pl.LightningModule):\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        Initialize the classifier network.\n",
    "        \n",
    "        Args:\n",
    "        - layer_sizes (list): A list of integers specifying the number of nodes\n",
    "                              in each hidden layer. The first element is the input size,\n",
    "                              and the last element is the output size.\n",
    "        \"\"\"\n",
    "        super(ClassifierNet, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if i < len(layer_sizes) - 2:  # Add dropout only for hidden layers\n",
    "                self.layers.append(nn.ReLU())\n",
    "                self.layers.append(nn.Dropout(p=0.5))  # Dropout probability is 0.5\n",
    "\n",
    "        self.output_activation = nn.Sigmoid()  # Output layer activation for binary classification\n",
    "        self.criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "        # Initialize metric storage\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_activation(x)  # Apply sigmoid activation at the output layer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(acc.item())\n",
    "        return {'loss': loss, 'train_acc': acc}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_accuracies.append(acc.item())\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainPlot(layer_sizes, batch_size, max_epochs=50):\n",
    "    # Model instantiation\n",
    "    model = ClassifierNet(layer_sizes)\n",
    "    # Trainer setup\n",
    "    summary(model, input_size=(batch_size,28))\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs, log_every_n_steps=1);\n",
    "    trainer.fit(model, train_loader, val_loader);\n",
    "    plot_learning_curves(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "layer_sizes = [28, 16, 32, 32, 16, 1]  # Input size, hidden layers, output size\n",
    "batch_size=32\n",
    "TrainPlot(layer_sizes,batch_size, max_epochs=50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "structured_data_classification_from_scratch",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "MachLe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
