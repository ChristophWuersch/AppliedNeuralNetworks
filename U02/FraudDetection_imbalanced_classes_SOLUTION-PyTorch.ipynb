{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U02/FraudDetection_imbalanced_classes_SOLUTION-PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "\n",
    "# Imbalanced classification: Missbräuchliche Kreditkartenbezüge\n",
    "\n",
    "Demonstration des Umgangs mit stark unausgewogenen Klassifikationsproblemen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Dieses Beispiel befasst sich mit der\n",
    "[Kaggle Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud/)\n",
    "Datensatz, um zu demonstrieren, wie\n",
    "wie man ein Klassifizierungsmodell auf Daten mit stark unausgewogenen Klassen trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Print versions in a compact form\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning\n",
    "!pip install torchmetrics\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Datenaufbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/\n",
    "#fname = \"creditcard.csv.gz\"\n",
    "#data = pd.read_csv(fname)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/U02/creditcard.csv.gz\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "features = data.iloc[:, :-1].values  # All columns except the last\n",
    "targets = data.iloc[:, -1].values    # The last column\n",
    "\n",
    "print(\"features.shape:\", features.shape)\n",
    "print(\"targets.shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Alternative Datenquelle \n",
    "url = \"https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "features = data.iloc[:, :-1].values  # All columns except the last\n",
    "targets = data.iloc[:, -1].values    # The last column\n",
    "print(\"features.shape:\", features.shape)\n",
    "print(\"targets.shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (a) Aufteilen in einen Trainings- und Validierungsdatensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "num_val_samples = int(len(features) * 0.2)\n",
    "train_features = features[:-num_val_samples]\n",
    "train_targets = targets[:-num_val_samples].reshape(-1)\n",
    "val_features = features[-num_val_samples:]\n",
    "val_targets = targets[-num_val_samples:].reshape(-1)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_features))\n",
    "print(\"Number of validation samples:\", len(val_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (b) Analyse des Klassenungleichgewichts in der Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "counts = np.bincount(train_targets[:])\n",
    "print(counts)\n",
    "\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(train_targets)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (c) Standardisierung der Daten auf Basis der Statistik der Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "mean = np.mean(train_features, axis=0)\n",
    "train_features -= mean\n",
    "val_features -= mean\n",
    "std = np.std(train_features, axis=0)\n",
    "train_features /= std\n",
    "val_features /= std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = CustomDataset(train_features, train_targets)\n",
    "val_dataset   = CustomDataset(val_features, val_targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,drop_last=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the DataLoader\n",
    "for batch_features, batch_targets in train_loader:\n",
    "    print(batch_features.shape, batch_targets.shape)\n",
    "    break\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch_features, batch_targets in val_loader:\n",
    "    print(batch_features.shape, batch_targets.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (d) Aufbauen eines binären Klassifizierungsmodells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konfigurierbares Classifier-Netzwerk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class ClassifierNet(pl.LightningModule):\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        Initialize the classifier network.\n",
    "        \n",
    "        Args:\n",
    "        - layer_sizes (list): A list of integers specifying the number of nodes\n",
    "                              in each hidden layer. The first element is the input size,\n",
    "                              and the last element is the output size.\n",
    "        \"\"\"\n",
    "        super(ClassifierNet, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if i < len(layer_sizes) - 2:  # Add dropout only for hidden layers\n",
    "                self.layers.append(nn.ReLU())\n",
    "                self.layers.append(nn.Dropout(p=0.5))  # Dropout probability is 0.5\n",
    "\n",
    "        self.output_activation = nn.Sigmoid()  # Output layer activation for binary classification\n",
    "        self.criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "        # Initialize metric storage\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_activation(x)  # Apply sigmoid activation at the output layer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y.squeeze())\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(acc.item())\n",
    "        return {'loss': loss, 'train_acc': acc}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y.squeeze())\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_accuracies.append(acc.item())\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-Funktion für die Lernkurven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot learning curves\n",
    "def plot_learning_curves(model,max_epochs):\n",
    "    epochs_train = np.array(range(1, len(model.train_losses) + 1)) / len(model.train_losses)*max_epochs \n",
    "    epochs_val = np.array(range(1, len(model.val_losses) + 1)) / len(model.val_losses)*max_epochs\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_train, model.train_losses, 'b.-', label='Training Loss')\n",
    "    plt.plot(epochs_val, model.val_losses, 'r.-', label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_train, model.train_accuracies, 'b.-', label='Training Accuracy')\n",
    "    plt.plot(epochs_val, model.val_accuracies, 'r.-', label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Modell instanzieren und trainieren\n",
    "\n",
    "Verwenden Sie ein MLP mit der folgenden Anzahl an Neuronen:\n",
    "- Input Layer: 30 Neuronen (Dimension des Datensatzes)\n",
    "1. Hidden Layer: 16 Neuronen + Dropout + ReLU-Aktivierung\n",
    "2. Hidden Layer: 32 Neuronen + Dropout + ReLU-Aktivierung\n",
    "3. Hidden Layer: 32 Neuronen + Dropout + ReLU-Aktivierung\n",
    "4. Hidden Layer: 16 Neuronen \n",
    "- Output Layer: 1 (sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "layer_sizes = [30, 16, 32, 32, 16, 1]  # Input size, hidden layers, output size\n",
    "model = ClassifierNet(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainAndPlot(model, max_epochs=5):\n",
    "    # Trainer setup\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs, log_every_n_steps=1)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    plot_learning_curves(model, max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "TrainAndPlot(model, max_epochs=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (f) Trainieren Sie das Modell mit dem `class_weight`-Argument \n",
    "\n",
    "Da die Klassen sehr stark imbalanced sind, werden wir den Loss der Minderheitsklasse (minority class) indirekt proportional zu ihrem Anteil gewichten.\n",
    "\n",
    "$$ w_0 = \\frac{n_{\\text{tot}}}{n_0}= \\frac{n_0+n_1}{n_0}$$\n",
    "$$ w_1 = \\frac{n_{\\text{tot}}}{n_1}= \\frac{n_0+n_1}{n_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class weights (example with imbalanced data)\n",
    "total_samples = len(train_targets)\n",
    "weight_for_0 = total_samples / counts[0].item()\n",
    "weight_for_1 = total_samples / counts[1].item()\n",
    "class_weights = torch.tensor([weight_for_0, weight_for_1])\n",
    "print(\"Class weights:\", class_weights)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[0]/counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class ClassifierNet(pl.LightningModule):\n",
    "    def __init__(self, layer_sizes, class_weights):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - layer_sizes (list): A list specifying the layer sizes for the model.\n",
    "        - class_weights (torch.Tensor): Class weights for balancing loss computation.\n",
    "        \"\"\"\n",
    "        super(ClassifierNet, self).__init__()\n",
    "\n",
    "        self.class_weights = class_weights  # Store class weights\n",
    "\n",
    "        # Define the model\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if i < len(layer_sizes) - 2:  # Add activation and dropout for hidden layers\n",
    "                self.layers.append(nn.ReLU())\n",
    "                self.layers.append(nn.Dropout(0.5))  # Dropout probability\n",
    "\n",
    "        self.output_activation = nn.Sigmoid()  # Sigmoid for binary classification\n",
    "\n",
    "        # Initialize metric storage\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_activation(x)\n",
    "\n",
    "    def compute_loss(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        Compute weighted loss for a batch.\n",
    "        Args:\n",
    "        - y_hat: Predicted outputs\n",
    "        - y: Ground-truth labels\n",
    "        \"\"\"\n",
    "        # Generate weights for each batch element\n",
    "        batch_weights = self.class_weights[y.long()]  # Map weights based on labels\n",
    "        loss_fn = nn.BCELoss(weight=batch_weights)    # Use BCELoss with per-batch weights\n",
    "        loss = loss_fn(y_hat, y.float())\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()  # Forward pass\n",
    "        loss = self.compute_loss(y_hat,y)  # Compute loss\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(acc.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze() # Forward pass\n",
    "        loss = self.compute_loss(y_hat,y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_accuracies.append(acc.item())\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir berechnen für jede Klasse das zugehörige Klassengewicht und instanzieren unser Modell (`model2`) mit der neuen Klasse, die in der Verlustfunktion die Klassengewichte berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Model initialization\n",
    "layer_sizes = [30, 16, 32, 32, 16, 1]  # Input size, hidden layers, output size\n",
    "model2 = ClassifierNet(layer_sizes, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[0].item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun starten wir das Training mit `PyTorch Lightning`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainAndPlot(model2, max_epochs=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g) Berechnen der Konfusionsmatrix\n",
    "\n",
    "Metriken wie **True Positives (TP)**, **True Negatives (TN)**, **False Positives (FP)**, **False Negatives (FN)** und den **F1-Score** lassen sich in PyTorch Lightning einfach berechnen und nach dem Training für sowohl `train_loader` als auch `test_loader` anzeigen. Wir verwenden die Bibliothek `torchmetrics`, die nahtlos mit PyTorch Lightning integriert werden kann.\n",
    "\n",
    "Diese Begriffe `TP,TN,FP,FN`stammen aus der **Confusion Matrix**, die zur Evaluierung von Klassifikationsmodellen verwendet wird. Für ein binäres Klassifikationsproblem gibt es zwei Klassen: **Positiv** (1) und **Negativ** (0).\n",
    "\n",
    "- **True Positives (TP):** Anzahl der positiven Beispiele, die korrekt als positiv vorhergesagt wurden.\n",
    "- **True Negatives (TN):** Anzahl der negativen Beispiele, die korrekt als negativ vorhergesagt wurden.\n",
    "- **False Positives (FP):** Anzahl der negativen Beispiele, die fälschlicherweise als positiv vorhergesagt wurden.\n",
    "- **False Negatives (FN):** Anzahl der positiven Beispiele, die fälschlicherweise als negativ vorhergesagt wurden.\n",
    "\n",
    "Der **F1-Score** ist ein Maß für die Balance zwischen **Precision** und **Recall**. Er wird wie folgt berechnet:\n",
    "\n",
    "1. **Precision (P):** Der Anteil der korrekt vorhergesagten positiven Beispiele an allen als positiv vorhergesagten Beispielen:\n",
    "   $$ P = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$\n",
    "\n",
    "2. **Recall (R):** Der Anteil der korrekt vorhergesagten positiven Beispiele an allen tatsächlich positiven Beispielen:\n",
    "   $$ R = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n",
    "\n",
    "3. **F1-Score:** Das harmonische Mittel von Precision und Recall:\n",
    "   $$ F1 = 2 \\cdot \\frac{P \\cdot R}{P + R} $$\n",
    "\n",
    "\n",
    "Falls `torchmetrics` noch nicht installiert ist: `pip install torchmetrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def compute_and_display_metrics(model, data_loader, metrics, device, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    Compute and display metrics for a given data loader.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained PyTorch Lightning model.\n",
    "    - data_loader: DataLoader for the dataset (train or test).\n",
    "    - metrics: torchmetrics.MetricCollection to compute metrics.\n",
    "    - device: Device to perform computations (e.g., 'cpu' or 'cuda').\n",
    "    - mode: 'train' or 'test' to label the output.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    metrics.reset()  # Reset metrics\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x).squeeze()\n",
    "\n",
    "            preds = (y_hat > 0.5).long()  # Binary predictions\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(y)\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "\n",
    "    # Compute metrics\n",
    "    metric_results = metrics(all_preds, all_targets)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nMetrics for {mode} set:\")\n",
    "    for name, value in metric_results.items():\n",
    "        if isinstance(value, torch.Tensor) and value.numel() > 1:  # For tensors like confusion matrix\n",
    "            print(f\"{name}: \\n{value}\")\n",
    "        else:  # Scalar metrics like F1-score\n",
    "            print(f\"{name}: {value.item():.4f}\")\n",
    "\n",
    "    # Return results for further use if needed\n",
    "    return metric_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryConfusionMatrix, BinaryF1Score\n",
    "from torchmetrics import MetricCollection, ConfusionMatrix, F1Score\n",
    "\n",
    "# Define metrics for binary classification\n",
    "metrics = MetricCollection({\n",
    "    \"confusion_matrix\": BinaryConfusionMatrix(),  # Confusion Matrix for binary tasks\n",
    "    \"f1_score\": BinaryF1Score(),                 # F1-score for binary tasks\n",
    "}).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train_loader' and 'val_loader' are defined, and 'model' is trained\n",
    "# Compute metrics for train and test sets\n",
    "compute_and_display_metrics(model2, train_loader, metrics, device, mode=\"train\")\n",
    "compute_and_display_metrics(model2, val_loader, metrics, device, mode=\"test\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (h) Interpretation, Bewertung des Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Am Ende des Trainings haben wir von 56.961 Validierungstransaktionen:\n",
    "\n",
    "- 60 von ihnen korrekt als betrügerisch identifiziert\n",
    "- 15 betrügerische Transaktionen werden übersehen\n",
    "- um den Preis, dass wir 120 legitime Transaktionen falsch kennzeichnen\n",
    "\n",
    "In der realen Welt würde man der Klasse 1 ein noch höheres Gewicht beimessen,\n",
    "um der Tatsache Rechnung zu tragen, dass falsche Negativmeldungen teurer sind als falsche Positivmeldungen.\n",
    "\n",
    "Das nächste Mal, wenn Ihre Kreditkarte bei einem Online-Einkauf abgelehnt wird - das ist der Grund."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "imbalanced_classification",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ANNtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
