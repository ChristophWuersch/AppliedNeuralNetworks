{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN04/4.3-AutoDiff_Demo.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "%pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AutoGrad-Demo\n",
    "\n",
    "Wir werden uns drei verschiedene Methoden zur Berechnung von Gradienten ansehen, die alle auf demselben Grundprinzip beruhen (Backprop). \n",
    "\n",
    "1. Die erste Methode besteht darin, eine Funktion und ihre Ableitungen manuell nach den Regeln von Backprop zu kodieren.\n",
    "2. Die zweite Methode ist die automatische Differenzierung mit einem Tool namens\n",
    "[autograd](https://github.com/HIPS/autograd).\n",
    "3. Die dritte Methode ist die Verwendung von [Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mehr über ``autograd``\n",
    "\n",
    "[autograd](https://github.com/HIPS/autograd) ist ein Python-Paket für **algorithmische Differenzierung**. Es erlaubt die automatische Berechnung der Ableitung von Funktionen, die in (fast) nativem Code geschrieben sind. Das macht die Berechnung von Ableitungen sehr einfach. Unter der Haube verwendet es auch Reverse Mode Autodiff (Backprop).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation: Beispiel 1\n",
    "\n",
    "Wir werden drei verschiedene Möglichkeiten zur Berechnung des Gradienten von\n",
    "\n",
    "$$f(x,y,z) = (2x + y)\\cdot z$$\n",
    "\n",
    "am Punkt $(x,y,z)=(1,2,3)$\n",
    "\n",
    "### Manuelle Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install autograd\n",
    "# !git clone https://github.com/dsgiitr/d2l-pytorch.git\n",
    "# Homepage\n",
    "# https://d2l.ai/\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Backprop example\n",
    "\n",
    "# Compute f(x,y,z) = (2*x+y)*z\n",
    "x = 1.0\n",
    "y = 2.0\n",
    "z = 3.0\n",
    "\n",
    "# Forward pass\n",
    "q = 2.0 * x + y  # Node 1\n",
    "f = q * z  # Node 2\n",
    "\n",
    "# Backward pass\n",
    "f_bar = 1\n",
    "q_bar = z * f_bar  # Node 2 input\n",
    "z_bar = q * f_bar  # Node 2 input\n",
    "x_bar = 2 * q_bar  # Node 1 input\n",
    "y_bar = 1 * q_bar  # Node 1 input\n",
    "\n",
    "grad = np.array([x_bar, y_bar, z_bar])\n",
    "\n",
    "print(f)\n",
    "print(grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install autograd\n",
    "\n",
    "import autograd.numpy as np  # Thinly wrapped version of numpy\n",
    "from autograd import grad\n",
    "\n",
    "\n",
    "def f(args):\n",
    "    x, y, z = args\n",
    "    return (2 * x + y) * z\n",
    "\n",
    "\n",
    "f_grad = grad(f)  # magic: returns a function that computes the gradient of f\n",
    "\n",
    "x = 1.0\n",
    "y = 2.0\n",
    "z = 3.0\n",
    "\n",
    "print(f([x, y, z]))\n",
    "print(f_grad([x, y, z]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation: Beispiel 2\n",
    "\n",
    "Hier ist ein etwas komplizierteres Beispiel:\n",
    "\n",
    "$$f(x) = 10\\cdot \\exp(\\sin(x)) + \\cos^2(x)$$\n",
    "\n",
    "\n",
    "Berechnen Sie mit manuell mit dem Backpropagation Algorithmus den Gradienten am Punkt $(x,y,z)=(1,2,3)$\n",
    "\n",
    "### Manual backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Backprop example\n",
    "# f(x) = 10*np.exp(np.sin(x)) + np.cos(x)**2\n",
    "\n",
    "# Forward pass\n",
    "x = 1000\n",
    "a = np.sin(x)  # Node 1\n",
    "b = np.cos(x)  # Node 1\n",
    "c = b**2  # Node 3\n",
    "d = np.exp(a)  # Node 4\n",
    "f = 10 * d + c  # Node 5 (final output)\n",
    "\n",
    "# Backward pass\n",
    "f_bar = 1\n",
    "d_bar = 10 * f_bar  # Node 5 input\n",
    "c_bar = 1 * f_bar  # Node 5 input\n",
    "a_bar = np.exp(a) * d_bar  # Node 4 input\n",
    "b_bar = 2 * b * c_bar  # Node 3 input\n",
    "x_bar = np.cos(x) * a_bar - np.sin(x) * b_bar  # Node 2 and 1 input\n",
    "\n",
    "print(f, x_bar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np  # Thinly wrapped version of numpy\n",
    "from autograd import grad\n",
    "\n",
    "\n",
    "def f(args):\n",
    "    x = args\n",
    "    return 10 * np.exp(np.sin(x)) + np.cos(x) ** 2\n",
    "\n",
    "\n",
    "f_grad = grad(f)\n",
    "\n",
    "x = 1000.0\n",
    "\n",
    "print(f([x]))\n",
    "print(f_grad([x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pytorch-lightning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.autograd import grad\n",
    "\n",
    "# PyTorch Lightning Backpropagation und AutoDiff Beispiele\n",
    "\n",
    "class BackpropModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialisiere Tensoren mit requires_grad=True, um Berechnungen zu verfolgen\n",
    "        self.x = torch.tensor(1.0, requires_grad=True)\n",
    "        self.y = torch.tensor(2.0, requires_grad=True)\n",
    "        self.z = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "    def forward(self):\n",
    "        # Berechne q = 2*x + y\n",
    "        q = 2 * self.x + self.y  # Knoten 1\n",
    "        # Berechne f = q * z\n",
    "        f = q * self.z  # Knoten 2\n",
    "        return f\n",
    "\n",
    "    def compute_gradients(self):\n",
    "        # Führe den Vorwärtsdurchlauf durch\n",
    "        f = self.forward()\n",
    "        # Berechne Gradienten (Rückwärtsdurchlauf)\n",
    "        f.backward()\n",
    "        # Gebe die Gradienten von x, y und z zurück\n",
    "        return self.x.grad, self.y.grad, self.z.grad\n",
    "\n",
    "# PyTorch Lightning Version der komplexen Funktion f(x)\n",
    "\n",
    "class ComplexFunction(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialisiere Tensor mit requires_grad=True, um Berechnungen zu verfolgen\n",
    "        self.x = torch.tensor(1000.0, requires_grad=True)\n",
    "\n",
    "    def forward(self):\n",
    "        # Berechne a = sin(x)\n",
    "        a = torch.sin(self.x)  # Knoten 1\n",
    "        # Berechne b = cos(x)\n",
    "        b = torch.cos(self.x)  # Knoten 2\n",
    "        # Berechne c = b^2\n",
    "        c = b**2  # Knoten 3\n",
    "        # Berechne d = exp(a)\n",
    "        d = torch.exp(a)  # Knoten 4\n",
    "        # Berechne f = 10*d + c\n",
    "        f = 10 * d + c  # Knoten 5 (Endergebnis)\n",
    "        return f\n",
    "\n",
    "    def compute_gradients(self):\n",
    "        # Führe den Vorwärtsdurchlauf durch\n",
    "        f = self.forward()\n",
    "        # Berechne Gradienten (Rückwärtsdurchlauf)\n",
    "        f.backward()\n",
    "        # Gebe den Gradienten von x zurück\n",
    "        return self.x.grad\n",
    "\n",
    "\n",
    "# Instanziiere und berechne Gradienten\n",
    "model = BackpropModel()\n",
    "grads = model.compute_gradients()\n",
    "print(\"Funktionsausgabe:\", model.forward().item())\n",
    "print(\"Gradienten:\", grads)\n",
    "\n",
    "# Führe das Beispiel der komplexen Funktion aus\n",
    "complex_model = ComplexFunction()\n",
    "x_grad = complex_model.compute_gradients()\n",
    "print(\"Ausgabe der komplexen Funktion:\", complex_model.forward().item())\n",
    "print(\"Gradient der komplexen Funktion:\", x_grad.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
