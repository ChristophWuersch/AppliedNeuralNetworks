{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U06/ANN06_resnet_TEMPLATE_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN U06: CNN für Computer Vision\n",
    "### Imports\n",
    "Hier werden alle notwendigen Bibliotheken für das Training eines neuronalen Netzes mit PyTorch Lightning importiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchviz import make_dot\n",
    "from pytorch_lightning import Trainer\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from torchmetrics.functional import accuracy\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1: ResNet – Training und Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Allgemeine Einstellungen und Konstanten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diese Werte werden als Standard genutzt, falls die automatische Berechnung deaktiviert ist.\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary: Übersetzung der one-hot-codierten Vektoren in reale Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktion: Unnormalisieren der Bilder für Visualisierungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(img, mean, std):\n",
    "    # Erstelle eine Kopie des Bildes, um die Originaldaten nicht zu verändern\n",
    "    img = img.clone().detach()\n",
    "    # Denormalisiere das Bild, indem die Mittelwerte hinzugefügt und durch die Standardabweichungen geteilt werden\n",
    "    for t, m, s in zip(img, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule: CIFAR-10 Vorbereitung und DataLoader mit automatischer Berechnung von Mean und Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir=\"./data\", batch_size=BATCH_SIZE, auto_normalize=True):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.auto_normalize = auto_normalize\n",
    "\n",
    "        # Platzhalter für die Normalisierungswerte\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "        # Vorläufige Transformation: Nur in Tensor umwandeln\n",
    "        self.base_transform = transforms.ToTensor()\n",
    "\n",
    "        # Die finale Transformationskette wird in setup() definiert, nachdem ggf. Mean/Std berechnet wurden.\n",
    "        self.transform = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Datensatz herunterladen, falls noch nicht vorhanden\n",
    "        datasets.CIFAR10(self.data_dir, train=True, download=True)\n",
    "        datasets.CIFAR10(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Erstelle den Trainingsdatensatz ohne Normalisierung, um die Werte zu berechnen\n",
    "        train_dataset = datasets.CIFAR10(\n",
    "            self.data_dir, train=True, transform=self.base_transform\n",
    "        )\n",
    "\n",
    "        if self.auto_normalize:\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                train_dataset, batch_size=100, shuffle=False, num_workers=NUM_WORKERS\n",
    "            )\n",
    "            mean = torch.zeros(3)\n",
    "            std = torch.zeros(3)\n",
    "            nb_samples = 0\n",
    "            for data, _ in loader:\n",
    "                batch_samples = data.size(0)\n",
    "                data = data.view(batch_samples, data.size(1), -1)\n",
    "                mean += data.mean(2).sum(0)\n",
    "                std += data.std(2).sum(0)\n",
    "                nb_samples += batch_samples\n",
    "            self.mean = (mean / nb_samples).tolist()\n",
    "            self.std = (std / nb_samples).tolist()\n",
    "        else:\n",
    "            self.mean = CIFAR10_MEAN\n",
    "            self.std = CIFAR10_STD\n",
    "\n",
    "        # Jetzt die finale Transformation definieren (inklusive Normalisierung)\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize(self.mean, self.std)]\n",
    "        )\n",
    "\n",
    "        # Erstelle die Datensätze mit der kompletten Transformationskette\n",
    "        self.cifar10_train = datasets.CIFAR10(\n",
    "            self.data_dir, train=True, transform=self.transform\n",
    "        )\n",
    "        # Hinweis: Hier wird der Trainingsdatensatz auch als Validierungsdatensatz genutzt.\n",
    "        self.cifar10_val = datasets.CIFAR10(\n",
    "            self.data_dir, train=True, transform=self.transform\n",
    "        )\n",
    "        self.cifar10_test = datasets.CIFAR10(\n",
    "            self.data_dir, train=False, transform=self.transform\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.cifar10_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.cifar10_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.cifar10_test,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningModule: ResNet-Modell für CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLightning(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # Verwende ResNet18, angepasst für CIFAR-10\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        # Anpassung der ersten Convolution: CIFAR-10 Bilder haben 32x32 Pixel\n",
    "        self.model.conv1 = nn.Conv2d(\n",
    "            3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.model.maxpool = nn.Identity()  # Entferne die MaxPooling-Schicht\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.test_preds = []\n",
    "        self.test_targets = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Vorwärtsdurchlauf des Modells\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Trainingsschritt\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = accuracy(F.softmax(logits, dim=1), y, task=\"multiclass\", num_classes=10)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Validierungsschritt\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = accuracy(F.softmax(logits, dim=1), y, task=\"multiclass\", num_classes=10)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Testschritt\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_preds.append(preds.cpu())\n",
    "        self.test_targets.append(y.cpu())\n",
    "        acc = accuracy(F.softmax(logits, dim=1), y, task=\"multiclass\", num_classes=10)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Am Ende der Test-Epoche: Konfusionsmatrix und Klassifikationsreport anzeigen\n",
    "        preds = torch.cat(self.test_preds)\n",
    "        targets = torch.cat(self.test_targets)\n",
    "        cm = confusion_matrix(targets, preds)\n",
    "        print(\"Konfusionsmatrix:\")\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            xticklabels=classes,\n",
    "            yticklabels=classes,\n",
    "            cmap=\"Blues\",\n",
    "        )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nKlassifikationsreport:\")\n",
    "        print(classification_report(targets, preds, target_names=list(classes.values())))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Optimierer konfigurieren\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung der Modellarchitektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model):\n",
    "    dummy_input = torch.randn(1, 3, 32, 32)\n",
    "    out = model(dummy_input)\n",
    "    dot = make_dot(out, params=dict(model.named_parameters()))\n",
    "    dot.format = \"png\"\n",
    "    dot.render(\"resnet_architecture\")\n",
    "    img = plt.imread(\"resnet_architecture.png\")\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"ResNet Architektur\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darstellung der ersten 25 Testbilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_images(data_module):\n",
    "    test_loader = data_module.test_dataloader()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images[:25]\n",
    "    labels = labels[:25]\n",
    "\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    for img, label, ax in zip(images, labels, axes):\n",
    "        # Nutze hier die automatisch berechneten Mean/Std zur Unnormalisierung\n",
    "        img = unnormalize(img, data_module.mean, data_module.std)\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        ax.imshow(np.clip(img, 0, 1))\n",
    "        ax.set_title(classes[label.item()])\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darstellung der Softmax-Ausgaben für die ersten 6 Bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_softmax_outputs(model, data_module):\n",
    "    test_loader = data_module.test_dataloader()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images[:6]\n",
    "    labels = labels[:6]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(images)\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(6, 2, figsize=(20, 24))\n",
    "    for i in range(6):\n",
    "        # Linke Spalte: Bildanzeige\n",
    "        img_disp = (\n",
    "            unnormalize(images[i], data_module.mean, data_module.std)\n",
    "            .permute(1, 2, 0)\n",
    "            .numpy()\n",
    "        )\n",
    "        axes[i, 0].imshow(np.clip(img_disp, 0, 1))\n",
    "        axes[i, 0].set_title(f\"True: {classes[labels[i].item()]}\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "        # Rechte Spalte: Balkendiagramm der Softmax-Ausgabe\n",
    "        axes[i, 1].bar(range(len(probs[i])), probs[i])\n",
    "        axes[i, 1].set_yscale(\"log\")\n",
    "        axes[i, 1].set_xticks(range(len(probs[i])))\n",
    "        axes[i, 1].set_xticklabels(list(classes.values()), rotation=45)\n",
    "        axes[i, 1].set_title(\"Softmax-Ausgabe\")\n",
    "        axes[i, 1].set_xlabel(\"Klassen\")\n",
    "        axes[i, 1].set_ylabel(\"Wahrscheinlichkeit (log)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Evaluation und zusätzliche Visualisierungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLossHistory(pl.Callback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        loss = trainer.callback_metrics.get(\"train_loss\")\n",
    "        if loss is not None:\n",
    "            self.train_losses.append(loss.cpu().detach().item())\n",
    "\n",
    "\n",
    "print(\"Aufgabe 1: ResNet – Training und Evaluation\")\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# DataModule und Modell initialisieren\n",
    "# Hier wird die automatische Berechnung der Normalisierungswerte aktiviert.\n",
    "data_module = CIFAR10DataModule(auto_normalize=True)\n",
    "model = ResNetLightning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Visualisierung der Modellarchitektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Modellarchitektur\n",
    "visualize_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geräteauswahl: GPU, MPS (Apple) oder CPU\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = \"gpu\"\n",
    "    devices = 1\n",
    "    print(\"Verwende GPU für das Training.\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    accelerator = \"mps\"\n",
    "    devices = 1\n",
    "    print(\"Verwende MPS (Apple Silicon) für das Training.\")\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    devices = None\n",
    "    print(\"Verwende CPU für das Training.\")\n",
    "\n",
    "loss_history = TrainLossHistory()\n",
    "trainer = Trainer(\n",
    "    max_epochs=20, accelerator=accelerator, devices=devices, callbacks=[loss_history]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training des ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training des ResNet\n",
    "trainer.fit(model, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d-e) Konfusionsmatrix, Klassifikationsreport, Lernkurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: Testen, Konfusionsmatrix & Klassifikationsreport\n",
    "trainer.test(model, datamodule=data_module)\n",
    "\n",
    "# Plot Trainings-Losskurve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(loss_history.train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epoche\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Trainings-Losskurve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Darstellung der ersten 25 Testbilder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Darstellung der ersten 25 Testbilder\n",
    "plot_test_images(data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g) Softmax Ausgaben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Darstellung der Softmax-Ausgaben für die ersten 6 Bilder\n",
    "plot_softmax_outputs(model, data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Bearbeitung der Aufgabe 2 bietet eine hervorragende Gelegenheit, PyTorch Lightning praktisch auszuprobieren und ein tieferes Verständnis für dessen Funktionsweise zu entwickeln. Durch das Testen verschiedener Ansätze können wertvolle Erfahrungen gesammelt werden, die beim effizienten Einsatz dieser leistungsstarken Bibliothek helfen."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
