digraph {
	graph [size="59.25,59.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1848037083088 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	1848767569904 [label=AddmmBackward0]
	1848768075168 -> 1848767569904
	1848768013936 [label="model.fc.bias
 (10)" fillcolor=lightblue]
	1848768013936 -> 1848768075168
	1848768075168 [label=AccumulateGrad]
	1848768081360 -> 1848767569904
	1848768081360 [label=ViewBackward0]
	1848768075456 -> 1848768081360
	1848768075456 [label=MeanBackward1]
	1848768075504 -> 1848768075456
	1848768075504 [label=ReluBackward0]
	1847404714096 -> 1848768075504
	1847404714096 [label=AddBackward0]
	1848768075648 -> 1847404714096
	1848768075648 [label=NativeBatchNormBackward0]
	1848768075744 -> 1848768075648
	1848768075744 [label=ConvolutionBackward0]
	1848768075888 -> 1848768075744
	1848768075888 [label=ReluBackward0]
	1848768076272 -> 1848768075888
	1848768076272 [label=NativeBatchNormBackward0]
	1848768076368 -> 1848768076272
	1848768076368 [label=ConvolutionBackward0]
	1848768072816 -> 1848768076368
	1848768072816 [label=ReluBackward0]
	1848768076704 -> 1848768072816
	1848768076704 [label=AddBackward0]
	1848768076800 -> 1848768076704
	1848768076800 [label=NativeBatchNormBackward0]
	1848768076944 -> 1848768076800
	1848768076944 [label=ConvolutionBackward0]
	1848768077136 -> 1848768076944
	1848768077136 [label=ReluBackward0]
	1848768077280 -> 1848768077136
	1848768077280 [label=NativeBatchNormBackward0]
	1848768077376 -> 1848768077280
	1848768077376 [label=ConvolutionBackward0]
	1848768077568 -> 1848768077376
	1848768077568 [label=ReluBackward0]
	1848768077712 -> 1848768077568
	1848768077712 [label=AddBackward0]
	1848768077808 -> 1848768077712
	1848768077808 [label=NativeBatchNormBackward0]
	1848768077952 -> 1848768077808
	1848768077952 [label=ConvolutionBackward0]
	1848768078144 -> 1848768077952
	1848768078144 [label=ReluBackward0]
	1848768078288 -> 1848768078144
	1848768078288 [label=NativeBatchNormBackward0]
	1848768078384 -> 1848768078288
	1848768078384 [label=ConvolutionBackward0]
	1848768077856 -> 1848768078384
	1848768077856 [label=ReluBackward0]
	1848768078672 -> 1848768077856
	1848768078672 [label=AddBackward0]
	1848768078768 -> 1848768078672
	1848768078768 [label=NativeBatchNormBackward0]
	1848768078912 -> 1848768078768
	1848768078912 [label=ConvolutionBackward0]
	1848768079104 -> 1848768078912
	1848768079104 [label=ReluBackward0]
	1848768079248 -> 1848768079104
	1848768079248 [label=NativeBatchNormBackward0]
	1848768079344 -> 1848768079248
	1848768079344 [label=ConvolutionBackward0]
	1848768079584 -> 1848768079344
	1848768079584 [label=ReluBackward0]
	1848768079728 -> 1848768079584
	1848768079728 [label=AddBackward0]
	1848768079824 -> 1848768079728
	1848768079824 [label=NativeBatchNormBackward0]
	1848768080016 -> 1848768079824
	1848768080016 [label=ConvolutionBackward0]
	1848768080208 -> 1848768080016
	1848768080208 [label=ReluBackward0]
	1848768080352 -> 1848768080208
	1848768080352 [label=NativeBatchNormBackward0]
	1848768080448 -> 1848768080352
	1848768080448 [label=ConvolutionBackward0]
	1848768079920 -> 1848768080448
	1848768079920 [label=ReluBackward0]
	1848768080832 -> 1848768079920
	1848768080832 [label=AddBackward0]
	1848768080928 -> 1848768080832
	1848768080928 [label=NativeBatchNormBackward0]
	1848768081072 -> 1848768080928
	1848768081072 [label=ConvolutionBackward0]
	1848768081792 -> 1848768081072
	1848768081792 [label=ReluBackward0]
	1848768082080 -> 1848768081792
	1848768082080 [label=NativeBatchNormBackward0]
	1848768082176 -> 1848768082080
	1848768082176 [label=ConvolutionBackward0]
	1848768082368 -> 1848768082176
	1848768082368 [label=ReluBackward0]
	1848768082512 -> 1848768082368
	1848768082512 [label=AddBackward0]
	1848768084048 -> 1848768082512
	1848768084048 [label=NativeBatchNormBackward0]
	1848768084192 -> 1848768084048
	1848768084192 [label=ConvolutionBackward0]
	1848768084384 -> 1848768084192
	1848768084384 [label=ReluBackward0]
	1848768084528 -> 1848768084384
	1848768084528 [label=NativeBatchNormBackward0]
	1848768084624 -> 1848768084528
	1848768084624 [label=ConvolutionBackward0]
	1848768084096 -> 1848768084624
	1848768084096 [label=ReluBackward0]
	1848768084912 -> 1848768084096
	1848768084912 [label=AddBackward0]
	1848768085056 -> 1848768084912
	1848768085056 [label=NativeBatchNormBackward0]
	1848768085200 -> 1848768085056
	1848768085200 [label=ConvolutionBackward0]
	1848768085392 -> 1848768085200
	1848768085392 [label=ReluBackward0]
	1848768085536 -> 1848768085392
	1848768085536 [label=NativeBatchNormBackward0]
	1848768085680 -> 1848768085536
	1848768085680 [label=ConvolutionBackward0]
	1848768084768 -> 1848768085680
	1848768084768 [label=ReluBackward0]
	1848768085824 -> 1848768084768
	1848768085824 [label=NativeBatchNormBackward0]
	1848768725152 -> 1848768085824
	1848768725152 [label=ConvolutionBackward0]
	1848768725344 -> 1848768725152
	1848768016896 [label="model.conv1.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	1848768016896 -> 1848768725344
	1848768725344 [label=AccumulateGrad]
	1848768725056 -> 1848768085824
	1848037118576 [label="model.bn1.weight
 (64)" fillcolor=lightblue]
	1848037118576 -> 1848768725056
	1848768725056 [label=AccumulateGrad]
	1848768725200 -> 1848768085824
	1848037118496 [label="model.bn1.bias
 (64)" fillcolor=lightblue]
	1848037118496 -> 1848768725200
	1848768725200 [label=AccumulateGrad]
	1848768085872 -> 1848768085680
	1848037118096 [label="model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1848037118096 -> 1848768085872
	1848768085872 [label=AccumulateGrad]
	1848768085488 -> 1848768085536
	1848037118176 [label="model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1848037118176 -> 1848768085488
	1848768085488 [label=AccumulateGrad]
	1848768085728 -> 1848768085536
	1848037118016 [label="model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1848037118016 -> 1848768085728
	1848768085728 [label=AccumulateGrad]
	1848768085440 -> 1848768085200
	1848037117616 [label="model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1848037117616 -> 1848768085440
	1848768085440 [label=AccumulateGrad]
	1848768085248 -> 1848768085056
	1848037117696 [label="model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1848037117696 -> 1848768085248
	1848768085248 [label=AccumulateGrad]
	1848768085152 -> 1848768085056
	1848037117376 [label="model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1848037117376 -> 1848768085152
	1848768085152 [label=AccumulateGrad]
	1848768084768 -> 1848768084912
	1848768084816 -> 1848768084624
	1848037116976 [label="model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1848037116976 -> 1848768084816
	1848768084816 [label=AccumulateGrad]
	1848768084672 -> 1848768084528
	1848037117056 [label="model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1848037117056 -> 1848768084672
	1848768084672 [label=AccumulateGrad]
	1848768084480 -> 1848768084528
	1848037116736 [label="model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1848037116736 -> 1848768084480
	1848768084480 [label=AccumulateGrad]
	1848768084432 -> 1848768084192
	1848037116176 [label="model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1848037116176 -> 1848768084432
	1848768084432 [label=AccumulateGrad]
	1848768084240 -> 1848768084048
	1848037116256 [label="model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1848037116256 -> 1848768084240
	1848768084240 [label=AccumulateGrad]
	1848768084144 -> 1848768084048
	1848037116096 [label="model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1848037116096 -> 1848768084144
	1848768084144 [label=AccumulateGrad]
	1848768084096 -> 1848768082512
	1848768082416 -> 1848768082176
	1848037082848 [label="model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1848037082848 -> 1848768082416
	1848768082416 [label=AccumulateGrad]
	1848768082224 -> 1848768082080
	1848037082928 [label="model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1848037082928 -> 1848768082224
	1848768082224 [label=AccumulateGrad]
	1848768082032 -> 1848768082080
	1848037091168 [label="model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1848037091168 -> 1848768082032
	1848768082032 [label=AccumulateGrad]
	1848768081984 -> 1848768081072
	1848037082288 [label="model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1848037082288 -> 1848768081984
	1848768081984 [label=AccumulateGrad]
	1848768081120 -> 1848768080928
	1848037082608 [label="model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1848037082608 -> 1848768081120
	1848768081120 [label=AccumulateGrad]
	1848768081024 -> 1848768080928
	1848037082448 [label="model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1848037082448 -> 1848768081024
	1848768081024 [label=AccumulateGrad]
	1848768080976 -> 1848768080832
	1848768080976 [label=NativeBatchNormBackward0]
	1848768082128 -> 1848768080976
	1848768082128 [label=ConvolutionBackward0]
	1848768082368 -> 1848768082128
	1848768082320 -> 1848768082128
	1848037116816 [label="model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1848037116816 -> 1848768082320
	1848768082320 [label=AccumulateGrad]
	1848768081216 -> 1848768080976
	1848037121296 [label="model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1848037121296 -> 1848768081216
	1848768081216 [label=AccumulateGrad]
	1848768081168 -> 1848768080976
	1848037121216 [label="model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1848037121216 -> 1848768081168
	1848768081168 [label=AccumulateGrad]
	1848768080640 -> 1848768080448
	1848037082208 [label="model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1848037082208 -> 1848768080640
	1848768080640 [label=AccumulateGrad]
	1848768080496 -> 1848768080352
	1848037090768 [label="model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1848037090768 -> 1848768080496
	1848768080496 [label=AccumulateGrad]
	1848768080304 -> 1848768080352
	1848037082128 [label="model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1848037082128 -> 1848768080304
	1848768080304 [label=AccumulateGrad]
	1848768080256 -> 1848768080016
	1848037081968 [label="model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1848037081968 -> 1848768080256
	1848768080256 [label=AccumulateGrad]
	1848768080064 -> 1848768079824
	1848037082048 [label="model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1848037082048 -> 1848768080064
	1848768080064 [label=AccumulateGrad]
	1848768079968 -> 1848768079824
	1848037097088 [label="model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1848037097088 -> 1848768079968
	1848768079968 [label=AccumulateGrad]
	1848768079920 -> 1848768079728
	1848768079632 -> 1848768079344
	1848037090048 [label="model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1848037090048 -> 1848768079632
	1848768079632 [label=AccumulateGrad]
	1848768079392 -> 1848768079248
	1848037096528 [label="model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1848037096528 -> 1848768079392
	1848768079392 [label=AccumulateGrad]
	1848768079200 -> 1848768079248
	1848037089968 [label="model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1848037089968 -> 1848768079200
	1848768079200 [label=AccumulateGrad]
	1848768079152 -> 1848768078912
	1848037090128 [label="model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1848037090128 -> 1848768079152
	1848768079152 [label=AccumulateGrad]
	1848768078960 -> 1848768078768
	1848037090208 [label="model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1848037090208 -> 1848768078960
	1848768078960 [label=AccumulateGrad]
	1848768078864 -> 1848768078768
	1848037096448 [label="model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1848037096448 -> 1848768078864
	1848768078864 [label=AccumulateGrad]
	1848768078816 -> 1848768078672
	1848768078816 [label=NativeBatchNormBackward0]
	1848768079296 -> 1848768078816
	1848768079296 [label=ConvolutionBackward0]
	1848768079584 -> 1848768079296
	1848768079488 -> 1848768079296
	1848037081728 [label="model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1848037081728 -> 1848768079488
	1848768079488 [label=AccumulateGrad]
	1848768079056 -> 1848768078816
	1848037096928 [label="model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1848037096928 -> 1848768079056
	1848768079056 [label=AccumulateGrad]
	1848768079008 -> 1848768078816
	1848037096848 [label="model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1848037096848 -> 1848768079008
	1848768079008 [label=AccumulateGrad]
	1848768078624 -> 1848768078384
	1848037089728 [label="model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1848037089728 -> 1848768078624
	1848768078624 [label=AccumulateGrad]
	1848768078432 -> 1848768078288
	1848037096208 [label="model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1848037096208 -> 1848768078432
	1848768078432 [label=AccumulateGrad]
	1848768078240 -> 1848768078288
	1848037089648 [label="model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1848037089648 -> 1848768078240
	1848768078240 [label=AccumulateGrad]
	1848768078192 -> 1848768077952
	1848037089408 [label="model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1848037089408 -> 1848768078192
	1848768078192 [label=AccumulateGrad]
	1848768078000 -> 1848768077808
	1848037095888 [label="model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1848037095888 -> 1848768078000
	1848768078000 [label=AccumulateGrad]
	1848768077904 -> 1848768077808
	1848037089328 [label="model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1848037089328 -> 1848768077904
	1848768077904 [label=AccumulateGrad]
	1848768077856 -> 1848768077712
	1848768077616 -> 1848768077376
	1848037088688 [label="model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1848037088688 -> 1848768077616
	1848768077616 [label=AccumulateGrad]
	1848768077424 -> 1848768077280
	1848037088768 [label="model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1848037088768 -> 1848768077424
	1848768077424 [label=AccumulateGrad]
	1848768077232 -> 1848768077280
	1848037095168 [label="model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1848037095168 -> 1848768077232
	1848768077232 [label=AccumulateGrad]
	1848768077184 -> 1848768076944
	1848037088368 [label="model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1848037088368 -> 1848768077184
	1848768077184 [label=AccumulateGrad]
	1848768076992 -> 1848768076800
	1848037088448 [label="model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1848037088448 -> 1848768076992
	1848768076992 [label=AccumulateGrad]
	1848768076896 -> 1848768076800
	1848037094848 [label="model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1848037094848 -> 1848768076896
	1848768076896 [label=AccumulateGrad]
	1848768076848 -> 1848768076704
	1848768076848 [label=NativeBatchNormBackward0]
	1848768077328 -> 1848768076848
	1848768077328 [label=ConvolutionBackward0]
	1848768077568 -> 1848768077328
	1848768077520 -> 1848768077328
	1848037095568 [label="model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1848037095568 -> 1848768077520
	1848768077520 [label=AccumulateGrad]
	1848768077088 -> 1848768076848
	1848037089088 [label="model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1848037089088 -> 1848768077088
	1848768077088 [label=AccumulateGrad]
	1848768077040 -> 1848768076848
	1848037089008 [label="model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1848037089008 -> 1848768077040
	1848768077040 [label=AccumulateGrad]
	1848768076608 -> 1848768076368
	1848037088128 [label="model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1848037088128 -> 1848768076608
	1848768076608 [label=AccumulateGrad]
	1848768076416 -> 1848768076272
	1848037094608 [label="model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1848037094608 -> 1848768076416
	1848768076416 [label=AccumulateGrad]
	1848768075792 -> 1848768076272
	1848037088048 [label="model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1848037088048 -> 1848768075792
	1848768075792 [label=AccumulateGrad]
	1848768075840 -> 1848768075744
	1848037087648 [label="model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1848037087648 -> 1848768075840
	1848768075840 [label=AccumulateGrad]
	1848768076080 -> 1848768075648
	1848037094208 [label="model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1848037094208 -> 1848768076080
	1848768076080 [label=AccumulateGrad]
	1848768075696 -> 1848768075648
	1848037087568 [label="model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1848037087568 -> 1848768075696
	1848768075696 [label=AccumulateGrad]
	1848768072816 -> 1847404714096
	1848768072288 -> 1848767569904
	1848768072288 [label=TBackward0]
	1848768075552 -> 1848768072288
	1848768266400 [label="model.fc.weight
 (10, 512)" fillcolor=lightblue]
	1848768266400 -> 1848768075552
	1848768075552 [label=AccumulateGrad]
	1848767569904 -> 1848037083088
}
