{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN08/8.1-Was_CNN_lernen_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisierung: Was CNNs lernen (towards understandable AI)\n",
    "\n",
    "This notebook contains the code sample found in Chapter 5, Section 4 of [Deep Learning with Python] (https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff) and\n",
    "[Deep Learning mit Python](http://www.mitp.de/Alle-Buecher-7/Deep-Learning-mit-Python-und-Keras.html)\n",
    "\n",
    "\n",
    "\n",
    "Oft heisst es, Deep-Learning-Modelle seien »Blackboxes«, die Repräsentationen in einer Form erlernen, die schwer zu extrahieren und für Menschen kaum verständlich sind. \n",
    "- Für einige Arten von Deep-Learning-Modellen mag das teilweise stimmen, auf CNNs trifft es jedoch definitiv nicht zu. \n",
    "- Die von CNNs erlernten Repräsentationen sind in hohem Mass für Visualisierungen geeignet, und zwar vor allem deshalb, weil es sich um Repräsentationen visueller Konzepte handelt. Seit 2013 wurde eine Vielzahl von Verfahren zur Visualisierung und Interpretation dieser Repräsentationen entwickelt. Wir werden nicht alle diese Verfahren betrachten, aber die drei verständlichsten und nützlichsten erörtern:\n",
    "\n",
    "Abgesehen von der Befürchtung, dass böse künstliche Intelligenzen die Welt übernehmen werden, kann das Gebiet der künstlichen Intelligenz für Außenstehende entmutigend sein. Facebooks Direktor für künstliche Intelligenz, **Yann LeCun**, verwendet die Analogie, dass KI eine Blackbox mit einer Million Knöpfen ist; das Innenleben ist den meisten ein Rätsel. Aber jetzt haben wir einen Blick hinein geworfen.\n",
    "\n",
    "**Adam Harley**, Masterstudent an der Ryerson University, hat eine interaktive Visualisierung erstellt, die erklärt, wie ein neuronales Faltungsnetz, eine Art Programm für künstliche Intelligenz, das zur Analyse von Bildern verwendet wird, intern funktioniert.\n",
    "\n",
    "[Visualization of a CNN](http://www.cs.cmu.edu/~aharley/nn_vis/cnn/3d.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook zeigen wir drei Ansätze, um Einblicke in das Innenleben von Convolutional Neural Networks (CNNs) zu erhalten:\n",
    "\n",
    "1. **Visualisierung zwischenliegender Aktivierungen** – Wir betrachten, wie ein kleines CNN (hier zur Klassifikation von Katzen und Hunden) verschiedene Eingabemerkmale extrahiert.\n",
    "2. **Visualisierung der CNN-Filter** – Mittels Gradientenanstieg im Eingaberaum ermitteln wir, welche visuellen Muster einen bestimmten Filter maximal aktivieren.\n",
    "3. **Grad‑CAM** – Wir berechnen Heatmaps, die zeigen, welche Bereiche eines Bildes massgeblich zur Klassifikationsentscheidung beitragen.\n",
    "## 0. Setup und Versionsabfrage\n",
    "\n",
    "Hier laden wir zunächst die nötigen Bibliotheken, prüfen die Lightning-Version und importieren alle Module, die wir später brauchen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "print(\"Lightning Version:\", L.__version__)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.cm as cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualisierung zwischenliegender Aktivierungen\n",
    "In diesem Abschnitt wird gezeigt, wie die Aktivierungen (Outputs) der einzelnen Schichten eines CNNs visualisiert werden können, um zu verstehen, welche Merkmale das Modell in den verschiedenen Schichten extrahiert.\n",
    "### 1.1 Definition eines einfachen CNN als LightningModule\n",
    "\n",
    "Wir definieren ein kleines CNN, das Katzen von Hunden unterscheidet. Neben der Vorhersage gibt uns die `forward`-Methode auch Zugriff auf die Aktivierungen der einzelnen Layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModelWithActivations(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(CNNModelWithActivations, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv4 = torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc1 = torch.nn.Linear(128 * 7 * 7, 512)  # Adjusted for input size\n",
    "        self.fc2 = torch.nn.Linear(512, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x, return_activations=False):\n",
    "        activations = {}\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        if return_activations:\n",
    "            activations[\"conv1\"] = x\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        if return_activations:\n",
    "            activations[\"conv2\"] = x\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        if return_activations:\n",
    "            activations[\"conv3\"] = x\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        if return_activations:\n",
    "            activations[\"conv4\"] = x\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        if return_activations:\n",
    "            return x, activations\n",
    "        return x\n",
    "\n",
    "\n",
    "# Modell laden mit Gewichten laden welches in Lektion 6 Trainiert wurde für Hunde und Katzen\n",
    "model = CNNModelWithActivations()\n",
    "model.load_state_dict(torch.load(\"cats_and_dogs_small_2.pth\"))  # Pfad zu den Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanzierung und Wechsel in den Evaluationsmodus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModelWithActivations()\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Bildvorverarbeitung und Anzeige\n",
    "\n",
    "Wir laden ein Katzenbild und bereiten es so vor, dass es als Eingabe in unser Modell passt. Die Transformationen umfassen das Ändern der Größe auf 150x150 und die Umwandlung in einen Tensor (Skalierung der Pixelwerte auf [0,1]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitte passe den Pfad zu deinem Katzenbild an.\n",
    "cat_img_path = \"Bilder/cat.jpg\"\n",
    "img = Image.open(cat_img_path).convert(\"RGB\")\n",
    "\n",
    "# Transformation: Grösse 150x150, Umwandlung in Tensor\n",
    "transform = transforms.Compose([transforms.Resize((150, 150)), transforms.ToTensor()])\n",
    "img_tensor = transform(img).unsqueeze(0)  # Shape: (1, 3, 150, 150)\n",
    "\n",
    "# Anzeige des Originalbildes\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.title(\"Originalbild (Katze)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Abruf und Anzeige der Zwischenaktivierungen\n",
    "\n",
    "Wir lassen das Bild durch unser CNN laufen und speichern die Ausgaben der verschiedenen Layer. Anschliessend zeigen wir beispielhaft einzelne Kanäle (zum Beispiel Kanal 6 und 7) aus dem ersten Convolutional Layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits, activations = model(img_tensor, return_activations=True)\n",
    "print(\"Logits:\", logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeige einzelner Kanäle des ersten Convolutional Layers (conv1)\n",
    "act_conv1 = activations[\"conv1\"]  # Shape: (1, 32, 150, 150)\n",
    "\n",
    "plt.figure()\n",
    "plt.matshow(act_conv1[0, 6].cpu(), cmap=\"viridis\")\n",
    "plt.title(\"Aktivierung, conv1 Kanal 6\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.matshow(act_conv1[0, 7].cpu(), cmap=\"viridis\")\n",
    "plt.title(\"Aktivierung, conv1 Kanal 7\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeige aller Kanäle des ersten Convolutional Layers\n",
    "for k in range(0, act_conv1.shape[1], 10):\n",
    "    plt.figure()\n",
    "    plt.matshow(act_conv1[0, k].cpu(), cmap=\"viridis\")\n",
    "    plt.title(f\"conv1, Kanal {k}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Visualisierung als Raster\n",
    "\n",
    "Mit der Funktion `display_activation_grid` erstellen wir ein Raster, in dem alle Kanäle eines bestimmten Layers (2D-Aktivierungskarten) angeordnet werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_activation_grid(activation, layer_name, images_per_row=8):\n",
    "    # Erwartete Form: (1, num_channels, H, W)\n",
    "    activation = activation.cpu().numpy()[0]  # (num_channels, H, W)\n",
    "    n_features = activation.shape[0]\n",
    "    size = activation.shape[1]\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((n_cols * size, images_per_row * size))\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_index = col * images_per_row + row\n",
    "            if channel_index >= n_features:\n",
    "                break\n",
    "            channel_image = activation[channel_index]\n",
    "            # Postprocessing: Normalisierung der Aktivierungskarte\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std() + 1e-5\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
    "            display_grid[\n",
    "                col * size : (col + 1) * size, row * size : (row + 1) * size\n",
    "            ] = channel_image\n",
    "    scale = 1.0 / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Rasters für alle Layer des Modells\n",
    "for name, act in activations.items():\n",
    "    display_activation_grid(act, layer_name=name, images_per_row=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualisierung von CNN-Filtern per Gradientenanstieg\n",
    "\n",
    "Im folgenden Abschnitt nutzen wir einen vortrainierten VGG16 aus Torchvision. Ziel ist es, ein Bild zu generieren, das einen bestimmten Filter in einem gewählten Layer maximal aktiviert.\n",
    "\n",
    "Dazu initialisieren wir ein zufälliges Bild (weißes Rauschen) und passen es mittels Gradientenanstieg an, sodass der Mittelwert der Aktivierung des Ziel-Filters maximiert wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filter_pattern(\n",
    "    model,\n",
    "    layer,\n",
    "    filter_index,\n",
    "    iterations=50,\n",
    "    learning_rate=10,\n",
    "    img_size=200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Erzeugt ein Bild, das einen bestimmten Filter in `layer` maximal aktiviert.\n",
    "    \"\"\"\n",
    "    # Überprüfen, ob eine GPU verfügbar ist, und das entsprechende Gerät auswählen\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialisiere ein zufälliges Bild (Werte in [0,1])\n",
    "    input_img = torch.rand(1, 3, img_size, img_size, device=device, requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([input_img], lr=learning_rate)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Verwenden eines Forward-Hooks, um die Aktivierung des gewünschten Layers zu erhalten\n",
    "        activation = None\n",
    "\n",
    "        def hook_fn(module, inp, outp):\n",
    "            nonlocal activation\n",
    "            activation = outp\n",
    "\n",
    "        hook = layer.register_forward_hook(hook_fn)\n",
    "        model(input_img)\n",
    "        hook.remove()\n",
    "\n",
    "        # Verlustfunktion: Negative des Mittelwerts der Zielaktivierung (wir minimieren, um zu maximieren)\n",
    "        loss = -activation[0, filter_index].mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Optional: Fortschritt ausgeben\n",
    "        # print(f\"Iteration {i}: Loss = {-loss.item():.4f}\")\n",
    "\n",
    "    result = input_img.detach().squeeze().cpu()\n",
    "    # Postprocessing: Normalisierung und Transformation in einen uint8-Bildbereich\n",
    "    result = result - result.mean()\n",
    "    result = result / (result.std() + 1e-5)\n",
    "    result = result * 64 + 128\n",
    "    result = torch.clamp(result, 0, 255)\n",
    "    result = result.permute(1, 2, 0).numpy().astype(\"uint8\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Laden eines vortrainierten VGG16\n",
    "vgg16 = models.vgg16(weights=VGG16_Weights.DEFAULT).to(\"cpu\")\n",
    "vgg16.eval()\n",
    "# Wähle einen Conv-Layer aus dem Feature-Extractor. Hier verwenden wir beispielhaft features[10].\n",
    "target_layer = vgg16.features[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung des Filtermusters für einen Beispiel-Filter (hier: Filter 2)\n",
    "pattern = generate_filter_pattern(\n",
    "    vgg16,\n",
    "    target_layer,\n",
    "    filter_index=2,\n",
    "    iterations=50,\n",
    "    learning_rate=10,\n",
    "    img_size=200,\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(pattern)\n",
    "plt.title(\"Filtervisualisierung: Filter 2\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Erzeugen eines Rasters mit den ersten 64 Filtern\n",
    "\n",
    "Wir generieren die Filtermuster für die ersten 64 Filter und fügen diese in einem Raster zusammen, wobei zwischen den Bildern ein kleiner Rand (margin) eingehalten wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "for filter_index in range(64):\n",
    "    print(f\"Verarbeite Filter {filter_index}\")\n",
    "    img_pattern = generate_filter_pattern(\n",
    "        vgg16,\n",
    "        target_layer,\n",
    "        filter_index=filter_index,\n",
    "        iterations=50,\n",
    "        learning_rate=10,\n",
    "        img_size=200,\n",
    "    )\n",
    "    all_images.append(img_pattern)\n",
    "\n",
    "margin = 5\n",
    "n = 8\n",
    "img_h, img_w, _ = all_images[0].shape\n",
    "grid_h = n * img_h + (n - 1) * margin\n",
    "grid_w = n * img_w + (n - 1) * margin\n",
    "stitched_filters = np.zeros((grid_h, grid_w, 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        stitched_filters[\n",
    "            i * (img_h + margin) : i * (img_h + margin) + img_h,\n",
    "            j * (img_w + margin) : j * (img_w + margin) + img_w,\n",
    "        ] = all_images[i * n + j]\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(stitched_filters)\n",
    "plt.title(\"Raster der ersten 64 Filter\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern des Filter-Rasters als Bilddatei\n",
    "from PIL import Image as PILImage\n",
    "import requests\n",
    "\n",
    "pil_img = PILImage.fromarray(stitched_filters)\n",
    "pil_img.save(\"filters_for_layer.png\", dpi=(600, 600))\n",
    "print(\"Filter-Raster gespeichert als 'filters_for_layer.png'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualisierung der Heatmaps der Klassenaktivierung (Grad‑CAM)\n",
    "\n",
    "Im letzten Abschnitt demonstrieren wir Grad‑CAM. Dabei wird mithilfe von Hook-Funktionen der Einfluss einzelner Pixelbereiche\n",
    "auf die Klassifikation sichtbar gemacht. Wir berechnen die Gradienten im Ziel-Layer und gewichten die Aktivierungen, um eine Heatmap zu erhalten, die anschließend auf das Originalbild gelegt wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(model, img_tensor, target_class, target_layer):\n",
    "    \"\"\"\n",
    "    Berechnet die Grad‑CAM Heatmap für das gegebene Bild.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    activation = None\n",
    "    gradient = None\n",
    "\n",
    "    def forward_hook(module, inp, outp):\n",
    "        nonlocal activation\n",
    "        activation = outp.detach()\n",
    "\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        nonlocal gradient\n",
    "        gradient = grad_out[0].detach()\n",
    "\n",
    "    hook_forward = target_layer.register_forward_hook(forward_hook)\n",
    "    hook_backward = target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    output = model(img_tensor)\n",
    "    # Falls target_class als Index übergeben wird:\n",
    "    score = (\n",
    "        output[0, target_class]\n",
    "        if isinstance(target_class, int)\n",
    "        else output[0, output.argmax()]\n",
    "    )\n",
    "    model.zero_grad()\n",
    "    score.backward()\n",
    "\n",
    "    hook_forward.remove()\n",
    "    hook_backward.remove()\n",
    "\n",
    "    # Globales Durchschnittspooling der Gradienten über räumliche Dimensionen\n",
    "    weights = torch.mean(gradient, dim=(2, 3))  # (batch, channels)\n",
    "    cam = torch.zeros(activation.shape[2:], dtype=torch.float32, device=device)\n",
    "    for i, w in enumerate(weights[0]):\n",
    "        cam += w * activation[0, i, :, :]\n",
    "    cam = torch.relu(cam)\n",
    "    cam = cam - cam.min()\n",
    "    if cam.max() != 0:\n",
    "        cam = cam / cam.max()\n",
    "    return cam.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bildvorbereitung für Grad‑CAM\n",
    "\n",
    "Wir laden ein Elefantenbild, transformieren es auf 224×224 Pixel und normalisieren es gemäss den ImageNet-Standards. So passt es zum vortrainierten VGG16-Modell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitte passe den Pfad zu deinem Elefantenbild an.\n",
    "elephant_img_path = \"Bilder/elefant.jpg\"\n",
    "img_elephant = Image.open(elephant_img_path).convert(\"RGB\")\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "img_tensor_vgg = preprocess(img_elephant).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersage mit VGG16 und Bestimmung der Zielklasse\n",
    "device = next(vgg16.parameters()).device  # Get the device of the model\n",
    "img_tensor_vgg = img_tensor_vgg.to(\n",
    "    device\n",
    ")  # Move the input tensor to the same device as the model\n",
    "output = vgg16(img_tensor_vgg)\n",
    "preds = torch.nn.functional.softmax(output, dim=1)\n",
    "pred_class = preds.argmax(dim=1).item()\n",
    "\n",
    "# Herunterladen der ImageNet-Klassen\n",
    "imagenet_classes_url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "imagenet_classes = requests.get(imagenet_classes_url).json()\n",
    "\n",
    "# Ausgabe der Top-3-Klassen\n",
    "top3_probs, top3_indices = torch.topk(preds, 3)\n",
    "top3_probs = top3_probs[0].tolist()\n",
    "top3_indices = top3_indices[0].tolist()\n",
    "\n",
    "print(\"Top-3 Vorhersagen:\")\n",
    "for i, (prob, idx) in enumerate(zip(top3_probs, top3_indices)):\n",
    "    print(f\"{i + 1}. {imagenet_classes[idx]} ({prob * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Grad‑CAM Berechnung und Visualisierung\n",
    "\n",
    "Wir wählen als Ziel-Layer für Grad‑CAM den letzten Convolutional Layer im Feature-Extractor.\n",
    "Bei VGG16 eignet sich hierfür beispielsweise `features[28]`. Die berechnete Heatmap wird anschliessend auf das Originalbild gelegt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_cam = vgg16.features[28]\n",
    "cam = grad_cam(\n",
    "    vgg16, img_tensor_vgg, target_class=pred_class, target_layer=target_layer_cam\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "# Erzeugen der Heatmap und Überlagerung auf das Originalbild\n",
    "# Wir passen die Grösse der Heatmap an das Originalbild an, wandeln sie in einen uint8-Bereich um und verwenden dann die \"jet\"-Colormap.\n",
    "cam_resized = cv2.resize(cam, (img_elephant.size[0], img_elephant.size[1]))\n",
    "cam_resized = np.uint8(255 * cam_resized)\n",
    "\n",
    "jet = matplotlib.colormaps.get_cmap(\"jet\")\n",
    "jet_colors = jet(np.arange(256))[:, :3]\n",
    "jet_heatmap = jet_colors[cam_resized]\n",
    "jet_heatmap = np.uint8(jet_heatmap * 255)\n",
    "\n",
    "# Umwandlung des Originalbildes in ein Numpy-Array (RGB)\n",
    "img_np = np.array(img_elephant)\n",
    "# Überlagerung: 60% Originalbild, 40% Heatmap\n",
    "superimposed_img = cv2.addWeighted(img_np, 0.6, jet_heatmap, 0.4, 0)\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.imshow(superimposed_img)\n",
    "plt.title(\"Grad-CAM Heatmap auf Elefantenbild\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Speichern des Grad‑CAM Ergebnisses\n",
    "result_img = PILImage.fromarray(superimposed_img)\n",
    "result_img.save(\"elephant_gradcam.png\", dpi=(600, 600))\n",
    "print(\"Grad-CAM Bild gespeichert als 'elephant_gradcam.png'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
