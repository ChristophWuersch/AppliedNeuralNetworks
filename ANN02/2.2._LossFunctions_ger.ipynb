{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN02/2.2._LossFunctions_ger.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "%pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verlustfunktionen in Torch\n",
    "\n",
    "Dieses Notizbuch beschreibt und visualisiert einige der am häufigsten verwendeten Verlustfunktionen in PyTorch aus `torch.nn`. Weitere Informationen finden Sie in der [offiziellen PyTorch-Dokumentation zu Verlustfunktionen] (https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "Verlustfunktionen sind eine entscheidende Komponente beim Training neuronaler Netze, da jedes Modell für maschinelles Lernen eine Optimierung erfordert, die bei der Reduzierung des Verlustes und der Erstellung korrekter Vorhersagen hilft. Ohne Verlustfunktionen gibt es keine Möglichkeit, Ihr Modell so zu steuern, dass es korrekte Vorhersagen macht. Aber was genau sind Verlustfunktionen, und wie verwendet man sie? In diesem Notizbuch werden wir uns mit den verschiedenen Verlustfunktionen beschäftigen, die bei der Optimierung Ihrer Modelle verwendet werden können\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `nn.L1Loss`\n",
    "\n",
    "- **Beschreibung**: Berechnet den mittleren absoluten Fehler (MAE) zwischen Ziel- und vorhergesagten Werten.Die L1 -Verlustfunktion, die auch als Mittelwert für Absolute Fehler (MAE) bezeichnet wird, berechnet den Durchschnitt der Summe der absoluten Unterschiede zwischen den vorhergesagten und den tatsächlichen Werten.Es berechnet zunächst die absoluten Unterschiede zwischen den vorhergesagten und den tatsächlichen Werten und fasst dann alle Werte zusammen.Schließlich braucht es den Durchschnitt, um den Verlust zu berechnen. Der L1 -Verlust wird hauptsächlich für Regressionsprobleme verwendet und ist für Ausreißer robuster.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{L1}}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n",
    "$$\n",
    "\n",
    "Vorteil\n",
    "- MAE ist für Ausreißer robuster im Vergleich zu einem mittleren Quadratfehler (MSE), da es den absoluten Unterschied nimmt und die Auswirkungen extrem grosser Fehler verringert.\n",
    "- Der MAE -Verlust ist unkompliziert zu interpretieren, da er die durchschnittliche Grösse der Fehler darstellt und die Leistung des Modells den Stakeholdern leichter vermittelt.\n",
    "\n",
    "Nachteil\n",
    "- Mae behandelt alle Fehler gleichermassen, unabhängig von ihrer Grösse.Dies kann ein Nachteil sein, wenn die Unterscheidung zwischen kleinen und grossen Fehlern wichtig ist.\n",
    "- Der Gradient von MAE ist ein konstanter Wert, der die Konvergenz während der Optimierung verlangsamen kann, insbesondere im Vergleich zu MSE, wo der Gradient mit abnimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = torch.linspace(-1, 1, 100)\n",
    "y = torch.zeros_like(x)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "losses = [loss_fn(torch.tensor([xi]), torch.tensor([yi])) for xi, yi in zip(x, y)]\n",
    "\n",
    "plt.plot(x.numpy(), losses, label=\"L1Loss\")\n",
    "plt.title(\"L1Loss\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. `nn.MSELoss`\n",
    "\n",
    "- **Beschreibung**: Berechnet den mittleren quadratischen Fehler (MSE) zwischen Ziel- und vorhergesagten Werten. Der MSE, auch bekannt als L2-Verlust, ähnelt dem MAE-Verlust, aber anstatt die absoluten Differenzen wie bei MAE zu berechnen, berechnet L2 den Durchschnitt der quadrierten Differenzen zwischen den vorhergesagten und tatsächlichen Werten. Die Hauptidee hinter der Quadrierung besteht darin, das Modell für grosse Differenzen zu bestrafen, damit das Modell grössere Differenzen vermeidet. Diese Funktion wird auch für Regressionsprobleme verwendet, ist aber weniger robust als MAE. Sie ist die Standardverlustfunktion für die meisten Pytorch-Regressionsprobleme.\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{MSE}(y, \\hat{y}) =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Vorteile:\n",
    "- Geeignet für **Regressionsaufgaben**, bei denen das Endziel darin besteht, die quadrierten Differenzen zwischen vorhergesagten Werten und tatsächlichen Werten zu minimieren.\n",
    "- Differenzierbar und konvex.\n",
    "\n",
    "Benachteiligungen:\n",
    "- **Empfindlich gegenüber Ausreissern** aufgrund der Quadrierungsoperation, die die Ergebnisse im Optimierungsprozess verfälscht.\n",
    "- Nicht ideal für Aufgaben, bei denen Fehler unterschiedlich bestraft werden sollen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 100)\n",
    "y = torch.zeros_like(x)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "losses = [loss_fn(torch.tensor([xi]), torch.tensor([yi])) for xi, yi in zip(x, y)]\n",
    "\n",
    "plt.plot(x.numpy(), losses, label=\"MSELoss\")\n",
    "plt.title(\"MSELoss\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `nn.BCELoss`\n",
    "\n",
    "- **Beschreibung**: Binärer Cross-Entropy-Verlust für binäre Klassifikationsprobleme. Dieser Verlust ist eine spezielle Klasse des Cross-Entropie-Verlustes, der für binäre Klassifizierungsprobleme verwendet wird. Der Ausgang des neuronalen Netzes ist eine Sigmoid-Schicht, die sicherstellt, dass das Endergebnis entweder ein Wert nahe Null oder nahe Eins ist. Wie der Name schon sagt, wird es für binäre Klassifizierungsaufgaben verwendet.\n",
    "\n",
    "$$\n",
    "\\text{BCELoss}(y,\\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\big( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\big)\n",
    "$$\n",
    "\n",
    "Vorteile:\n",
    "- Es wird häufig bei **binären Klassifizierungsproblemen** verwendet. Es hilft auch bei der Handhabung unausgeglichener Datensätze.\n",
    "- Es ermutigt das Modell, hohe Wahrscheinlichkeiten für die richtige Klasse vorherzusagen.\n",
    "\n",
    "Nachteile:\n",
    "- Es kann einen verschwindenden Gradienten und eine langsame Konvergenz aufweisen, wenn die vorhergesagten Wahrscheinlichkeiten weit von den wahren Bezeichnungen entfernt sind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0.01, 0.99, 100)\n",
    "y = torch.ones_like(x)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "losses = [loss_fn(torch.tensor([xi]), torch.tensor([yi])) for xi, yi in zip(x, y)]\n",
    "\n",
    "plt.plot(x.numpy(), losses, label=\"BCELoss\")\n",
    "plt.title(\"BCELoss\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. `nn.CrossEntropyLoss`\n",
    "\n",
    "- **Beschreibung**: Kreuzentropieverlust für Mehrklassen-Klassifikationsprobleme. Kombiniert `nn.LogSoftmax` und `nn.NLLLoss` in einem. Die Cross-Entropie-Verlustfunktion wird bei Klassifizierungsproblemen verwendet. Diese Verlustfunktion berechnet die Unterschiede zwischen zwei Wahrscheinlichkeitsverteilungen für einen gegebenen Satz von Zufallsvariablen. Die Ausgabe des Netzes ist eine Softmax-Schicht, die sicherstellt, dass der endgültige Wahrscheinlichkeitswert im Bereich von 0 bis 1 bleibt. Sie wird bei binären Klassifizierungsaufgaben verwendet, für die sie die Standardverlustfunktion in Pytorch ist.\n",
    "\n",
    "$$\n",
    "\\text{CrossEntropyLoss}(y,\\hat{y}) = - \\sum_{i=1}^{C} y_i \\log \\left( \\frac{\\exp(\\hat{y}_i)}{\\sum_{j=1}^{C} \\exp(\\hat{y}_j)} \\right)\n",
    "$$\n",
    "\n",
    "Vorteile\n",
    "- Unveränderlich gegenüber Skalierung und Verschiebung der vorhergesagten Wahrscheinlichkeiten.\n",
    "- Liefert ein Maß für die Unsicherheit, das zu einem besseren Verständnis der Modellvorhersagen beiträgt.\n",
    "\n",
    "Nachteile\n",
    "- Empfindlich gegenüber Ausreißern und unausgewogenen Daten (kann zugunsten der Mehrheitsklasse verzerrt sein).\n",
    "- Es liefert keine Ähnlichkeit zwischen den Klassen, was in einigen Fällen erforderlich sein kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "y = torch.tensor([0])  # Class 0\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(x, y)\n",
    "print(\"CrossEntropyLoss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "### 5. `nn.HuberLoss`\n",
    "\n",
    "- **Beschreibung**: Kombiniert L1- und L2-Verluste und reduziert die Empfindlichkeit gegenüber Ausreißern. Dieser Verlust wird bei der Lösung von Regressionsproblemen verwendet, insbesondere bei Ausreißern. Er kombiniert sowohl MAE (mittlerer absoluter Fehler) als auch MSE (mittlerer quadratischer Fehler), und welcher Verlust verwendet wird, hängt vom Delta-Wert ab. Zunächst wird die Differenz zwischen den tatsächlichen und den vorhergesagten Werten berechnet. Ist diese Differenz kleiner als der Schwellenwert, d.h. Delta, verhält es sich wie MAE, andernfalls wechselt es zu MSE.\n",
    "\n",
    "$$\n",
    "\\text{HuberLoss}(y,\\hat{y}) = \\begin{cases}\n",
    "\\frac{1}{2}(\\hat{y}_i - y_i)^2, & \\text{if } |\\hat{y}_i - y_i| \\leq \\delta \\\\\n",
    "\\delta |\\hat{y}_i - y_i| - \\frac{1}{2} \\delta^2, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "MAE, MSE und Huber-Verluste werden bei Regressionsproblemen verwendet, aber welche sollten wir verwenden? MSE kann verwendet werden, wenn Sie größere Fehler stärker bestrafen wollen. Er ist nützlich, wenn die Daten keine signifikanten Ausreißer aufweisen und man davon ausgeht, dass die Fehler normalverteilt sind. MAE kann verwendet werden, wenn Sie eine robuste Verlustfunktion wünschen, die weniger von Ausreißern beeinflusst wird. Der Huber-Verlust kann verwendet werden, wenn man einen Kompromiss zwischen den Vorteilen von MAE und MSE finden möchte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-2, 2, 100)\n",
    "y = torch.zeros_like(x)\n",
    "loss_fn = torch.nn.HuberLoss(delta=1.0)\n",
    "losses = [loss_fn(torch.tensor([xi]), torch.tensor([yi])) for xi, yi in zip(x, y)]\n",
    "\n",
    "plt.plot(x.numpy(), losses, label=\"HuberLoss\")\n",
    "plt.title(\"HuberLoss\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "### 6. `nn.SoftMarginLoss`\n",
    "\n",
    "- **Description**: A loss function for binary classification that encourages the model to predict probabilities close to 0 or 1.\n",
    "\n",
    "$$\n",
    "\\text{SoftMarginLoss}(y,\\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\log\\big(1 + \\exp(-y_i \\cdot \\hat{y}_i)\\big)\n",
    "$$\n",
    "\n",
    "\n",
    "For further details, check the [official PyTorch documentation on loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-2, 4, 100)\n",
    "y = torch.ones_like(x)  # Binary target (1 for positive class)\n",
    "loss_fn = torch.nn.SoftMarginLoss()\n",
    "losses = [loss_fn(torch.tensor([xi]), torch.tensor([yi])) for xi, yi in zip(x, y)]\n",
    "\n",
    "plt.plot(x.numpy(), losses, label=\"SoftMarginLoss\")\n",
    "plt.title(\"SoftMarginLoss\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7. `nn.KLDivLoss`\n",
    "\n",
    "- **Beschreibung**: Der Kullback-Leibler-Divergenzverlust misst den Unterschied zwischen zwei Wahrscheinlichkeitsverteilungen. Der Kullback-Leibler-Divergenzverlust berechnet die Differenz zwischen zwei Wahrscheinlichkeitsverteilungen. Sie gibt an, wie groß die Divergenz ist, wenn die vorhergesagte Wahrscheinlichkeitsverteilung anstelle der Zielwahrscheinlichkeitsverteilung verwendet wird. Ist die Divergenz größer, ist auch die Verlustfunktion größer. Wenn die KL-Divergenz null ist, bedeutet dies, dass die beiden Wahrscheinlichkeitsverteilungen gleich sind. Sie wird zur Annäherung komplexer Funktionen, bei Klassifizierungsaufgaben mit mehreren Klassen und wenn Sie sicherstellen wollen, dass die Verteilung der Vorhersagen der der Trainingsdaten ähnelt, verwendet.\n",
    "\n",
    "$$\n",
    "\\text{KLDivLoss}(P, Q) = \\sum_{i} P(x_i) \\cdot \\left( \\log(P(x_i)) - \\log(Q(x_i)) \\right)\n",
    "$$\n",
    "\n",
    "Vorteile:\n",
    "- Sie misst die Differenz zwischen zwei Wahrscheinlichkeitsverteilungen und wird häufig für Aufgaben wie Variational Autoencoders (VAEs) verwendet.\n",
    "- Sie wird auch beim Training generativer Modelle und beim Abgleich der jeweiligen vorhergesagten Verteilung mit der wahren Verteilung verwendet.\n",
    "\n",
    "Nachteilig:\n",
    "- Sie ist nicht symmetrisch und eignet sich nicht als eigenständige Verlustfunktion beim Training von Klassifikatoren.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0.1, 0.2, 0.7])\n",
    "y = torch.tensor([0.1, 0.3, 0.6])\n",
    "loss_fn = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "loss = loss_fn(x.log(), y)\n",
    "print(\"KLDivLoss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 8. `nn.BCEWithLogitsLoss`\n",
    "\n",
    "- **Description**: Combines a Sigmoid layer and the Binary Cross-Entropy loss in one. This is numerically more stable than using a plain Sigmoid followed by `nn.BCELoss`.\n",
    "\n",
    "$$\n",
    "\\text{BCEWithLogitsLoss}(y,\\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\big( y_i \\cdot \\log(\\sigma(\\hat{y}_i)) + (1 - y_i) \\cdot \\log(1 - \\sigma(\\hat{y}_i)) \\big)\n",
    "$$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$ is the Sigmoid function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-2, 2, 100)\n",
    "y = torch.ones_like(x)  # Binary target (1 for positive class)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "losses = [loss_fn(torch.tensor([xi]), torch.tensor([yi])) for xi, yi in zip(x, y)]\n",
    "\n",
    "plt.plot(x.numpy(), losses, label=\"BCEWithLogitsLoss\")\n",
    "plt.title(\"BCEWithLogitsLoss\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`BCEWithLogitsLoss` kombiniert zwei Vorgänge:\n",
    "1. **Sigmoide Aktivierungsfunktion**: Konvertiert Rohwerte (Logits) in Wahrscheinlichkeiten im Bereich $[0, 1]$.\n",
    "2. **Binärer Kreuz-Entropie-Verlust**: Misst die Diskrepanz zwischen den vorhergesagten Wahrscheinlichkeiten und den tatsächlichen binären Bezeichnungen.\n",
    "\n",
    "Anstatt die Sigmoid-Funktion separat anzuwenden und anschliessend den `BCELoss` zu berechnen, wird bei `BCEWithLogitsLoss` die Sigmoid-Berechnung direkt in die Verlustfunktion integriert. Diese kombinierte Operation ist numerisch stabiler, insbesondere bei extremen Logits-Werten (sehr grosse positive oder negative Werte), wodurch das Risiko eines Über- oder Unterlaufs bei der in der Sigmoid-Funktion verwendeten Exponentialfunktion verringert wird.\n",
    "\n",
    "- **Numerische Stabilität**: Durch die Kombination der Sigmoid- und BCE-Operationen in einer einzigen, vermeidet die Implementierung die direkte Berechnung von $ \\sigma(x) $ (Sigmoid) und nutzt stattdessen mathematische Vereinfachungen, um die Stabilität zu erhalten.\n",
    "- **Effizienz**: Die Durchführung beider Operationen in einem Schritt ist rechnerisch effizient.\n",
    "- **Anwendbarkeit**: Es wird häufig bei binären Klassifizierungsaufgaben und bei Aufgaben mit Wahrscheinlichkeiten verwendet, bei denen die Ziele entweder 0 oder 1 sind.\n",
    "- **Binäre Klassifizierungsaufgaben**: Vorhersage eines einzigen binären Ergebnisses (z. B. wahr/falsch, ja/nein).\n",
    "- **Multilabel-Klassifikation**: Wenn jede Stichprobe zu mehreren unabhängigen Klassen gehören kann (verwenden Sie `BCEWithLogitsLoss` mit kodierten One-Hot- oder Multi-Hot-Zielmarken).\n",
    "\n",
    "\n",
    "Der binäre Cross-Entropy Loss wird als **negative Log-Likelihood** einer Bernoulli-Verteilung abgeleitet. Für ein binäres Klassifikationsproblem:\n",
    "\n",
    "1. **Bernoulli-Likelihood-Funktion**:\n",
    "   \n",
    "   Die Bernoulli-Verteilung modelliert die Wahrscheinlichkeit eines binären Ergebnisses ($y \\in \\{0, 1\\}$):\n",
    "   $$\n",
    "   P(y \\mid x) = \\sigma(x)^y \\cdot (1 - \\sigma(x))^{1-y}\n",
    "   $$\n",
    "   Dabei ist $ \\sigma(x) $ die Sigmoid-Funktion:\n",
    "   $$\n",
    "   \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n",
    "   $$\n",
    "\n",
    "2. **Log-Likelihood**:\n",
    "   Nehmen Sie den Logarithmus der Wahrscheinlichkeit für einen einzelnen Datenpunkt:\n",
    "   $$\n",
    "   \\log P(y \\mid x) = y \\cdot \\log(\\sigma(x)) + (1 - y) \\cdot \\log(1 - \\sigma(x))\n",
    "   $$\n",
    "\n",
    "3. **Negative Log-Likelihood (Verlust)**:\n",
    "   Die negative Log-Likelihood für einen einzelnen Datenpunkt wird:\n",
    "   $$\n",
    "   \\mathcal{L}(x, y) = -\\left(y \\cdot \\log(\\sigma(x)) + (1 - y) \\cdot \\log(1 - \\sigma(x))\\right)\n",
    "   $$\n",
    "\n",
    "4. **Direktes Ersetzen von $ \\sigma(x) $**:\n",
    "   Wenn wir anstelle von Wahrscheinlichkeiten Logits ($z = x$) eingeben:\n",
    "   $$\n",
    "   \\mathcal{L}(z, y) = \\max(z, 0) - z \\cdot y + \\log(1 + \\exp(-|z|))\n",
    "   $$\n",
    "   Diese Formulierung vermeidet die direkte Berechnung der Sigmoid-Funktion, die für grosse Werte von $z$ instabil sein kann.\n",
    "\n",
    "\n",
    "\n",
    "**Vorteile der Verwendung von Logits**\n",
    "- **Numerische Stabilität**: Die direkte Berechnung von Sigmoid-Werten für grosse Logits kann zu einem Über- oder Unterlauf führen. Die alternative Formulierung oben entschärft dies durch die Verwendung von $\\log(1 + \\exp(-|z|))$, die für grosse positive oder negative Werte von $z$ stabil ist.\n",
    "- **Interpretierbarkeit**: Die Verwendung von Logits ermöglicht eine klare Interpretation der Modellergebnisse als Rohwerte, die später bei Bedarf in Wahrscheinlichkeiten umgewandelt werden können.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "de",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "de",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
