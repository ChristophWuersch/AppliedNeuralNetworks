{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN02/2.2-Klassifizierung_Movie-Reviews_PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "%pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "import pickle\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print versions in a compact form\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Filmkritiken klassifizieren: ein binäres Klassifizierungsbeispiel\n",
    "\n",
    "Dieses Notebook enthält die Codebeispiele aus Kapitel 3, Abschnitt 5 von [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python).\n",
    "\n",
    "Die Zweiklassenklassifikation oder binäre Klassifikation ist möglicherweise die am weitesten verbreitete Art des maschinellen Lernens. In diesem Beispiel werden wir lernen, basierend auf dem Textinhalt der Rezensionen Filmkritiken in \"positive\" und \"negative\" Rezensionen zu klassifizieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Der IMDB-Datensatz\n",
    "\n",
    "\n",
    "Wir werden mit dem \"IMDB-Datensatz\" arbeiten, einem Satz von 50.000 hochpolarisierenden Rezensionen aus der Internet Movie Database. Sie sind aufgeteilt in 25.000 Rezensionen für das Training und 25.000 Rezensionen für das Testen, wobei jedes Set aus 50% negativen und 50% positiven Rezensionen besteht.\n",
    "\n",
    "Warum haben wir diese zwei getrennten Trainings- und Test-Sets? \n",
    "- Sie sollten ein Modell für maschinelles Lernen **niemals mit denselben Daten testen, mit denen Sie es trainiert haben!** \n",
    "- Nur weil ein Modell bei seinen Trainingsdaten gut abschneidet, bedeutet das nicht, dass es auch bei Daten gut abschneidet, die es noch nie gesehen hat.\n",
    "- Was und was ins wirklich interessiert, ist die Leistung unseres Modells bei neuen Daten (da Sie die Labels Ihrer Trainingsdaten bereits kennen - offensichtlich brauchen Sie Ihr Modell nicht, um diese vorherzusagen). \n",
    "- Es ist z. B. möglich, dass Ihr Modell am Ende nur ein Mapping zwischen Ihren Trainingsproben und deren Zielen speichert - was für die Aufgabe der Vorhersage von Zielen für nie zuvor gesehene Daten völlig nutzlos wäre. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Genau wie der MNIST-Datensatz wird auch der IMDB-Datensatz mit `tensorflow.keras` ausgeliefert. Er wurde bereits vorverarbeitet: Die Bewertungen (Wortfolgen) wurden in Folgen von Ganzzahlen umgewandelt, wobei jede Ganzzahl für ein bestimmtes Wort in einem Wörterbuch steht.\n",
    "\n",
    "Der folgende Code lädt den Datensatz (wenn Sie ihn zum ersten Mal ausführen, werden etwa 80 MB an Daten auf Ihren Rechner heruntergeladen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dateien laden aus Github\n",
    "!wget -O imdb_dataset.pkl \"https://github.com/ChristophWuersch/AppliedNeuralNetworks/raw/refs/heads/main/ANN02/Daten/imdb_dataset.pkl\"\n",
    "!wget -O imdb_word_index.pkl \"https://github.com/ChristophWuersch/AppliedNeuralNetworks/raw/refs/heads/main/ANN02/Daten/imdb_word_index.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the IMDB dataset\n",
    "\n",
    "# Load the dataset from the pickle file\n",
    "with open(\"imdb_dataset.pkl\", \"rb\") as dataset_file:\n",
    "    imdb_data = pickle.load(dataset_file)\n",
    "\n",
    "# Load the word index from the pickle file\n",
    "with open(\"imdb_word_index.pkl\", \"rb\") as word_index_file:\n",
    "    word_index = pickle.load(word_index_file)\n",
    "\n",
    "# Access the dataset\n",
    "train_data = imdb_data[\"train_data\"]\n",
    "train_labels = imdb_data[\"train_labels\"]\n",
    "test_data = imdb_data[\"test_data\"]\n",
    "test_labels = imdb_data[\"test_labels\"]\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training samples and {len(test_data)} test samples.\")\n",
    "print(f\"Loaded word index with {len(word_index)} words.\")\n",
    "\n",
    "print(\"IMDB dataset and word index saved to pickle files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) We reverse it, mapping integer indices to words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# (c) We decode the review; note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_review = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Das Argument `num_words=10000` bedeutet, dass wir nur die 10.000 am häufigsten vorkommenden Wörter in den Trainingsdaten behalten werden. Seltene Wörter \n",
    "werden verworfen. Dies erlaubt uns, mit Vektordaten von überschaubarer Größe zu arbeiten.\n",
    "\n",
    "Die Variablen `train_data` und `test_data` sind Listen von Bewertungen, wobei jede Bewertung eine Liste von Wortindizes ist (die eine Folge von Wörtern kodieren). \n",
    "Die Variablen `train_labels` und `test_labels` sind Listen von 0en und 1en, wobei 0 für \"negativ\" und 1 für \"positiv\" steht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_data[1][:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_labels[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Da wir uns auf die 10.000 häufigsten Wörter beschränkt haben, wird kein Wortindex 10.000 überschreiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "max([max(sequence) for sequence in train_data])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Zum Spass können wir eine dieser Rezensionen schnell in englische Wörter zurückdekodieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# (b) We reverse it, mapping integer indices to words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# (c) We decode the review; note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_review = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- (a) `word_index`  ist ein Dictionary, das den Wörtern einen Integerindex zuordnet.\n",
    "- (b) Kehrt die Zuordnung um und ordnet Wortindizes Wörter zu.\n",
    "- (c) Decodiert die Bewertung. Beachten Sie, dass die Indizes um drei Stellen verschoben sind, weil die Indizes 0, 1 und 2 für die Markierungen \"padding\", \"start of sequence\" und \"unknown\" reserviert sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "decoded_review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vorbereiten der Daten\n",
    "\n",
    "\n",
    "Wir können keine Listen mit ganzen Zahlen in ein neuronales Netzwerk einspeisen. Wir müssen unsere Listen in Tensoren umwandeln. Es gibt zwei Möglichkeiten, dies zu tun:\n",
    "\n",
    "* Wir könnten unsere Listen so auffüllen, dass sie alle die gleiche Länge haben, und sie in einen Integer-Tensor der Form `(samples, word_indices)` umwandeln, und dann als erste Schicht in unserem Netzwerk eine Schicht verwenden, die mit solchen ganzzahligen Tensoren umgehen kann (die Schicht `Embedding`, werden wir später ausführlich behandeln werden). \n",
    "\n",
    "* Wir könnten unsere Listen in einem Schritt kodieren, um sie in Vektoren von 0en und 1en zu verwandeln. Konkret würde das zum Beispiel bedeuten, dass wir die Sequenz `[3, 5]` in einen 10.000-dimensionalen Vektor zu verwandeln, der bis auf die Indizes 3 und 5, die Einsen wären, aus lauter Nullen bestehen würde. Dann könnten wir als Schicht in unserem Netzwerk eine `Dense`-Schicht verwenden, die in der Lage ist, Fliesskomma-Vektordaten zu verarbeiten.\n",
    "\n",
    "Wir werden uns für die letztere Lösung entscheiden. Lassen Sie uns unsere Daten vektorisieren, was wir aus Gründen der Übersichtlichkeit manuell tun werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.0  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So sehen unsere Stichproben jetzt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_train[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wir sollten auch unsere Beschriftungen vektorisieren, was ganz einfach ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to tensors\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Combine into datasets\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Jetzt sind unsere Daten bereit, in ein neuronales Netzwerk eingespeist zu werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aufbau unseres Netzwerks\n",
    "\n",
    "\n",
    "Unsere Eingabedaten sind einfach Vektoren, und unsere Beschriftungen sind Skalare (1en und 0en): Das ist die einfachste Konfiguration, die Sie je finden werden. Ein Typ von \n",
    "Netzes, das bei einem solchen Problem gut funktioniert, wäre ein einfacher Stapel voll verbundener (`Dense`) Schichten mit `relu`-Aktivierungen: `Dense(16, activation='relu')`\n",
    "\n",
    "Das Argument, das an jede `Dense`-Schicht übergeben wird (16), ist die Anzahl der \"versteckten Einheiten\" der Schicht. Was ist eine versteckte Einheit? Es ist eine Dimension \n",
    "im Darstellungsraum der Schicht. Sie erinnern sich vielleicht aus dem vorigen Kapitel, dass jede solche `Dense`-Schicht mit einer `relu`-Aktivierung folgende Kette von Tensoroperationen implementiert \n",
    "die folgende Kette von Tensoroperationen implementiert:\n",
    "\n",
    "`Output = relu(dot(W, input) + b)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 16 versteckte Einheiten (hiddne units) zu haben bedeutet, dass die Gewichtsmatrix `W` die Form `(input_dimension, 16)` haben wird, d.h. das Punktprodukt mit `W` projiziert die Eingabedaten auf einen 16-dimensionalen Darstellungsraum projizieren (und dann würden wir den Bias-Vektor `b` hinzufügen und die Operation `relu` anwenden). \n",
    "- Sie können Sie können die Dimensionalität Ihres Repräsentationsraums intuitiv als \"wie viel Freiheit Sie dem Netzwerk beim Lernen interner Repräsentationen einräumen\" verstehen. interne Repräsentationen zu lernen\". Wenn Sie mehr versteckte Einheiten (einen höherdimensionalen Repräsentationsraum) haben, kann Ihr Netz komplexere Repräsentationen lernen, aber es macht Ihr Netz rechenintensiver und kann zum Erlernen unerwünschter Muster führen (Muster, die die Leistung bei den Trainingsdaten verbessern, aber nicht bei den Testdaten).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es gibt zwei wichtige Architektur-Entscheidungen, die für einen solchen Stapel von dichten Schichten getroffen werden müssen:\n",
    "\n",
    "* Wie viele Schichten verwendet werden sollen.\n",
    "* Wie viele \"versteckte Einheiten\" für jede Schicht gewählt werden sollen.\n",
    "\n",
    "Bezüglich der Architektur eines solchen Stapels von Dense-Layern müssen zwei wichtige **Designentscheidungen** getroffen werden:\n",
    "- wie viele Layer verwendet werden und\n",
    "- wie viele verdeckte Einheiten jeder Layer besitzt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In den folgenden Lektionen werden  Sie die formalen Regeln kennenlernen, nach denen Sie sich bei diesen Entscheidungen richten können. Fürs Erste müssen Sie darauf vertrauen, dass die folgende Wahl die richtige ist:\n",
    "- zwei zwischenliegende Layer mit jeweils 16 verdeckten Einheiten\n",
    "- ein dritter Layer zur Ausgabe der skalaren Vorhersage der aktuellen Filmbewertung\n",
    "\n",
    "Die zwischenliegenden Layer verwenden als Aktivierungsfunktion `relu`, und der letzte Layer nutzt zur Aktivierung eine **Sigmoidfunktion**, um eine Wahrscheinlichkeit ausgeben zu können (einen Wert zwischen 0 und 1, der angibt, wie wahrscheinlich\n",
    "es ist, dass das fragliche Sample den Zielwert 1 besitzt, also wie hoch die Wahrscheinlichkeit ist, dass die Bewertung positiv ist). Die relu-Funktion (Rectified Linear Units, rektifizierte Lineareinheiten) sorgt dafür, dass negative Werte auf null gesetzt werden (siehe Abbildung 3.4). Die Sigmoidfunktion »quetscht« beliebige Werte in das Intervall `[0, 1]` (siehe Abbildung), damit die Ausgabe als Wahrscheinlichkeit interpretiert werden kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Was sind Aktivierungsfunktionen, und wofür werden sie benötigt?\n",
    "\n",
    "Ohne eine Aktivierungsfunktion wie relu (die auch als Nichtlinearität bezeichnet wird) würde der Dense-Layer aus zwei linearen Operationen bestehen – dem Tensorprodukt und einer Addition:\n",
    "\n",
    "`output = dot(W, input) + b`\n",
    "\n",
    "- Der Layer könnte in diesem Fall nur lineare Transformationen (affine Abbildungen) der Daten erlernen: Der *Hypothesenraum* des Layers wäre die *Menge aller möglichen linearen Transformationen* der Eingabedaten in einen 16-dimensionalen Raum. Ein solcher Hypothesenraum ist zu beschränkt und würde nicht von mehreren Repräsentations-Layern profitieren, denn auch ein großer Stapel linearer Layer stellt noch immer eine lineare Operation dar: Das Hinzufügen weiterer Layer würde den Hypothesenraum nicht vergrössern.\n",
    "\n",
    "- Um Zugang zu einem sehr viel umfassenderen Hypothesenraum zu erlangen, der von vielschichtigen Repräsentationen profitieren kann, ist eine **Nichtlinearität bzw. eine Aktivierungsfunktion** erforderlich. Beim Deep Learning ist `relu` die\n",
    "verbreitetste Aktivierungsfunktion, es gibt jedoch noch viele weitere, die ähnlich seltsame Bezeichnungen besitzen, wie `prelu`, `elu` usw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's what our network looks like:\n",
    "\n",
    "![3-layer network](https://s3.amazonaws.com/book.keras.io/img/ch3/3_layer_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Und hier ist die Keras-Implementierung, dem MNIST-Beispiel, das Sie zuvor gesehen haben, sehr ähnlich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10000, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.output = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "# Instantiate the model and move it to the device\n",
    "model = SentimentModel().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the model\n",
    "summary(\n",
    "    model, input_size=(1, 10000)\n",
    ")  # Adjust input_size as per your model's requirement\n",
    "\n",
    "# Visualize the model architecture\n",
    "x = torch.randn(1, 10000)  # Adjust input size as per your model's requirement\n",
    "y = model(x)\n",
    "\n",
    "\n",
    "# Summarize the model\n",
    "summary(\n",
    "    model, input_size=(1, 10000)\n",
    ")  # Adjust input_size as per your model's requirement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Abschliessend müssen Sie noch eine **Verlustfunktion** und einen **Optimierer** auswählen.\n",
    "\n",
    "- Da wir es hier mit einer Binärklassifizierung zu tun haben und die Ausgabe des NNs eine Wahrscheinlichkeit ist (der letzte Layer des NNs besitzt nur eine verdeckte Einheit und wird durch eine Sigmoidfunktion aktiviert), ist es am besten, die **binäre Kreuzentropie** (`binary_crossentropy`) als Verlustfunktion zu verwenden. Dabei handelt es sich jedoch nicht um die einzige brauchbare Lösung.\n",
    "- Sie könnten beispielsweise auch den mittleren quadratischen Fehler (`mean_squared_error`) benutzen. Für Modelle, die Wahrscheinlichkeiten ausgeben, ist die Kreuzentropie jedoch für gewöhnlich die beste Wahl. Die Kreuzentropie ist eine Größe, die einem Teilgebiet der Informationstheorie entstammt, das die Differenz zwischen Wahrscheinlichkeitsverteilungen bemisst oder, wie im vorliegenden Fall, zwischen der beobachteten Wahrscheinlichkeitsverteilung und der Vorhersage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kreuzentropie $H(p,q)$\n",
    "\n",
    "Die Kreuzentropie der Verteilung $q$ relativ zu einer Verteilung $p$ über eine gegebene Menge ist wie folgt definiert:\n",
    "\n",
    "$$ H(p,q)=-\\mathbb{E}_{p}[\\log q]$$\n",
    "\n",
    "Bei **Klassifizierungsproblemen** wollen wir die Wahrscheinlichkeit verschiedener Ergebnisse schätzen. Wenn die geschätzte Wahrscheinlichkeit $q_i(x)$ des Ergebnisses $i$ ist, während die Häufigkeit (empirische Wahrscheinlichkeit) des Ergebnisses $i$ in der Trainingsmenge $p_{i}$ ist, und es $N$ bedingt unabhängige Stichproben in der Trainingsmenge gibt, dann ist die Wahrscheinlichkeit des Modells auf der Trainingsmenge:\n",
    "\n",
    "$$\\prod_i q_i(x)^{N p_i}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "so wird die **log-likelihood** (dividiert durch $N$) gerade die Kreuzentropie,\n",
    "\n",
    "$$ \\frac{1}{N}\\log \\prod_{i}q_{i}^{Np_{i}}=\\sum_{i}p_{i}\\log q_{i}(x)=-H(p,q)$$\n",
    "\n",
    "sodass die Maximierung der Wahrscheinlichkeit (likelihood) dasselbe ist wie die Minimierung der Kreuzentropie.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J(\\mathbf{w}) &=\\frac{1}{N} \\sum_{n=1}^{N} H(p_{n},q_{n}) \n",
    "= -\\frac{1}{N} \\sum_{n=1}^{N} \\bigg\\lbrace y_{n} \\log \\left[\\hat{y}_{n}(x)\\right]+(1-y_{n}) \\log \\left[ 1-\\hat{y}_{n}(x)\\right] \\bigg\\rbrace\\,\n",
    "\\end{aligned}\n",
    "$$\n",
    "                                                                                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "# Metric tracking\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir übergeben unseren Optimierer, die Verlustfunktion und die Metriken als Strings, was möglich ist, weil `rmsprop`, `binary_crossentropy` und `accuracy` als Teil von Keras mitgeliefert werden. \n",
    "\n",
    "Gelegentlich möchte man die Parameter des Optimierers konfigurieren oder eine benutzerdefinierte Verlustfunktion\n",
    "bzw. Kennzahl übergeben. Ersteres kann man durch Übergabe einer Instanz einer Optimierer-Klasse als optimizer-Argument erreichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validierung des Ansatzes\n",
    "\n",
    "Um während des Trainings die Korrektklassifizierungsrate des Modells für unbekannte Daten zu überwachen, halten wir 10.000 Samples der ursprünglichen Trainingsdaten zurück, die dann als Validierungsmenge verwendet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nun wird das Modell 20 Epochen lang mit kleinen Stapeln trainiert, die jeweils 512 Samples enthalten (20 Durchläufe sämtlicher Samples in den Tensoren `x_train` und `y_train`). Gleichzeitig überwachen wir den Wert der Verlustfunktion und die Korrektklassifizierungsrate für die zurückgehaltenen 10.000 Samples. Zu diesem Zweck übergeben wir als validation_data-Argument die Validierungsdatenmenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    y_pred = (y_pred > 0.5).float()  # Convert probabilities to binary predictions\n",
    "    return (y_pred == y_true).float().mean()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x_batch).squeeze()\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += loss.item() * x_batch.size(0)\n",
    "        train_acc += calculate_accuracy(y_pred, y_batch).item() * x_batch.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(x_batch).squeeze()\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item() * x_batch.size(0)\n",
    "            val_acc += calculate_accuracy(y_pred, y_batch).item() * x_batch.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc /= len(val_loader.dataset)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "\n",
    "    # Print epoch results\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{epochs} - \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Auf einer CPU dauert die Berechnung weniger als 2 Sekunden pro Epoche – das Training ist nach 20 Sekunden erledigt. Am Ende jeder Epoche gibt es eine kurze Verzögerung, weil das Modell den Wert der Verlustfunktion und die Korrektklassifizierungsrate\n",
    "für die 10.000 Samples der Validierungsdatenmenge berechnet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Das Dictionary enthält vier Einträge: einen für jede Kennzahl, die während des Trainings und der Validierung überwacht wurde. Die beiden folgenden Listings zeigen, wie man mit `Matplotlib` den Wert der Verlustfunktion bzw. der\n",
    "Korrektklassifizierungsrate beim Training und bei der Validierung ausgeben und vergleichen kann. Ihre Ergebnisse können aufgrund einer anderen zufälligen Initialisierung des NNs ein wenig davon abweichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, epochs + 1), train_loss_history, label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(range(1, epochs + 1), val_loss_history, label=\"Validation Loss\", marker=\"o\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, epochs + 1), train_acc_history, label=\"Training Accuracy\", marker=\"o\")\n",
    "plt.plot(range(1, epochs + 1), val_acc_history, label=\"Validation Accuracy\", marker=\"o\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell wird auf dem Testdatensatz bewertet, um die endgültige Leistung zu messen. Der Testverlust und die Testgenauigkeit werden berechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_pred = model(x_batch).squeeze()\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        test_loss += loss.item() * x_batch.size(0)\n",
    "        test_acc += calculate_accuracy(y_pred, y_batch).item() * x_batch.size(0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_acc /= len(test_loader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das trainierte Modell wird als Datei gespeichert, um es später laden und wiederverwenden zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = \"sentiment_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias-Variance Tradeoff\n",
    "Wie Sie sehen, sinkt der Wert der Verlustfunktion beim Training mit jeder Epoche, während die Korrektklassifizierungsrate zunimmt. Das ist bei einer Optimierung durch ein Gradientenabstiegsverfahren auch zu erwarten – die Grösse, die man zu\n",
    "minimieren versucht, sollte bei jedem Durchlauf kleiner werden. \n",
    "\n",
    "- Der Wert der Verlustfunktion und die Korrektklassifizierungsrate bei der Validierung zeigen dieses Verhalten allerdings nicht: Sie erreichen offenbar in der vierten Epoche die besten Werte. \n",
    "- Hierbei handelt es sich um ein Beispiel für das, vor dem ich Sie bereits gewarnt habe: Ein Modell, das mit den Trainingsdaten eine bessere Leistung erzielt, funktioniert nicht unbedingt auch mit unbekannten Daten besser.\n",
    "Genauer gesagt, liegt hier eine **Überanpassung (overfitting, high variance)** vor: Nach der zweiten Epoche wird\n",
    "das Modell zu stark an die Trainingsdaten angepasst und erlernt Repräsentationen, die den Trainingsdaten zu eigen sind, sich aber nicht auf unbekannte Daten verallgemeinern lassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Regularisierung durch \"early stopping\"**: In diesem Fall könnten Sie das Training nach drei Epochen abbrechen, um eine Überanpassung zu verhindern. Sie können im Allgemeinen eine Reihe verschiedener Verfahren zum Abschwächen der Überanpassung einsetzen, auf die wir in\n",
    "Kapitel 4 näher eingehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Early stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training routine with early stopping\n",
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    patience=5,\n",
    "    max_epochs=50,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(x_batch).squeeze()\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{max_epochs} - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Check early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses, val_losses = train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    patience=5,\n",
    "    max_epochs=50,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dieser ziemlich naive Ansatz erreicht eine Korrektklassifizierungsrate von 88%.\n",
    "Mit Ansätzen, die dem Stand der Technik entsprechen, lassen sich aber auch\n",
    "Werte von bis zu 95% erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Einführung in PyTorch Lightning\n",
    "\n",
    "PyTorch Lightning ist ein High-Level-Framework, das auf PyTorch aufbaut. Es vereinfacht die Strukturierung von Deep-Learning-Projekten, indem es Boilerplate-Code für Training, Validierung und Logging eliminiert. Mit Lightning kann man sich auf das Wesentliche konzentrieren: die Modelllogik und die Forschung.\n",
    "\n",
    "### **Vorteile von PyTorch Lightning**:\n",
    "- **Modularer Aufbau:** Lightning-Module kapseln Modelle, Optimierer und Trainingslogik.\n",
    "- **Automatisches Management:** Training, Validierung und Testläufe werden automatisch durch den `Trainer` verwaltet.\n",
    "- **Callbacks:** Unterstützt nützliche Callbacks wie Early Stopping und Checkpointing.\n",
    "- **Hardware-Unterstützung:** Läuft problemlos auf CPUs, GPUs und TPUs mit minimalem Aufwand.\n",
    "\n",
    "\n",
    "\n",
    "###  **Überblick über den Code**\n",
    "\n",
    "### Modelldefinition\n",
    "Das Modell wird in einer Lightning-Klasse definiert, die von `pl.LightningModule` erbt. Diese Klasse kapselt:\n",
    "- **Forward-Methode:** Definiert, wie die Eingaben durch das Modell fließen.\n",
    "- **Training Step:** Berechnet den Verlust während des Trainings.\n",
    "- **Validation Step:** Berechnet den Verlust für die Validierungsdaten.\n",
    "- **Optimizers:** Gibt den Optimierer zurück, der während des Trainings verwendet wird.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example usage with PyTorch Lightning Trainer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SentimentModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10000, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.output = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # Initialize metric storage\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return {\"loss\": loss, \"train_acc\": acc}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Access logged training metrics\n",
    "        self.train_losses.append(self.trainer.callback_metrics[\"train_loss\"].item())\n",
    "        self.train_accuracies.append(self.trainer.callback_metrics[\"train_acc\"].item())\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Access logged validation metrics\n",
    "        self.val_losses.append(self.trainer.callback_metrics[\"val_loss\"].item())\n",
    "        self.val_accuracies.append(self.trainer.callback_metrics[\"val_acc\"].item())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=True, mode=\"min\")\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    callbacks=[early_stopping],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "# Instantiate the model\n",
    "model = SentimentModel()\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(0, len(model.train_losses) + 1)\n",
    "epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics after training\n",
    "def plot_metrics_from_model(model):\n",
    "    epochs_train = np.arange(1, len(model.train_losses) + 1)\n",
    "    epochs_val = np.arange(1, len(model.val_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_train, model.train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs_val, model.val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_train, model.train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(epochs_val, model.val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_from_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weitere Experimente (Aufgaben)\n",
    "\n",
    "Die folgenden Experimente werden Sie davon überzeugen, dass die für die Architektur\n",
    "getroffenen Designentscheidungen durchaus vernünftig waren, wenngleich\n",
    "es noch Verbesserungsmöglichkeiten gibt:\n",
    "- Sie haben zwei verdeckte Layer verwendet. Probieren Sie aus, nur einen oder drei zu benutzen, und überprüfen Sie, wie sich das auf die Korrektklassifizierungsrate der Validierungs- bzw. der Testdatenmenge auswirkt.\n",
    "- Probieren Sie aus, weniger oder weitere verdeckte Einheiten zu verwenden: `8, 32, 64` usw.\n",
    "- Probieren Sie aus, statt der binären Kreuzentropie (binary_crossentropy) den mittleren quadratischen Fehler (Mean Squared Error, mse) zu verwenden. \n",
    "- Probieren Sie aus, statt der `relu`-Funktion tanh (Tangens hyperbolicus) als Aktivierungsfunktion zu verwenden (diese Aktivierungsfunktion war in den Anfangstagen der NNs verbreitet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Nehmen Sie Folgendes aus diesem Abschnitt mit:\n",
    "\n",
    "1. Für gewöhnlich ist mit der Vorverarbeitung der Rohdaten durchaus etwas Aufwand verbunden, damit sie – als Tensoren – in ein NN eingespeist werden können. Sequenzen von Wörtern können als binäre Vektoren codiert werden, aber meistens gibt es noch weitere Möglichkeiten, sie zu codieren.\n",
    "2. Mit Stapeln (batches) von `Dense`-Layern mit `relu`-Aktivierungsfunktion lässt sich eine Vielzahl von Aufgaben lösen (z.B. Stimmungsanalysen = sentiment analysis). Sie werden sie häufig nutzen.\n",
    "3. Bei **binären Klassifizierungsaufgaben** (zwei Klassen als mögliche Ausgabe) sollte das NN mit einem `Dense`-Layer mit einer verdeckten Einheit und **sigmoid-Aktivierung** enden. Die Ausgabe sollte ein Skalar zwischen `0` und `1` sein, der eine Wahrscheinlichkeit angibt.\n",
    "4. Bei einer solchen sigmoiden Ausgabe einer binären Klassifizierungsaufgabe sollten Sie als **Verlustfunktion** die **binäre Kreuzentropie** (`binary_crossentropy`) verwenden.\n",
    "5. Der `rmsprop`-**Optimierer** ist im Allgemeinen, unabhängig von der gegebenen Aufgabe, eine gute Wahl. Eine Sache weniger, um die Sie sich kümmern müssen.\n",
    "6. Wenn die Leistung eines NNs bei den Trainingsdaten zunimmt, kommt es irgendwann zu einer **Überanpassung**, die zu immer schlechteren Ergebnissen bei unbekannten Daten führt. Bei Daten, die nicht zur Trainingsmenge gehören, sollten Sie stets die Leistung überwachen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "de",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "de",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
