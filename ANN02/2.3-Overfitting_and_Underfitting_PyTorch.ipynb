{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN02/2.3-Overfitting_and_Underfitting_PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "%pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import Image, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# Print versions in a compact form\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bias-Varianz Trade-Off\n",
    "\n",
    "Bei allen Beispielen, die wir im vorigen Kapitel gesehen haben - Vorhersage der Stimmung in Filmrezensionen, Themenklassifizierung und Regression der Hauspreise - konnten wir feststellen, dass die Leistung unseres Modells auf den ausgeklammerten Validierungsdaten immer nach ein paar Epochen ihren Höhepunkt erreichte und dann begann \n",
    "verschlechtern würde, d. h. unser Modell würde schnell beginnen, sich an die Trainingsdaten _überanzupassen_. \n",
    "\n",
    "- Überanpassung (**overfitting**) kommt bei jedem einzelnen Problem des maschinellen Lernens Problem. \n",
    "- Um das maschinelle Lernen zu beherrschen, muss man lernen, mit der Überanpassung umzugehen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Das grundlegende Problem beim maschinellen Lernen ist der **Trade-off (Kompromiss) zwischen Optimierung und Verallgemeinerung**.\n",
    "\n",
    "- *Optimierung* bezieht sich auf den Prozess der Anpassung eines Modells, um die bestmögliche Leistung aus den Trainingsdaten zu erzielen (das \"Lernen\" in \"maschinelles Lernen\"), während sich *Generalisierung* darauf bezieht, wie gut das trainierte Modell \n",
    "auf Daten funktioniert, die es noch nie zuvor gesehen hat. Das Ziel des Maschinellen Lernens ist es natürlich, eine gute Generalisierung zu erreichen. \n",
    "\n",
    "\n",
    "Aber Sie haben keinen Einfluss auf die Generalisierung; \n",
    "Sie können das Modell nur auf der Grundlage seiner Trainingsdaten anpassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lernkurven\n",
    "\n",
    "- Zu Beginn des Trainings sind Optimierung und Generalisierung korreliert: Je geringer der Verlust (**loss**) bei den Trainingsdaten ist, desto geringer ist der Verlust bei den Testdaten. \n",
    "- Während dies geschieht, wird Ihr Modell als _unzureichend angepasst_ bezeichnet (high bias): Es müssen noch Fortschritte gemacht werden; das Netz hat noch nicht alle relevanten Muster in den Trainingsdaten modelliert. \n",
    "- Nach einer bestimmten Anzahl von Iterationen (Epochen) mit den Trainingsdaten verbessert sich die Generalisierung nicht mehr, und die Validierungsmetriken beginnen sich zu verschlechtern: Das Modell beginnt dann, sich zu sehr anzupassen, d. h. es beginnt, Muster zu lernen die für die Trainingsdaten spezifisch sind, die aber irreführend oder irrelevant sind, wenn es um neue Daten geht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/BiasVarianceTradeOff.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Um zu verhindern, dass ein Modell irreführende oder irrelevante Muster lernt, die in den Trainingsdaten gefunden wurden, _ist die beste Lösung , dass man mehr Trainingsdaten_ zur Verfügung stellt. Ein Modell, das mit mehr Daten trainiert wurde, kann i.A. besser verallgemeinern. \n",
    "- Wenn dies nicht mehr möglich ist, besteht die nächstbeste Lösung darin, die Menge der Informationen, die Ihr Modell speichern darf, zu modifizieren oder die Informationen, die es speichern darf, einzuschränken. \n",
    "\n",
    "Wenn ein Netz es sich nur leisten kann, eine kleine Anzahl von Mustern zu speichern, wird es durch den Optimierungsprozess gezwungen, sich auf die Muster zu konzentrieren, die eine bessere Chance auf eine gute Generalisierung haben.\n",
    "\n",
    "Der Prozess der Bekämpfung der Überanpassung auf diese Weise wird _Regelmäßigkeit_ genannt. Schauen wir uns einige der gängigsten Regularisierungs und wenden wir sie in der Praxis an, um unser Filmklassifizierungsmodell aus dem vorherigen Kapitel zu verbessern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_words = 10000\n",
    "\n",
    "\n",
    "# Load the dataset from the pickle file\n",
    "with open(\"./Daten/imdb_dataset.pkl\", \"rb\") as dataset_file:\n",
    "    imdb_data = pickle.load(dataset_file)\n",
    "\n",
    "# Load the word index from the pickle file\n",
    "with open(\"./Daten/imdb_word_index.pkl\", \"rb\") as word_index_file:\n",
    "    word_index = pickle.load(word_index_file)\n",
    "\n",
    "# Access the dataset\n",
    "train_data = imdb_data[\"train_data\"]\n",
    "train_labels = imdb_data[\"train_labels\"]\n",
    "test_data = imdb_data[\"test_data\"]\n",
    "test_labels = imdb_data[\"test_labels\"]\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training samples and {len(test_data)} test samples.\")\n",
    "print(f\"Loaded word index with {len(word_index)} words.\")\n",
    "\n",
    "print(\"IMDB dataset and word index saved to pickle files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hinweis: In diesem Notizbuch werden wir die IMDB-Testmenge als Validierungsmenge verwenden. Das spielt in diesem Zusammenhang keine Rolle.\n",
    "\n",
    "Bereiten wir die Daten mit dem Code aus Kapitel 3, Abschnitt 5 vor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.0  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)\n",
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to tensors\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Combine into datasets\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with PyTorch Lightning Trainer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SentimentModel(pl.LightningModule):\n",
    "    def __init__(self, n_hidden=16):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10000, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output = nn.Linear(n_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # Initialize metric storage\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(acc.item())\n",
    "        return {\"loss\": loss, \"train_acc\": acc}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_accuracies.append(acc.item())\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=True, mode=\"min\")\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "max_epochs = 30\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # callbacks=[early_stopping],\n",
    "    log_every_n_steps=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = SentimentModel(n_hidden=16)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot learning curves\n",
    "def plot_learning_curves(model):\n",
    "    epochs_train = (\n",
    "        np.array(range(1, len(model.train_losses) + 1))\n",
    "        / len(model.train_losses)\n",
    "        * max_epochs\n",
    "    )\n",
    "    epochs_val = (\n",
    "        np.array(range(1, len(model.val_losses) + 1))\n",
    "        / len(model.val_losses)\n",
    "        * max_epochs\n",
    "    )\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_train, model.train_losses, \"b.-\", label=\"Training Loss\")\n",
    "    plt.plot(epochs_val, model.val_losses, \"r.-\", label=\"Validation Loss\")\n",
    "    plt.title(\"Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_train, model.train_accuracies, \"b.-\", label=\"Training Accuracy\")\n",
    "    plt.plot(epochs_val, model.val_accuracies, \"r.-\", label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy Over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fighting overfitting\n",
    "\n",
    "## Verkleinerung des Netzwerks\n",
    "\n",
    "\n",
    "- Die einfachste Möglichkeit, Overfitting zu verhindern, besteht darin, die Größe des Modells zu reduzieren, d. h. die Anzahl der lernbaren Parameter im Modell (die durch die Anzahl der Schichten und die Anzahl der Einheiten pro Schicht bestimmt wird). \n",
    "- Beim Deep Learning wird die Anzahl der lernbaren Parameter in einem Modell oft als **Kapazität** des Modells bezeichnet. Intuitiv betrachtet hat ein Modell mit mehr Parametern eine größere \"Modellierungskapazität (höhere Dimension des Hypothesenraums\" und kann daher in der Lage, eine perfekte wörterbuchähnliche Abbildung zwischen den Trainingsproben und ihren Zielen zu erlernen, eine Abbildung ohne jegliche  Generalisierungskraft. \n",
    "- Ein Modell mit 500.000 binären Parametern könnte zum Beispiel leicht die Klasse aller Ziffern in der MNIST-Trainingsmenge zu erlernen: Für jede der 50.000 Ziffern würden nur 10 binäre Parameter benötigt. Ein solches Modell wäre unbrauchbar für die Klassifizierung neuen Ziffernproben. \n",
    "\n",
    "**Denken Sie immer daran: Deep-Learning-Modelle sind in der Regel gut darin, sich an die Trainingsdaten anzupassen, aber die eigentliche Herausforderung ist die Verallgemeinerung, nicht die Anpassung.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wenn das Netzwerk andererseits nur über begrenzte Speicherressourcen verfügt, kann es diese Zuordnung nicht so leicht erlernen, so dass es zur Minimierung seiner Verluste \n",
    "Um den Verlust zu minimieren, muss es komprimierte Repräsentationen lernen, die eine Vorhersagekraft in Bezug auf die Ziele haben -- genau die Art von Repräsentationen, an denen wir interessiert sind. Gleichzeitig sollten Sie darauf achten, dass Sie Modelle verwenden, die über genügend Parameter verfügen, um nicht unterdurchschnittlich gut zu passen: Ihr Modell sollte nicht unter dem Mangel an Erinnerungsressourcen leiden. Es muss ein Kompromiss zwischen \"zu viel Kapazität\" und \"nicht genug Kapazität\" gefunden werden.\n",
    "\n",
    "Leider gibt es keine magische Formel, um die richtige Anzahl von Schichten oder die richtige Grösse der einzelnen Schichten zu bestimmen. Sie müssen eine Reihe verschiedener Architekturen evaluieren (natürlich auf Ihrem Validierungssatz, nicht auf Ihrem Testsatz), um die richtige richtige Modellgrösse für Ihre Daten zu finden. \n",
    "- Der allgemeine Arbeitsablauf zur Ermittlung einer geeigneten Modellgrösse besteht darin, mit relativ wenigen Schichten und Parametern zu beginnen und dann die Grösse der Schichten zu erhöhen oder neue Schichten hinzuzufügen, bis Sie einen abnehmenden Ertrag in Bezug auf den Validierungsverlust.\n",
    "\n",
    "Versuchen wir dies an unserem Netzwerk zur Klassifizierung von Filmrezensionen. Unser ursprüngliches Netzwerk sah wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "small_model = SentimentModel(n_hidden=3)\n",
    "\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # callbacks=[early_stopping],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(small_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(small_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nun wollen wir versuchen, es durch dieses kleinere Netz zu ersetzen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Wie Sie sehen können, beginnt das kleinere Netz später mit der Überanpassung als das Referenznetz (nach 6 Epochen statt nach 4) und seine Leistung verschlechtert sich viel langsamer, sobald es mit der Überanpassung beginnt.\n",
    "\n",
    "Fügen wir nun zur Abwechslung zu diesem Benchmark ein Netz hinzu, das viel mehr Kapazität hat, weit mehr, als es das Problem rechtfertigen würde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "bigger_model = SentimentModel(n_hidden=512)\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # callbacks=[early_stopping],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(bigger_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(bigger_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hier sehen Sie, wie das grössere Netz im Vergleich zum Referenznetz abschneidet. Die Punkte sind die Validierungsverlustwerte des grösseren Netzes, und die Kreuze sind das ursprüngliche Netz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Das grössere Netz beginnt fast sofort, nach nur einer Epoche, mit der Überanpassung, und die Überanpassung ist viel stärker. Sein Validierungsverlust ist auch stärker verrauscht.\n",
    "\n",
    "In der Zwischenzeit sind hier die Trainingsverluste für unsere beiden Netzwerke:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wie Sie sehen können, geht der Trainingsverlust bei einem größeren Netz sehr schnell gegen Null. Je mehr Kapazität das Netz hat, desto schneller ist es in der Lage \n",
    "es die Trainingsdaten modellieren kann (was zu einem niedrigen Trainingsverlust führt), aber desto anfälliger ist es für eine Überanpassung (was zu einer grossen Unterschied zwischen dem Trainings- und dem Validierungsverlust)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hinzufügen einer Gewichtsregulierung\n",
    "\n",
    "Vielleicht kennen Sie das Prinzip von _Occam's Razor_: \n",
    "**Wenn es zwei Erklärungen für etwas gibt, ist die Erklärung, die am wahrscheinlichsten richtig ist, die die \"einfachste\", diejenige, die die wenigsten Annahmen enthält.**\n",
    "\n",
    "Dies gilt auch für die Modelle, die von neuronalen Netzen gelernt werden: Bei einigen Trainingsdaten und einer Netzwerkarchitektur gibt es mehrere Sätze von Gewichtungswerten (mehrere _Modelle_), die die Daten erklären könnten, und \n",
    "bei einfacheren Modellen ist die Wahrscheinlichkeit einer Überanpassung geringer als bei komplexen Modellen.\n",
    "\n",
    "- Ein \"einfaches Modell\" ist in diesem Zusammenhang ein Modell, bei dem die Verteilung der Parameterwerte eine geringere Entropie aufweist (oder ein Modell mit weniger Parametern insgesamt, wie wir im obigen Abschnitt gesehen haben). \n",
    "- Eine gängige Methode zur **Abschwächung der Überanpassung ist daher die Beschränkung der Komplexität eines Netzes**, indem man seine Gewichte zwingt, nur kleine Werte anzunehmen, wodurch die Verteilung der Gewichtungswerte \"regelmässiger\" wird. \n",
    "- Dies wird als Dies wird als *Gewichtsregulierung* bezeichnet und geschieht, indem zur Verlustfunktion des Netzes ein **Strafterm für Komplexität** hinzugefügt wird, der mit großen Gewichten verbunden ist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Diese Kosten gibt es in zwei Varianten:\n",
    "\n",
    "- **L1-Regularisierung**, bei der die hinzugefügten Kosten proportional zum _absoluten Wert der Gewichtskoeffizienten_ sind (d.h. zu dem, was man die \"L1-Norm\" der Gewichte).\n",
    "- **L2-Regularisierung**, bei der die zusätzlichen Kosten proportional zum _Quadrat des Wertes der Gewichtungskoeffizienten_ sind (d.h. zu dem, was als der \"L2-Norm\" der Gewichte). \n",
    "\n",
    "Die L2-Regularisierung wird im Zusammenhang mit neuronalen Netzen auch als _Gewichtsabbau_ bezeichnet. Lassen Sie sich nicht durch den anderen  Namen nicht verwirren: Gewichtsabnahme ist mathematisch gesehen genau dasselbe wie L2-Regularisierung.\n",
    "\n",
    "In Keras wird die Gewichtsregularisierung hinzugefügt, indem _Gewichtsregularisierungsinstanzen_ als Schlüsselwortargumente an Schichten übergeben werden. Fügen wir L2-Gewichtsregulierung \n",
    "Regularisierung zu unserem Klassifizierungsnetzwerk für Filmrezensionen hinzu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with PyTorch Lightning Trainer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SentimentModel(pl.LightningModule):\n",
    "    def __init__(self, n_hidden=16, l2_lambda=0.001):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10000, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output = nn.Linear(n_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # L2 regularization (weight_decay)\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        # Initialize metric storage\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(acc.item())\n",
    "        return {\"loss\": loss, \"train_acc\": acc}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_accuracies.append(acc.item())\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(), lr=0.001, weight_decay=self.l2_lambda\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "regularized_model = SentimentModel(n_hidden=512, l2_lambda=0.001)\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # callbacks=[early_stopping],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(\n",
    "    regularized_model, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`l2(0.001)` means that every coefficient in the weight matrix of the layer will add `0.001 * weight_coefficient_value` to the total loss of \n",
    "the network. Note that because this penalty is _only added at training time_, the loss for this network will be much higher at training \n",
    "than at test time.\n",
    "\n",
    "Here's the impact of our L2 regularization penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(regularized_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Wie Sie sehen können, ist das Modell mit L2-Regularisierung (Punkte) viel resistenter gegen Overfitting als das Referenzmodell (Kreuze), obwohl beide Modelle die gleiche Anzahl von Parametern haben.\n",
    "\n",
    "Als Alternative zur L2-Regularisierung können wir eine Lasso- oder L1-Regularisierung verwenden. Diese wird einige Gewichte auf Null setzen und somit eine kompakes Netzwerk forcieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example usage with PyTorch Lightning Trainer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SentimentModel(pl.LightningModule):\n",
    "    def __init__(self, n_hidden=16, l1_lambda=0.001, l2_lambda=0.0):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10000, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output = nn.Linear(n_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # L2 regularization (weight_decay)\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.l1_lambda = l1_lambda\n",
    "\n",
    "        # Initialize metric storage\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def compute_l1_loss(self):\n",
    "        l1_loss = 0\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                l1_loss += torch.sum(torch.abs(param))\n",
    "        return self.l1_lambda * l1_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        BCE_loss = self.criterion(y_hat, y)\n",
    "        # Add L1 regularization term\n",
    "        l1_loss = self.compute_l1_loss()\n",
    "        loss = BCE_loss + l1_loss\n",
    "\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(acc.item())\n",
    "        return {\"loss\": loss, \"train_acc\": acc}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_accuracies.append(acc.item())\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(), lr=0.001, weight_decay=self.l2_lambda\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "regularized_model = SentimentModel(n_hidden=512, l1_lambda=1e-5, l2_lambda=0.0)\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "max_epochs = 30\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # callbacks=[early_stopping],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(\n",
    "    regularized_model, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(regularized_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hinzufügen von Dropout\n",
    "\n",
    "**Dropout** ist eine der effektivsten und am häufigsten verwendeten *Regularisierungstechniken* für neuronale Netze, die von Hinton und seinen Studenten an der Universität von Toronto entwickelt wurde. Dropout wird auf eine Schicht angewandt und besteht darin, dass während des Trainings **eine Reihe von Ausgangsmerkmalen (activations) der Schicht zufällig \"weggelassen\" (d. h. auf Null gesetzt) werden.**\n",
    "\n",
    "Nehmen wir an, eine bestimmte Schicht hätte normalerweise einen Vektor `[0.2, 0.5, 1.3, 0.8, 1.1]` für eine bestimmte Eingabeprobe während des Trainings geliefert; \n",
    "- nach Anwendung von Dropout wird dieser Vektor einige zufällig verteilte Nulleinträge haben, z. B. `[0, 0.5, 1.3, 0, 1.1]`. \n",
    "- Die **Dropout-Rate** ist der Anteil der Merkmale, die mit Nullen versehen werden; sie wird normalerweise zwischen 0.2 und 0.5 festgelegt. \n",
    "- Zum **Testzeitpunkt werden keine Einheiten herausgenommen**, stattdessen werden die Ausgabewerte der Schicht um einen Faktor reduziert, der der Dropout-Rate entspricht, um die Tatsache auszugleichen, dass mehr Einheiten aktiv sind als zur Trainingszeit.\n",
    "\n",
    "Betrachten wir eine Numpy-Matrix, die die Ausgabe einer Schicht, `layer_output`, in der Form `(batch_size, features)` enthält. Zum Zeitpunkt des Trainings würden wir einen Teil der Werte in der Matrix nach dem Zufallsprinzip ausschliessen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- Zum Testzeitpunkt würden wir die Ausgabe um die Abbruchrate herunterskalieren. Hier skalieren wir um 0.5 (weil wir zuvor die Hälfte der Einheiten fallen gelassen haben):\n",
    "- Beachten Sie, dass dieser Prozess implementiert werden kann, indem beide Operationen zur Trainingszeit durchgeführt werden und die Ausgabe zur Testzeit unverändert bleibt, was in der Praxis häufig der Fall ist. was in der Praxis häufig der Fall ist:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Diese Technik mag seltsam und willkürlich erscheinen. Warum sollte dies dazu beitragen, die Überanpassung zu verringern? \n",
    "Geoffry Hinton hat gesagt, dass er unter anderem von einem Betrugspräventionsmechanismus inspiriert wurde, der von Banken verwendet wird - in seinen eigenen Worten: \n",
    "\n",
    "*\"Ich ging zu meiner Bank. Die Kassierer wechselten ständig, und ich fragte einen von ihnen, warum. Er sagte, er wisse es nicht, aber sie würden oft versetzt. Ich dachte mir, das muss daran liegen, dass es eine Zusammenarbeit zwischen den Angestellten erfordert, um die Bank erfolgreich zu betrügen. Da wurde mir klar, dass das zufällige Entfernen einer anderen Untergruppe von Neuronen bei jedem Beispiel Verschwörungen verhindern und somit die Überanpassung reduzieren würde.*\n",
    "\n",
    "Die Kernidee ist, dass die Einführung von Rauschen in die Ausgabewerte einer Schicht zufällige Muster aufbrechen kann, die nicht signifikant sind (was Hinton als \"Verschwörungen\" bezeichnet) und die sich das Netzwerk merken würde, wenn kein Rauschen vorhanden wäre. \n",
    "\n",
    "In PyTprcj kann man `Dropout` in ein Netzwerk über die `Dropout`-Schicht einführen, die auf die Ausgabe der Schicht direkt davor angewendet wird, z. B.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fügen wir zwei `Dropout`-Schichten zu unserem IMDB-Netzwerk hinzu, um zu sehen, wie gut sie die Überanpassung reduzieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example usage with PyTorch Lightning Trainer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SentimentModel(pl.LightningModule):\n",
    "    def __init__(self, n_hidden=16, l1_lambda=0.001, l2_lambda=0.0):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10000, n_hidden)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.output = nn.Linear(n_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # L2 regularization (weight_decay)\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        # Initialize metric storage\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.train_accuracies.append(acc.item())\n",
    "        return {\"loss\": loss, \"train_acc\": acc}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5).float() == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_accuracies.append(acc.item())\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(), lr=0.001, weight_decay=self.l2_lambda\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Schauen wir uns das Resultat an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "dropout_model = SentimentModel(n_hidden=512, l1_lambda=1e-5, l2_lambda=0.0)\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "max_epochs = 30\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # callbacks=[early_stopping],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(dropout_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(dropout_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Zusammenfassung: Overfitting bekämpfen\n",
    "\n",
    "Auch dies ist eine deutliche Verbesserung gegenüber dem Referenznetz.\n",
    "\n",
    "Zusammenfassend lässt sich sagen, dass die gängigsten Methoden zur Vermeidung von Overfitting in neuronalen Netzen wie folgt aussehen\n",
    "\n",
    "* Mehr Trainingsdaten erhalten.\n",
    "* Verringern der Kapazität des Netzes.\n",
    "* Hinzufügen einer Gewichtsregulierung.\n",
    "* Hinzufügen von Dropout."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "ger",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ger",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
