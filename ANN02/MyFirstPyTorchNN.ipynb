{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN02/MyFirstPyTorchNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mehrschicht-Perzeptron (MLP) für den Wisconsin Breast Cancer Datensatz\n",
    "\n",
    "In diesem Jupyter-Notebook wird ein vollständiges Beispiel zur Implementierung eines MLP-Modells für die binäre Klassifikation auf dem Wisconsin Breast Cancer Datensatz vorgestellt. Jeder Schritt wird ausführlich erklärt und mit Codebeispielen ergänzt.\n",
    "\n",
    "## 1. Notwendige Bibliotheken importieren\n",
    "Zuerst müssen die notwendigen Bibliotheken importiert werden, die für Datenmanipulation, Modelltraining und Visualisierung benötigt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warum werden diese Bibliotheken benötigt?\n",
    "1. **`torch`**: Die Hauptbibliothek von PyTorch wird für alle grundlegenden Tensoroperationen und die Implementierung neuronaler Netze verwendet.\n",
    "   - Beispiel: Tensoren erstellen, Operationen durchführen und Daten auf GPU verschieben.\n",
    "\n",
    "2. **`torch.nn`**: Dieses Modul enthält vorgefertigte Schichten und Funktionen zur Erstellung neuronaler Netze.\n",
    "   - Beispiel: `nn.Linear`, `nn.ReLU` für vollverbundene Schichten und Aktivierungsfunktionen.\n",
    "\n",
    "3. **`torch.optim`**: Enthält Optimierungsalgorithmen wie `SGD`, `Adam` usw., die für die Anpassung der Modellparameter während des Trainings verwendet werden.\n",
    "   - Beispiel: `optim.Adam` zur Aktualisierung der Gewichte basierend auf den Gradienten.\n",
    "\n",
    "4. **`torch.utils.data`**: Enthält `Dataset` und `DataLoader`, um große Datensätze effizient zu handhaben und in Batches zu laden.\n",
    "   - Beispiel: `DataLoader` für das Laden von Daten in zufälliger Reihenfolge mit der gewünschten Batch-Größe.\n",
    "\n",
    "5. **`sklearn.datasets`**: Die Funktion `load_breast_cancer` stellt den Datensatz zur Verfügung, der in diesem Beispiel verwendet wird.\n",
    "   - Beispiel: Laden eines vorgefertigten Datensatzes zur Klassifikation von Tumoren.\n",
    "\n",
    "6. **`sklearn.model_selection`**: Mit `train_test_split` wird der Datensatz in Trainings- und Testmengen aufgeteilt.\n",
    "   - Beispiel: `train_test_split(X, y)` zum Aufteilen der Daten in 80% Training und 20% Test.\n",
    "\n",
    "7. **`sklearn.preprocessing`**: Der `StandardScaler` wird verwendet, um die Daten zu normalisieren, was für ein stabiles und schnelles Training erforderlich ist.\n",
    "   - Beispiel: Skalieren der Eingabedaten auf Mittelwert 0 und Standardabweichung 1.\n",
    "\n",
    "8. **`matplotlib.pyplot`**: Diese Bibliothek wird verwendet, um Trainings- und Validierungskurven zu visualisieren.\n",
    "   - Beispiel: Plotten der Verlustkurve über die Epochen mit `plt.plot()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Daten vorbereiten und Laden\n",
    "Der Wisconsin Breast Cancer Datensatz wird geladen und in Trainings- und Testdaten aufgeteilt. Die Merkmale werden normalisiert, um das Training zu stabilisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des Datensatzes\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target  # Merkmale und Zielvariablen\n",
    "\n",
    "# Aufteilen in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalisieren der Daten\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Konvertieren in PyTorch-Tensoren\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(\n",
    "    1\n",
    ")  # Ziel muss (N, 1) sein\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was bedeuten die Merkmale und Zielvariablen?\n",
    "- **Merkmale ($X$):** Diese repräsentieren verschiedene physikalische Eigenschaften der Tumore, die im Datensatz enthalten sind. Beispiele für solche Eigenschaften sind:\n",
    "  - Radius (Mittelwert, Extremwert)\n",
    "  - Textur (Standardabweichung der Grauwertintensitäten)\n",
    "  - Glätte (Gleichmäßigkeit der Zellen)\n",
    "  - Kompaktheit\n",
    "  - Symmetrie\n",
    "  - Fraktale Dimension\n",
    "  Jede dieser Eigenschaften wird numerisch dargestellt und hilft dabei, Muster zwischen gutartigen (benignen) und bösartigen (malignen) Tumoren zu erkennen.\n",
    "\n",
    "- **Zielvariablen ($y$):** Diese stellen die Klassifikation des Tumors dar:\n",
    "  - `0`: Gutartig (benign)\n",
    "  - `1`: Bösartig (malign)\n",
    "  Ziel des Modells ist es, basierend auf den Merkmalen vorherzusagen, ob ein Tumor bösartig oder gutartig ist.\n",
    "\n",
    "### Warum ist die Normalisierung wichtig?\n",
    "Die Normalisierung ist besonders für neuronale Netze wichtig, da sie dafür sorgt, dass die Merkmale des Datensatzes vergleichbare Wertebereiche haben. Ohne Normalisierung könnten Merkmale mit größeren Werten die Gradienten während des Trainings dominieren, was zu einer instabilen oder ineffizienten Optimierung führen kann.\n",
    "\n",
    "`StandardScaler` normalisiert die Daten nach der Formel:\n",
    "\n",
    "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
    "\n",
    "Hierbei sind:\n",
    "- $x$: Der ursprüngliche Wert eines Merkmals.\n",
    "- $\\mu$: Der Mittelwert des Merkmals.\n",
    "- $\\sigma$: Die Standardabweichung des Merkmals.\n",
    "\n",
    "Das Ergebnis ist ein Datensatz, bei dem jedes Merkmal einen Mittelwert von 0 und eine Standardabweichung von 1 hat. Dies hilft dem Optimierer, schneller zu konvergieren und stabilere Gradienten zu gewährleisten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Erzeugen eines Datasets und eines DataLoaders\n",
    "Die Daten werden in PyTorch-kompatiblen `Dataset`- und `DataLoader`-Objekten organisiert, um das Laden von Batches zu erleichtern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset erstellen\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# DataLoader erstellen\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modell-Klasse definieren: `model()`\n",
    "Ein einfaches MLP mit zwei versteckten Schichten wird erstellt. Die Ausgabe wird durch die Sigmoid-Funktion skaliert, da es sich um eine binäre Klassifikation handelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Modell initialisieren\n",
    "input_size = X_train.shape[1]\n",
    "model = BinaryClassifier(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Die `nn.Sequential`-Klasse in PyTorch ist eine praktische Möglichkeit, neuronale Netzwerke schnell und kompakt zu definieren, insbesondere für Architekturen, bei denen die Schichten sequentiell angeordnet sind. Sie ermöglicht es, mehrere Schichten in der Reihenfolge ihrer Anwendung zu definieren. Die `nn.Sequential`-Klasse nimmt eine geordnete Liste von Modulen (Schichten) als Eingabe, die dann in der angegebenen Reihenfolge auf die Eingabedaten angewendet werden. Jede Schicht muss eine `nn.Module`-Subklasse sein.\n",
    "\n",
    "1. Die Eingabedaten durchlaufen nacheinander jede Schicht, wie sie in `nn.Sequential` definiert ist.\n",
    "2. Die Ausgabe einer Schicht wird zur Eingabe der nächsten Schicht.\n",
    "3. Am Ende gibt das Modell das Ergebnis der letzten Schicht zurück.\n",
    "\n",
    "#### Vorteile der `nn.Sequential`-Klasse\n",
    "1. **Einfachheit**: Die Definition eines Modells ist kompakt und leserlich, besonders für einfache Architekturen.\n",
    "2. **Lesbarkeit**: Es ist leicht zu erkennen, wie die Schichten in einem Modell angeordnet sind.\n",
    "3. **Schnelles Prototyping**: Ideal für einfache, sequentielle Netzwerke, ohne zusätzliche Logik.\n",
    "4. **Weniger Boilerplate-Code**: Kein explizites Schreiben einer `forward`-Methode erforderlich, da `nn.Sequential` automatisch die Reihenfolge der Schichten verarbeitet.\n",
    "\n",
    "#### Nachteile der `nn.Sequential`-Klasse\n",
    "\n",
    "1. **Eingeschränkte Flexibilität**:\n",
    "   - Komplexere Architekturen, die Verzweigungen, Skip Connections (z. B. in ResNet) oder mehrere Eingaben/Ausgaben benötigen, können nicht mit `nn.Sequential` implementiert werden.\n",
    "   - Beispiel: Residual Blocks erfordern benutzerdefinierte Logik.\n",
    "\n",
    "   ```python\n",
    "   class ResidualBlock(nn.Module):\n",
    "       def __init__(self, in_features):\n",
    "           super().__init__()\n",
    "           self.block = nn.Sequential(\n",
    "               nn.Linear(in_features, in_features),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(in_features, in_features)\n",
    "           )\n",
    "\n",
    "       def forward(self, x):\n",
    "           return x + self.block(x)  # Skip Connection\n",
    "   ```\n",
    "\n",
    "2. **Keine benutzerdefinierte Logik**:\n",
    "   - Wenn individuelle Schichten spezifisches Verhalten benötigen (z. B. verschiedene Pfade basierend auf Bedingungen), ist `nn.Sequential` ungeeignet.\n",
    "\n",
    "3. **Fehlende Schichtbenennung**:\n",
    "   - In `nn.Sequential` sind die Schichten standardmäßig durchnummeriert (z. B. `0`, `1`, `2`).\n",
    "   - Für bessere Nachvollziehbarkeit ist es manchmal hilfreich, Schichten manuell zu benennen.\n",
    "     ```python\n",
    "     model = nn.Sequential(\n",
    "         ('fc1', nn.Linear(10, 32)),\n",
    "         ('activation', nn.ReLU()),\n",
    "         ('fc2', nn.Linear(32, 1))\n",
    "     )\n",
    "     ```\n",
    "Die `nn.Sequential`-Klasse ist ideal für einfache Modelle mit einer klaren, linearen Schichtanordnung. Für komplexe Architekturen mit Verzweigungen, Skip Connections oder benutzerdefinierter Logik ist jedoch eine explizite Subklasse von `nn.Module` besser geeignet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell mit nn.Sequential definieren\n",
    "model2 = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 32),  # Erste versteckte Schicht\n",
    "    nn.ReLU(),  # Aktivierungsfunktion\n",
    "    nn.Linear(32, 16),  # Zweite versteckte Schicht\n",
    "    nn.ReLU(),  # Aktivierungsfunktion\n",
    "    nn.Linear(16, 1),  # Ausgabe-Schicht\n",
    "    nn.Sigmoid(),  # Sigmoid für binäre Klassifikation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss-Funktion (criterium), Metrik und Optimierer definieren\n",
    "Die Binary Cross Entropy Loss (BCE) wird verwendet. Diese wird wie folgt definiert:\n",
    "\n",
    "$$ L(\\hat{y}, y) = -\\frac{1}{N} \\sum_{i=1}^{N} \\big(y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i)\\big) $$\n",
    "\n",
    "Zusätzlich verwenden wir den Adam-Optimierer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das **Gradientenabstiegsverfahren** ist ein grundlegendes Optimierungsverfahren, das in maschinellem Lernen verwendet wird, um die Parameter eines Modells zu aktualisieren und die Verlustfunktion zu minimieren. Der Adam-Optimierer ist eine erweiterte Variante des Gradientenabstiegs, die adaptives Lernen ermöglicht und in vielen Anwendungen bevorzugt wird. Das Ziel des Gradientenabstiegs ist es, eine Verlustfunktion $\\mathcal{L}(\\mathbf{w})$ zu minimieren, die von den Modellparametern $\\mathbf{w}$ abhängt. Die grundlegende Lernregel lautet:\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) $$\n",
    "\n",
    "### Erklärungen:\n",
    "1. **Parameter $\\mathbf{w}$:**\n",
    "   - Dies sind die Variablen (z. B. Gewichte und Biases), die im Modell angepasst werden.\n",
    "\n",
    "2. **Gradient $\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w})$:**\n",
    "   - Der Gradient der Verlustfunktion $\\mathcal{L}$ nach den Parametern $\\mathbf{w}$ gibt die Richtung des steilsten Anstiegs der Verlustfunktion an.\n",
    "   - Durch die Subtraktion des Gradienten bewegen wir uns in die Richtung des steilsten Abstiegs.\n",
    "\n",
    "3. **Lernrate $\\eta$ (Learning Rate):**\n",
    "   - Ein hyperparametrischer Faktor, der bestimmt, wie groß die Schritte bei der Aktualisierung der Parameter sind.\n",
    "   - Wenn $\\eta$ zu groß ist, kann der Algorithmus instabil werden oder über das Minimum hinaus schwingen.\n",
    "   - Wenn $\\eta$ zu klein ist, kann das Training langsam konvergieren oder in lokalen Minima hängen bleiben.\n",
    "\n",
    "**Ablauf**\n",
    "1. Initialisiere die Parameter $\\mathbf{w}$ zufällig.\n",
    "2. Berechne den Gradient $\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w})$ basierend auf den Trainingsdaten.\n",
    "3. Aktualisiere die Parameter gemäß der Lernregel.\n",
    "4. Wiederhole die Schritte, bis die Verlustfunktion ein Minimum erreicht.\n",
    "\n",
    "Der Adam-Optimierer (Adaptive Moment Estimation) ist eine verbesserte Variante des Gradientenabstiegs. Er kombiniert die Vorteile von zwei anderen Optimierungsverfahren:\n",
    "1. **Momentum**: Beschleunigt die Konvergenz durch die Berücksichtigung vorheriger Gradienten.\n",
    "2. **RMSProp**: Skaliert die Lernrate adaptiv basierend auf der Größe der Gradienten.\n",
    "\n",
    "Die Adam-Formel aktualisiert die Parameter wie folgt:\n",
    "1. **Berechnung des Gradienten:**\n",
    "   $$ g_t = \\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) $$\n",
    "\n",
    "2. **Erster Moment (mittlerer Gradient):** $\\beta_1$ ist der Vergessensfaktor für den Mittelwert (typisch $\\beta_1 = 0.9$).\n",
    "   $$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$\n",
    "\n",
    "3. **Zweiter Moment (mittlere quadratische Größe):** $\\beta_2$ ist der Vergessensfaktor für die Varianz (typisch $\\beta_2 = 0.999$).\n",
    "   $$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $$\n",
    "\n",
    "4. **Bias-Korrektur:** Diese Korrektur wird durchgeführt, da die Momente am Anfang unterschätzt werden.\n",
    "   $$ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} $$\n",
    "   \n",
    "5. **Parameteraktualisierung:** $\\epsilon$ ist ein kleiner Wert (z. B. $10^{-8}$), um Division durch Null zu vermeiden.\n",
    "   $$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$\n",
    "\n",
    "**Vorteile des Adam-Optimierers**\n",
    "1. **Adaptives Lernen:** Die Lernrate wird für jeden Parameter individuell skaliert.\n",
    "2. **Stabilität:** Kombiniert die Vorteile von Momentum und RMSProp.\n",
    "3. **Effizient:** Funktioniert gut auf großen Datensätzen und in hohen Dimensionen.\n",
    "4. **Geringer Hyperparameter-Aufwand:** Standardwerte ($\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$) funktionieren oft gut.\n",
    "\n",
    "**Nachteile des Adam-Optimierers**\n",
    "1. **Überanpassung:** Kann in einigen Fällen zu stark an die Trainingsdaten angepasst sein.\n",
    "2. **Schlechte Generalisierung:** Führt manchmal zu schlechteren Testergebnissen im Vergleich zu einfacheren Optimierern wie SGD.\n",
    "3. **Rechenaufwand:** Erfordert mehr Speicher und Rechenzeit als simpler SGD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trainings-Loop\n",
    "Der Trainings-Loop iteriert über die Trainingsdaten, berechnet den Loss und aktualisiert die Modellparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Nullsetzen der Gradienten\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Vorwärtsdurchlauf\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Rückwärtsdurchlauf und Optimierung\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Verlust: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validierungs-Loop\n",
    "Nach jedem Trainingsepoch wird das Modell auf den Testdaten evaluiert. Die Genauigkeit wird berechnet als:\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{\\text{Anzahl korrekter Vorhersagen}}{\\text{Gesamtanzahl der Beispiele}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_losses = []\n",
    "accuracies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        correct = (predicted == y_batch).sum().item()\n",
    "        accuracy = correct / y_batch.size(0)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "mean_test_loss = sum(test_losses) / len(test_losses)\n",
    "mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(f\"Testverlust: {mean_test_loss:.4f}, Genauigkeit: {mean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warum ist der Evaluierungsmodus notwendig?\n",
    "Der Evaluierungsmodus wird mit `model.eval()` aktiviert und hat zwei Hauptzwecke:\n",
    "\n",
    "1. **Deaktivierung von Dropout und Batch Normalization:** Während des Trainings können Mechanismen wie Dropout oder Batch Normalization verwendet werden, um die Generalisierung zu verbessern. Diese Verhaltensweisen sind im Evaluierungsmodus deaktiviert, da sie während der Inferenz nicht benötigt werden. Das sorgt für konsistente und deterministische Vorhersagen.\n",
    "\n",
    "2. **Optimierung der Leistung:** Durch das Deaktivieren von Gradientenberechnungen (`torch.no_grad()`), das üblicherweise in Verbindung mit `eval()` verwendet wird, wird die Berechnung effizienter und speicherschonender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusammenfassend sorgt der Evaluierungsmodus dafür, dass das Modell im Testmodus korrekte Vorhersagen liefert, ohne unnötige Trainings-spezifische Mechanismen oder zusätzliche Speicheranforderungen.\n",
    "\n",
    "## 8. Modell speichern und Validierungskurven plotten\n",
    "Das trainierte Modell wird gespeichert, und die Trainingsverluste werden visualisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell speichern\n",
    "torch.save(model.state_dict(), \"binary_classifier.pth\")\n",
    "print(\"Modell gespeichert.\")\n",
    "\n",
    "# Verlustkurven plotten\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Trainingsverlust\")\n",
    "plt.xlabel(\"Epoche\")\n",
    "plt.ylabel(\"Verlust\")\n",
    "plt.title(\"Trainingsverlustkurve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachLe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
