{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U09/ANN09_Pretrained_Word_Embeddings_SOLUTION_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Verwendung vortrainierter Word-Embeddings\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Beschreibung:** Textklassifikation auf dem Newsgroup20-Datensatz unter Verwendung vortrainierter GloVe-Worteinbettungen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pathlib\n",
    "import tarfile\n",
    "import zipfile\n",
    "import logging\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import Counter\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup Standard Logging\n",
    "logging.basicConfig(\n",
    "    level=\"INFO\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"[%H:%M:%S]\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Vortrainierte Word-Embeddings\n",
    "\n",
    "- In diesem Beispiel zeigen wir, wie man ein Textklassifizierungsmodell trainiert, das vortrainierte Worteinbettungen verwendet.\n",
    "- Wir arbeiten mit dem Newsgroup20-Datensatz, einem Satz von 20.000 Nachrichten auf Messageboards die zu 20 verschiedenen Themenkategorien gehören.\n",
    "\n",
    "Für die vortrainierten Worteinbettungen werden wir Folgendes verwenden\n",
    "[GloVe embeddings](http://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## (a) Laden Sie den Newsgroup20 Datensatz herunter\n",
    "\n",
    "- http://qwone.com/~jason/20Newsgroups/\n",
    "- http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
    "\n",
    "Der 20 Newsgroups-Datensatz ist eine Sammlung von etwa 20.000 Newsgroup-Texten, die (fast) gleichmässig über 20 verschiedene Newsgroups verteilt sind. Sie wurde sie ursprünglich von Ken Lang gesammelt, wahrscheinlich für seinen Newsweeder: *Learning to filter netnews*, obwohl er diese Sammlung nicht ausdrücklich erwähnt. Die Sammlung von 20 Newsgroups ist zu einem beliebten Datensatz für Experimente mit Textanwendungen von maschinellen Lerntechniken geworden, wie z. B. Textklassifizierung und Textclustering.\n",
    "\n",
    "**Organisation**\n",
    "Die Daten sind in 20 verschiedene Newsgroups unterteilt, die jeweils einem bestimmten Thema entsprechen. Einige der Newsgroups sind sehr eng miteinander verwandt (z. B. comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), während andere in keinem Zusammenhang stehen (z. B. misc.forsale / soc.religion.christian). Hier ist eine Liste der 20 Newsgroups, die (mehr oder weniger) nach Themen geordnet sind:\n",
    "\n",
    "| computers                  | recreation              |  science    | politics |\n",
    "|:---------------------------|:------------------------|:------------|:---------|\n",
    "| `comp.graphics`            | `rec.autos`             | `sci.crypt` | `talk.politics.misc` |\n",
    "| `comp.os.ms-windows.misc`  | `rec.motorcycles`       | `sci.electronics` | `talk.politics.misc` |\n",
    "| `comp.sys.ibm.pc.hardware` | `rec.sport.baseball`    | `sci.med` | `talk.politics.guns` |\n",
    "| `comp.sys.mac.hardware`    | `rec.sport.baseball`    | `sci.space` | `talk.politics.mideast` |\n",
    "| `comp.windows.x`           | `rec.sport.baseball`    |            | `talk.religion.misc`|\n",
    "|                            | `rec.sport.hockey`      |            |                     |\n",
    "\n",
    "Ausserdem gibt es noch die Bereiche:\n",
    "\n",
    "- `misc.forsale\t`\n",
    "- `alt.atheism`\n",
    "- `soc.religion.christian`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lädt den 20 Newsgroups-Datensatz herunter und extrahiert ihn ins angegebene Verzeichnis.\n",
    "\n",
    "### Ablauf:\n",
    "1. **URL festlegen**  \n",
    "   `news20.tar.gz` wird von einer festen URL geladen.\n",
    "\n",
    "2. **Verzeichnis anlegen**  \n",
    "   `os.makedirs(data_dir)` erstellt Zielordner, falls nicht vorhanden.\n",
    "\n",
    "3. **Download prüfen & durchführen**  \n",
    "   Falls `news20.tar.gz` noch nicht existiert → Download mit `urllib.request.urlretrieve`.\n",
    "\n",
    "4. **Entpacken**  \n",
    "   Falls noch nicht extrahiert → Entpacken mit `tarfile.open(...).extractall(...)`.\n",
    "\n",
    "5. **Pfad zurückgeben**  \n",
    "   Gibt den Pfad zum entpackten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# 0. Dataset herunterladen und extrahieren\n",
    "#####################################\n",
    "\n",
    "\n",
    "def download_and_extract_news20(data_dir):\n",
    "    \"\"\"\n",
    "    Lädt den News20-Datensatz von der Online-Quelle herunter und extrahiert ihn.\n",
    "    \"\"\"\n",
    "    url = \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    tar_path = os.path.join(data_dir, \"news20.tar.gz\")\n",
    "    if not os.path.exists(tar_path):\n",
    "        logger.info(\"Downloading News20 dataset...\")\n",
    "        urllib.request.urlretrieve(url, tar_path)\n",
    "    extract_path = os.path.join(data_dir, \"20_newsgroup\")\n",
    "    if not os.path.exists(extract_path):\n",
    "        logger.info(\"Extracting dataset...\")\n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=data_dir)\n",
    "    return extract_path\n",
    "\n",
    "\n",
    "data_base_dir = \"./news20_data\"\n",
    "data_dir = download_and_extract_news20(data_base_dir)\n",
    "logger.info(f\"Datensatzpfad: {data_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) Daten laden und vorverarbeiten\n",
    "Lädt den **20 Newsgroups**-Datensatz und bereitet ihn vor, indem die Header (ersten 10 Zeilen) jedes Dokuments entfernt werden.\n",
    "\n",
    "### Ablauf:\n",
    "\n",
    "1. **Pfad setzen**  \n",
    "   `data_dir` wird in ein `Path`-Objekt umgewandelt für einfacheres Dateihandling.\n",
    "\n",
    "2. **Klassennamen ermitteln**  \n",
    "   Listet alle Unterordner (eine Klasse = ein Ordner), sortiert alphabetisch.\n",
    "\n",
    "3. **Initialisierung von Datenlisten**  \n",
    "   `samples` für die Texte, `labels` für die zugehörigen Klassen.\n",
    "\n",
    "4. **Daten pro Klasse einlesen**  \n",
    "   Für jeden Klassenordner:\n",
    "   - Alle Dateien (außer versteckte) sammeln\n",
    "   - Jede Datei:\n",
    "     - Öffnen mit `latin-1` Encoding\n",
    "     - Header (erste 10 Zeilen) entfernen\n",
    "     - Restinhalt zu `samples` hinzufügen\n",
    "     - Klassenindex zu `labels` hinzufügen\n",
    "\n",
    "5. **Ausgabe**  \n",
    "   Gibt eine Liste von Texten (`samples`), zugehörige Labels (`labels`) und die Klassennamen zurück.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# 1. Daten laden und Vorverarbeitung\n",
    "#####################################\n",
    "\n",
    "\n",
    "def load_news20_data(data_dir):\n",
    "    \"\"\"\n",
    "    Liest den News20-Datensatz ein und entfernt die ersten 10 Zeilen (Header) jedes Dokuments.\n",
    "    \"\"\"\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    class_names = sorted([d for d in os.listdir(data_dir) if not d.startswith(\".\")])\n",
    "    samples = []\n",
    "    labels = []\n",
    "    for class_index, classname in enumerate(class_names):\n",
    "        dirpath = data_dir / classname\n",
    "        fnames = [\n",
    "            f for f in os.listdir(dirpath) if not f.startswith(\".\") and os.path.isfile(dirpath / f)\n",
    "        ]\n",
    "        logger.info(f\"Processing {classname}: {len(fnames)} Dateien gefunden\")\n",
    "        for fname in fnames:\n",
    "            fpath = dirpath / fname\n",
    "            with open(fpath, encoding=\"latin-1\") as f:\n",
    "                content = f.read()\n",
    "                lines = content.split(\"\\n\")[10:]  # entferne Header\n",
    "                content = \"\\n\".join(lines)\n",
    "                samples.append(content)\n",
    "                labels.append(class_index)\n",
    "    return samples, labels, class_names\n",
    "\n",
    "\n",
    "samples, labels, class_names = load_news20_data(data_dir)\n",
    "logger.info(f\"Klassen: {class_names}\")\n",
    "logger.info(f\"Anzahl Samples: {len(samples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) Trainings- und Validationsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# 2. Aufteilen in Trainings- und Validierungsdaten\n",
    "##########################################\n",
    "\n",
    "train_samples, val_samples, train_labels, val_labels = train_test_split(\n",
    "    samples, labels, test_size=0.2, random_state=1337\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d) Tokenizer und Vokabularaufbau\n",
    "Dieser Abschnitt bereitet Textdaten für maschinelles Lernen vor, indem die Texte in **numerische Sequenzen** umgewandelt werden. Ziel: Die Texte in eine Form bringen, die ein neuronales Netz verarbeiten kann.\n",
    "\n",
    "---\n",
    "\n",
    "### Funktion: `tokenize(text)`\n",
    "- Teilt einen Text in einzelne Wörter (Tokens).\n",
    "- Alles wird in Kleinbuchstaben umgewandelt.\n",
    "- **Beispiel:** `\"Hello World\"` → `[\"hello\", \"world\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### Funktion: `build_vocab(samples, max_tokens=20000)`\n",
    "- Erstellt ein **Vokabular** der häufigsten Wörter.\n",
    "- Nutzt `Counter`, um Wortfrequenzen zu zählen.\n",
    "- Die häufigsten `max_tokens` Wörter erhalten fortlaufende Indizes:\n",
    "  - `\"<PAD>\" = 0`: für Padding\n",
    "  - `\"<OOV>\" = 1`: für unbekannte Wörter (Out-Of-Vocabulary)\n",
    "- Gibt ein Dictionary `{wort: index}` zurück.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# 3. Tokenizer und Vokabularaufbau\n",
    "############################################\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Einfacher Tokenizer, der in Kleinbuchstaben in Wörter aufteilt.\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def build_vocab(samples, max_tokens=20000):\n",
    "    \"\"\"\n",
    "    Baut ein Vokabular aus den häufigsten Wörtern auf.\n",
    "    Reserviert Index 0 für Padding und 1 für unbekannte Wörter (OOV).\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for text in samples:\n",
    "        tokens = tokenize(text)\n",
    "        counter.update(tokens)\n",
    "    most_common = counter.most_common(max_tokens)\n",
    "    vocab = {\"<PAD>\": 0, \"<OOV>\": 1}\n",
    "    for idx, (word, _) in enumerate(most_common, start=2):\n",
    "        vocab[word] = idx\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = build_vocab(train_samples, max_tokens=20000)\n",
    "logger.info(f\"Groesse des Vokabulars: {len(vocab)}\")\n",
    "\n",
    "\n",
    "def encode_text(text, vocab, max_len=200):\n",
    "    \"\"\"\n",
    "    Kodiert einen Text in eine Sequenz von Indizes.\n",
    "    Schneidet auf max_len zu oder füllt mit Padding.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    encoded = [vocab.get(token, vocab[\"<OOV>\"]) for token in tokens]\n",
    "    if len(encoded) > max_len:\n",
    "        encoded = encoded[:max_len]\n",
    "    else:\n",
    "        encoded += [vocab[\"<PAD>\"]] * (max_len - len(encoded))\n",
    "    return np.array(encoded, dtype=np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e) Dataset und Dataloader\n",
    "Hier wird ein eigenes Dataset für PyTorch erstellt, um die vorbereiteten Texte und Labels effizient zu laden und zu batchen – ideal für Training und Validierung.\n",
    "\n",
    "---\n",
    "\n",
    "### Klasse: `NewsGroupDataset`\n",
    "Ein benutzerdefiniertes Dataset, das mit `torch.utils.data.Dataset` kompatibel ist.\n",
    "\n",
    "#### `__init__`\n",
    "- Speichert:\n",
    "  - `samples`: Liste von Texten\n",
    "  - `labels`: zugehörige Klassenzuordnungen\n",
    "  - `vocab`: Wörterbuch zur Umwandlung in Indizes\n",
    "  - `max_len`: maximale Länge pro Text (für Padding/Truncation)\n",
    "\n",
    "#### `__len__`\n",
    "- Gibt die Anzahl der Samples zurück → wichtig für Batch-Iteration.\n",
    "\n",
    "#### `__getitem__(idx)`\n",
    "- Holt das `idx`-te Text-Label-Paar.\n",
    "- Wandelt den Text mit `encode_text` in eine Zahlen-Sequenz um.\n",
    "- Gibt ein Tupel aus `(encoded_tensor, label_tensor)` zurück – beide als `torch.tensor`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 4. Dataset und DataLoader\n",
    "##############################################\n",
    "\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    def __init__(self, samples, labels, vocab, max_len=200):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoded = encode_text(text, self.vocab, self.max_len)\n",
    "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset = NewsGroupDataset(train_samples, train_labels, vocab, max_len=200)\n",
    "val_dataset = NewsGroupDataset(val_samples, val_labels, vocab, max_len=200)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (f) GloVe Einbettungen laden\n",
    "In diesem Abschnitt werden **vortrainierte GloVe-Wortvektoren** heruntergeladen, eingelesen und in eine **Embedding-Matrix** für das Modell überführt.\n",
    "\n",
    "---\n",
    "\n",
    "### Funktion: `download_glove_embeddings(...)`\n",
    "- Lädt das GloVe-Zip-Archiv von der offiziellen Stanford-Seite.\n",
    "- Entpackt es im Zielverzeichnis, falls es dort noch nicht vorliegt.\n",
    "- Gibt den Pfad zur gewünschten `.txt`-Datei zurück (z. B. `glove.6B.100d.txt`).\n",
    "\n",
    "---\n",
    "\n",
    "### Funktion: `get_embedding_dim_from_filename(...)`\n",
    "- Extrahiert die Vektor-Dimension (z. B. `100d`) aus dem Dateinamen.\n",
    "- Praktisch, um `embedding_dim` automatisch zu setzen.\n",
    "\n",
    "---\n",
    "\n",
    "### Funktion: `load_glove_embeddings(...)`\n",
    "- Liest die GloVe-Datei Zeile für Zeile ein.\n",
    "- Jede Zeile enthält ein Wort + Vektorwerte → wird als `np.array` gespeichert.\n",
    "- Ergebnis: Dictionary `{wort: vektor}`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# 5. Vortrainierte GloVe-Einbettungen laden\n",
    "#####################################################\n",
    "\n",
    "\n",
    "def load_glove_embeddings(glove_file_path):\n",
    "    \"\"\"\n",
    "    Liest GloVe-Einbettungen aus der Datei ein.\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            if len(values) < 2:\n",
    "                continue\n",
    "            word = values[0]\n",
    "            try:\n",
    "                vector = np.asarray(values[1:], dtype=np.float32)\n",
    "                embeddings_index[word] = vector\n",
    "            except ValueError:\n",
    "                logger.warning(\"Skipping line due to ValueError.\")\n",
    "                continue\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def download_glove_embeddings(glove_dir, glove_file=\"glove.6B.100d.txt\"):\n",
    "    \"\"\"\n",
    "    Lädt die GloVe-Einbettungen herunter und entpackt sie, falls noch nicht vorhanden.\n",
    "    \"\"\"\n",
    "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    os.makedirs(glove_dir, exist_ok=True)\n",
    "    zip_path = os.path.join(glove_dir, \"glove.6B.zip\")\n",
    "    glove_file_path = os.path.join(glove_dir, glove_file)\n",
    "\n",
    "    if not os.path.exists(glove_file_path):\n",
    "        if not os.path.exists(zip_path):\n",
    "            logger.info(\"Downloading GloVe embeddings...\")\n",
    "            urllib.request.urlretrieve(url, zip_path)\n",
    "        logger.info(\"Extracting GloVe embeddings...\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(glove_dir)\n",
    "    return glove_file_path\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def get_embedding_dim_from_filename(glove_file):\n",
    "    match = re.search(r\"\\.(\\d+)d\\.txt$\", glove_file)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    raise ValueError(f\"Ungültiger GloVe-Dateiname: {glove_file}\")\n",
    "\n",
    "\n",
    "glove_dir = \"./glove_data\"\n",
    "glove_file = \"glove.6B.100d.txt\"  # oder \"glove.6B.300d.txt\", \"glove.6B.50d.txt\", etc.\n",
    "glove_file_path = download_glove_embeddings(glove_dir, glove_file=glove_file)\n",
    "\n",
    "# Automatisch korrekte Dimensionalität setzen\n",
    "embedding_dim = get_embedding_dim_from_filename(glove_file)\n",
    "logger.info(f\"Verwende embedding_dim = {embedding_dim}\")\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "num_tokens = len(vocab)\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim), dtype=np.float32)\n",
    "\n",
    "hits, misses = 0, 0\n",
    "for word, i in vocab.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "logger.info(f\"Converted {hits} words ({misses} misses)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (g) PyTorch Modell aufstellen\n",
    "Dieses Modell ist ein **Convolutional Neural Network (CNN)** zur **Textklassifikation**, basierend auf GloVe-Embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### Architekturüberblick:\n",
    "\n",
    "#### `__init__`\n",
    "- **Embedding-Schicht**  \n",
    "  Vortrainierte GloVe-Vektoren, nicht trainierbar (`requires_grad=False`)\n",
    "\n",
    "- **Convolutional Layers**  \n",
    "  Drei 1D-Convs mit ReLU + Max-Pooling zur Extraktion lokaler Textmuster.\n",
    "\n",
    "- **Pooling & Dense Layer**  \n",
    "  - Globales Max-Pooling → reduziert Sequenz auf festen Vektor.\n",
    "  - `fc1` + ReLU + Dropout → Regularisierung\n",
    "  - `fc2` → Finale Klassenvorhersage\n",
    "\n",
    "---\n",
    "\n",
    "### `forward(x)`\n",
    "- Input: `(batch_size, sequence_length)`\n",
    "- Output: Logits für jede Klasse → `(batch_size, num_classes)`\n",
    "\n",
    "---\n",
    "\n",
    "### `training_step(...)` & `validation_step(...)`\n",
    "- Berechnen **Cross-Entropy Loss** und **Accuracy**\n",
    "- Loggen Metriken für jede Epoche\n",
    "- Speichern zusätzlich `loss` & `acc` in Listen (z. B. für spätere Visualisierung)\n",
    "\n",
    "---\n",
    "\n",
    "### `configure_optimizers()`\n",
    "- Verwendet **RMSprop** als Optimierer mit dem vorgegebenen Lernrate-Wert.\n",
    "\n",
    "---\n",
    "\n",
    "### `predict_text(...)`\n",
    "- Nutzt das Modell, um neue Texte zu kodieren und Klassifikationswahrscheinlichkeiten (via `softmax`) zurückzugeben.\n",
    "\n",
    "---\n",
    "\n",
    "### Ziel:\n",
    "`TextCNN` erkennt Textmuster über Convolutional Filters und eignet sich besonders gut für **Textklassifikation mit fixierter Eingabelänge** (z. B. bei Dokumenten oder Tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# 6. Modell in PyTorch Lightning: TextCNN\n",
    "########################################################\n",
    "\n",
    "\n",
    "class TextCNN(pl.LightningModule):\n",
    "    def __init__(self, embedding_matrix, num_classes, dropout=0.5, lr=1e-3):\n",
    "        super(TextCNN, self).__init__()\n",
    "        num_tokens, embedding_dim = embedding_matrix.shape\n",
    "        # Embedding-Schicht: vortrainierte Einbettungen, die fixiert sind.\n",
    "        self.embedding = nn.Embedding(num_tokens, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix), requires_grad=False)\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=5)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5)\n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.lr = lr\n",
    "\n",
    "        # loss Kurven\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length)\n",
    "        x = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
    "        x = x.transpose(1, 2)  # (batch_size, embedding_dim, sequence_length)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.max(x, dim=2)[0]  # Globales Max-Pooling\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        # loss Kurven\n",
    "        self.train_loss.append(loss.item())\n",
    "        self.train_acc.append(acc.item())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        # loss Kurven\n",
    "        self.val_loss.append(loss.item())\n",
    "        self.val_acc.append(acc.item())\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def predict_text(self, texts, vocab, max_len=200):\n",
    "        \"\"\"\n",
    "        Kodiert Texte und gibt Vorhersagen als Wahrscheinlichkeiten zurück.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        encoded = np.array([encode_text(text, vocab, max_len) for text in texts])\n",
    "        x = torch.tensor(encoded, dtype=torch.long).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self(x)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "        return probs.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (h) Training und Inferenz\n",
    "In diesem Codeabschnitt wird ein TextCNN-Modell trainiert:\n",
    "\n",
    "- Die Trainingsumgebung wird mit Checkpointing und Logging eingerichtet.\n",
    "\n",
    "- Das Modell speichert automatisch das beste Ergebnis auf Basis der Validierungsgenauigkeit.\n",
    "\n",
    "- Die Trainingsstatistiken werden in einer CSV-Datei protokolliert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 7. Training und Inferenz\n",
    "#########################################\n",
    "np.Inf = np.inf\n",
    "num_classes = len(class_names)\n",
    "model = TextCNN(embedding_matrix, num_classes)\n",
    "\n",
    "\n",
    "# Callback für das Speichern des besten Modells basierend auf der Validierungsgenauigkeit\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_acc\",  # Überwache die Validierungsgenauigkeit\n",
    "    mode=\"max\",  # Maximierung der Genauigkeit\n",
    "    save_top_k=1,  # Speichere nur das beste Modell\n",
    "    filename=\"best-checkpoint\",  # Dateiname des besten Modells\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# CSV Logger hinzufügen\n",
    "csv_logger = pl.loggers.CSVLogger(save_dir=\"logs/\", name=\"textcnn_logs\")\n",
    "\n",
    "# Trainer mit automatischer Geräteauswahl und CSV Logger\n",
    "trainer = Trainer(\n",
    "    max_epochs=15,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=csv_logger,\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(csv_logger.log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def find_latest_metrics_path(base_log_dir):\n",
    "    version_dirs = [d for d in os.listdir(base_log_dir) if d.startswith(\"version_\")]\n",
    "    if not version_dirs:\n",
    "        raise FileNotFoundError(\"Keine version_x Ordner im Log-Verzeichnis gefunden.\")\n",
    "\n",
    "    # Nach Versionsnummer sortieren\n",
    "    version_dirs.sort(key=lambda x: int(x.split(\"_\")[1]))\n",
    "    latest_version = version_dirs[-1]\n",
    "\n",
    "    return os.path.join(base_log_dir, latest_version, \"metrics.csv\")\n",
    "\n",
    "\n",
    "# Hauptverzeichnis für Logs\n",
    "base_log_dir = os.path.join(\"logs\", \"textcnn_logs\")\n",
    "metrics_path = find_latest_metrics_path(base_log_dir)\n",
    "\n",
    "print(\"Neueste metrics.csv gefunden unter:\", metrics_path)\n",
    "\n",
    "# DataFrame vorbereiten\n",
    "df = pd.read_csv(metrics_path)\n",
    "df_train = df[[\"epoch\", \"train_loss\", \"train_acc\"]].dropna().copy()\n",
    "df_val = df[[\"epoch\", \"val_loss\", \"val_acc\"]].dropna().copy()\n",
    "\n",
    "# Merge auf 'epoch'\n",
    "df_merged = pd.merge(df_train, df_val, on=\"epoch\", suffixes=(\"_train\", \"_val\"))\n",
    "\n",
    "df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotte df_merged mit seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setze den Stil für die Plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Erstelle eine Figur mit zwei Subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "# Plot für den Verlust\n",
    "sns.lineplot(data=df_merged, x=\"epoch\", y=\"train_loss\", ax=ax1, label=\"Trainingsverlust\")\n",
    "sns.lineplot(data=df_merged, x=\"epoch\", y=\"val_loss\", ax=ax1, label=\"Validierungsverlust\")\n",
    "ax1.set_title(\"Trainings- und Validierungsverlust\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Verlust\")\n",
    "ax1.legend()\n",
    "# Plot für die Genauigkeit\n",
    "sns.lineplot(data=df_merged, x=\"epoch\", y=\"train_acc\", ax=ax2, label=\"Trainingsgenauigkeit\")\n",
    "sns.lineplot(data=df_merged, x=\"epoch\", y=\"val_acc\", ax=ax2, label=\"Validierungsgenauigkeit\")\n",
    "ax2.set_title(\"Trainings- und Validierungsgenauigkeit\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Genauigkeit\")\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Beispieltexte definieren**  \n",
    "   Eine Liste `sample_texts` mit fünf Texten wird angelegt, die später klassifiziert werden sollen.\n",
    "\n",
    "2. **Bestes Modell laden**  \n",
    "   - `best_model_path` holt den Pfad zum aktuell besten Checkpoint aus `checkpoint_callback`.  \n",
    "   - Mit `TextCNN.load_from_checkpoint(...)` wird das gespeicherte Modell geladen (inkl. `embedding_matrix` und `num_classes`).\n",
    "\n",
    "3. **Evaluierungsmodus aktivieren**  \n",
    "   Durch `best_model.eval()` schaltet das Modell in den Inferenz‑Modus (z. B. deaktiviert Dropout).\n",
    "\n",
    "4. **Klassenvorhersagen berechnen**  \n",
    "   - `model.predict_text(texts=sample_texts, vocab=vocab)` liefert für jeden Text die Wahrscheinlichkeitsverteilung über die Klassen.  \n",
    "   - `probs.argmax(axis=1)` wählt jeweils die Klasse mit der höchsten Wahrscheinlichkeit aus.\n",
    "\n",
    "5. **Ergebnis**  \n",
    "   `predicted_classes` enthält die vorhergesagten Klassenindices für die fünf Beispieltexte.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Inferenz\n",
    "sample_texts = [\n",
    "    \"this message is about computer graphics and 3D modeling\",\n",
    "    \"I firmly believe in God.\",\n",
    "    \"I firmly believe in God and all the angels.\",\n",
    "    \"it's time to make peace in the Far East. War is bad and should be avoided.\",\n",
    "    \"the new Volvo is excellent. It has drive by wire and automatic gear control.\",\n",
    "]\n",
    "\n",
    "\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "logger.info(f\"Lade bestes Modell von: {best_model_path}\")\n",
    "\n",
    "best_model = TextCNN.load_from_checkpoint(\n",
    "    best_model_path, embedding_matrix=embedding_matrix, num_classes=num_classes\n",
    ")\n",
    "best_model.eval()\n",
    "probs = model.predict_text(texts=sample_texts, vocab=vocab)\n",
    "predicted_classes = probs.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Vorhersagen in normalem Logging-Format\n",
    "logger.info(\"Beispiel-Inferenz:\")\n",
    "logger.info(f\"{'Text':<80} | {'Predicted Class':<15}\")\n",
    "logger.info(\"-\" * 100)\n",
    "for text, pred in zip(sample_texts, predicted_classes):\n",
    "    logger.info(f\"{text:<80} | {class_names[pred]:<15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung und Ausgabe der Accuracy\n",
    "y_pred = model.predict_text(texts=val_samples, vocab=vocab)\n",
    "y_label_pred = [np.argmax(prob) for prob in y_pred]\n",
    "acc = accuracy_score(y_true=y_label_pred, y_pred=val_labels)\n",
    "logger.info(\"----- Evaluation Summary -----\")\n",
    "logger.info(f\"Accuracy: {acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pretrained_word_embeddings",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
