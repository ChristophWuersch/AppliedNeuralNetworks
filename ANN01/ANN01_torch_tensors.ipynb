{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN01/ANN01_torch_tensors.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "%pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Tensoren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Print Python version\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check and print CUDA availability and version\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tensors\n",
    "# You can create tensors from Python lists or NumPy arrays:\n",
    "data = [[2, 3], [5, 1]]\n",
    "x = torch.tensor(data)\n",
    "\n",
    "# Shape and Datatype\n",
    "print(x.shape)  # Output: torch.Size([2, 2])\n",
    "print(x.dtype)  # Output: torch.int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing and Slicing\n",
    "print(x[1, 0])  # Accessing individual element, Output: tensor(5)\n",
    "print(x[:, 1])  # Selecting a column, Output: tensor([3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Operations\n",
    "y = torch.rand(2, 2)\n",
    "print(x + y)  # Element-wise addition\n",
    "print(torch.mul(x, 2))  # Scalar multiplication\n",
    "\n",
    "# Transformations\n",
    "z = x.t()  # Matrix transpose\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction Operations\n",
    "print(x.sum())  # Sum of all elements\n",
    "# Convert to torch.float32, then get mean across rows\n",
    "print(\"Mean across rows:\")\n",
    "print(x.float().mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Operations with Broadcasting\n",
    "b = torch.ones(2)  # Tensor of ones with shape (2,)\n",
    "print(x + b)  # Broadcasting 'b' across rows\n",
    "\n",
    "# Random Number Generation\n",
    "random_tensor = torch.rand(3, 2)\n",
    "print(random_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping using view\n",
    "reshaped_tensor = random_tensor.view(6, 1)  # Reshape to 6 rows, 1 column\n",
    "print(reshaped_tensor)\n",
    "print(reshaped_tensor.shape)  # Output: torch.Size([6, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Operations (if you have a GPU available)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = x.to(device)\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    GPU_tensor = x + y  # Calculations performed on GPU\n",
    "\n",
    "# Move the tensor to the CPU\n",
    "cpu_tensor = GPU_tensor.cpu()\n",
    "\n",
    "# Convert the tensor to a NumPy array\n",
    "numpy_array = cpu_tensor.numpy()\n",
    "\n",
    "print(\"NumPy array:\", numpy_array)\n",
    "print(\"Type:\", type(numpy_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unterschied zwischen PyTorch und TensorFlow: Dynamische vs. Statische Graphen\n",
    "\n",
    "PyTorch erstellt den Berechnungsgraphen **dynamisch zur Laufzeit** (\"on the fly\"), was sich erheblich von der traditionellen Funktionsweise von TensorFlow (vor TensorFlow 2.x) unterscheidet. Hier eine Erläuterung, was das bedeutet, der Vergleich mit TensorFlow sowie die Vorteile dieses Ansatzes.\n",
    "\n",
    "\n",
    "## **PyTorch: Dynamische Berechnungsgraphen**\n",
    "\n",
    "- **Graph-Erstellung zur Laufzeit:** PyTorch baut den Berechnungsgraphen dynamisch auf, während der Code ausgeführt wird.\n",
    "- **Automatische Differentiation:** Der Graph wird für das Backpropagation-Verfahren genutzt, um Gradienten zu berechnen, und danach verworfen.\n",
    "- **Intuitives Debugging:** Da der Graph während der Ausführung erstellt wird, ist das Debugging in PyTorch einfacher und fühlt sich natürlicher an.\n",
    "\n",
    "### Beispiel:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * 2\n",
    "z = y.sum()\n",
    "z.backward()  # Gradientenberechnung\n",
    "print(x.grad)  # Gradienten von z nach x_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Operation $y = x \\times 2$\n",
    "- Für jedes Element $x_i$ im Tensor $x$ gilt: $y_i = 2 \\cdot x_i$\n",
    "- Die Ableitung von $y_i$ nach $x_i$ ist: $ \\frac{\\partial y_i}{\\partial x_i} = 2 $\n",
    "\n",
    "2. Operation $z = \\text{sum}(y)$\n",
    "- $z$ ist die Summe aller Elemente in $y$:   $ z = y_1 + y_2 + y_3 = 2x_1 + 2x_2 + 2x_3 $\n",
    "- Die Ableitung von $z$ nach $x_i$ ist: $\\frac{\\partial z}{\\partial x_i} = 2 $\n",
    "\n",
    "3. Backpropagation\n",
    "- Während der Ausführung von `z.backward()` berechnet PyTorch die Gradienten:\n",
    "  $$ \\frac{\\partial z}{\\partial x} = [2, 2, 2] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Der Graph wird dynamisch aufgebaut, wenn die Operationen ausgeführt werden (`x * 2`, `sum()`), was ihn flexibel und benutzerfreundlich macht.\n",
    "\n",
    "\n",
    "## **TensorFlow (Statischer Graph in TensorFlow 1.x)**\n",
    "\n",
    "- **Statischer Berechnungsgraph:** In TensorFlow 1.x definiert man den Berechnungsgraphen explizit und führt ihn später in einer Session aus.\n",
    "- **Graph-Ausführung:** Dieser Ansatz folgt oft einem Zwei-Schritt-Prozess:\n",
    "  1. **Graph erstellen** (Berechnung definieren).\n",
    "  2. **Graph ausführen** in einer Session, um Ergebnisse zu erhalten.\n",
    "\n",
    "### Beispiel in TensorFlow 1.x:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = x * 2\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(y, feed_dict={x: [1.0, 2.0, 3.0]})\n",
    "    print(result)\n",
    "```\n",
    "- Diese Trennung von Definition und Ausführung machte das Debugging schwieriger und weniger intuitiv.\n",
    "\n",
    "\n",
    "## **TensorFlow 2.x und Eager Execution**\n",
    "\n",
    "- TensorFlow 2.x führte **Eager Execution** als Standard ein, was ähnlich wie die dynamischen Berechnungsgraphen in PyTorch funktioniert.\n",
    "- Eager Execution ermöglicht die sofortige Ausführung von Operationen, ohne dass ein Graph explizit definiert und ausgeführt werden muss.\n",
    "\n",
    "### Beispiel in TensorFlow 2.x:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant([1.0, 2.0, 3.0])\n",
    "y = x * 2\n",
    "print(y.numpy())\n",
    "```\n",
    "- Dieser Stil ist sehr ähnlich zum Ansatz von PyTorch.\n",
    "\n",
    "\n",
    "## **Vorteile der dynamischen Graph-Erstellung**\n",
    "\n",
    "1. **Flexibilität und dynamische Kontrolle:**\n",
    "   - PyTorch erlaubt dynamische Änderungen am Berechnungsgraphen während der Laufzeit, was es für Modelle mit variablen Architekturen (z. B. RNNs mit variablen Eingabelängen) ideal macht.\n",
    "\n",
    "2. **Einfacheres Debugging:**\n",
    "   - Fehler in PyTorch treten während der Ausführung auf und nicht während eines separaten Graph-Kompilierungsschritts, was die Fehlersuche erleichtert.\n",
    "\n",
    "3. **Pythonischer Workflow:**\n",
    "   - Die Schnittstelle von PyTorch fühlt sich natürlicher und intuitiver für Python-Entwickler an.\n",
    "\n",
    "4. **Schnelles Prototyping:**\n",
    "   - Änderungen am Modell können sofort implementiert und getestet werden, ohne den Berechnungsgraphen neu definieren oder kompilieren zu müssen.\n",
    "\n",
    "\n",
    "\n",
    "## **Vergleichszusammenfassung**\n",
    "\n",
    "| Merkmal                       | PyTorch (Dynamischer Graph)     | TensorFlow 1.x (Statischer Graph) | TensorFlow 2.x (Eager Execution) |\n",
    "|-------------------------------|----------------------------------|------------------------------------|-----------------------------------|\n",
    "| Graph-Erstellung              | Dynamisch (zur Laufzeit)         | Statisch (vordefiniert)            | Dynamisch (Eager-Standard)        |\n",
    "| Debugging                     | Intuitiv                        | Schwierig                         | Intuitiv                          |\n",
    "| Flexibilität                  | Hoch                            | Niedrig                           | Hoch                              |\n",
    "| Laufzeitgeschwindigkeit (Training) | Leicht langsamer (keine Optimierung) | Potentiell schneller (optimiert)   | Vergleichbar mit PyTorch          |\n",
    "\n",
    "\n",
    "\n",
    "## **Warum dynamische Graphen verwenden?**\n",
    "\n",
    "Dynamische Berechnungsgraphen sind besonders vorteilhaft bei:\n",
    "\n",
    "- **Variablen Eingabegrößen:** Wie bei der Textverarbeitung oder dynamischen neuronalen Netzen (z. B. rekursiven Netzen).\n",
    "- **Komplexen, konditionalen Modellen:** Bei denen Verzweigungen der Berechnung von Laufzeitwerten abhängen.\n",
    "- **Interaktiver Entwicklung:** Für Forschung und schnelles Experimentieren.\n",
    "\n",
    "Statische Graphen können jedoch immer noch vorteilhaft für Produktionsumgebungen sein, die strikte Optimierung und Effizienz erfordern, weshalb Frameworks wie TensorFlow beide Ansätze unterstützen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
