{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942e18d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch  </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN01/1.2-BiologicalAndArtificialNeurons.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from IPython.display import Image, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f54cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Von biologischen und künstlichen Neuronen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4247467e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [History of Artificial Intelligence](https://qbi.uq.edu.au/brain/intelligent-machines/history-artificial-intelligence)\n",
    "- [A “weird” introduction to Deep Learning (towardsdatascience)](https://towardsdatascience.com/a-weird-introduction-to-deep-learning-7828803693b0)\n",
    "- [History and Background of Deep Learning (beamlab)](http://beamlab.org/deeplearning/2017/02/23/deep_learning_101_part1.html#)\n",
    "- [Deep Learning Introduction](https://witanworld.com/article/2019/10/15/deep-learning-introduction/)\n",
    "- [Aurélien Géron: Praxiseinstieg Machine Learning mit Scikit-Learn und Tensorflow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375cf34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Santiago Felipe Ramón y Cajal\n",
    "\n",
    "Bevor wir künstliche Neuronen besprechen, schauen wir uns kurz ein biologisches Neuron an. Es ist eine ungewöhnlich aussehende Zelle, wie man sie vor allem im zerebralen Kortex von Tieren antrifft (z.B. in Ihrem Gehirn). \n",
    "\n",
    "\n",
    "**Santiago Felipe Ramón y Cajal** (1852-1934) war ein spanischer Mediziner und Histologe. Er erhielt 1906 den Nobelpreis für Physiologie oder Medizin gemeinsam mit dem italienischen Mediziner und Physiologen Camillo Golgi. Ramón y Cajals bedeutendste Arbeiten waren Untersuchungen der Feinstruktur des Zentralnervensystems. Cajal verwendete eine histologische Färbetechnik, die kurz zuvor von Camillo Golgi entwickelt worden war. Golgi fand heraus, dass, wenn man Gehirngewebe mit einer Silbernitrat-Lösung behandelte, eine verhältnismäßig kleine Anzahl von Neuronen im Gehirn dunkel gefärbt wurde. Dieses erlaubte Golgi, die Struktur einzelner Neuronen im Detail zu klären und führte ihn zu dem Schluss, dass Nervengewebe ein zusammenhängendes Geflecht (oder Netz) aus untereinander verbundenen Zellen bildet – ganz ähnlich, wie es vom Kreislaufsystem bekannt war.\n",
    "\n",
    "[Quelle: Wikipedia](https://de.wikipedia.org/wiki/Santiago_Ram%C3%B3n_y_Cajal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65dfd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/PurkinjeCell.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0debabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/Cajal-Restored.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a4104",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ramón y Cajal betrieb intensive Studien zum Nachweis qualitativer Unterschiede zwischen den Gehirnen von Menschen und Tieren. Hierzu stellte er die Hypothese auf: \n",
    "\n",
    "**Die funktionelle Überlegenheit des menschlichen Gehirns hängt sehr eng mit dem erstaunlichen Überfluss und der ungewöhnlichen Formenvielfalt der sogenannten Neuronen mit kurzen Axonen zusammen.**\n",
    "\n",
    "Das war der Kern des Problems der Grosshirnrinde, und schliesslich musste er eingestehen: \n",
    "\n",
    "**„… die unbeschreibliche Komplexität der Struktur der grauen Substanz ist so vertrackt, dass sie der hartnäckigen Neugier von Forschern trotzt und noch viele Jahrhunderte trotzen wird.“**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5158b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Biologische Neuronen\n",
    "\n",
    "Bevor wir künstliche Neuronen besprechen, schauen wir uns kurz ein biologisches\n",
    "Neuron an. Es ist eine ungewöhnlich aussehende Zelle, wie man sie vor allem im zerebralen Kortex von Tieren antrifft (z.B. in Ihrem Gehirn). \n",
    "- Sie besteht aus einem Zellkörper mit dem Zellkern und den meisten komplexen Bestandteilen einer Zelle, vielen verzweigten Auswüchsen, den **Dendriten**, sowie einem sehr langen Auswuchs, dem **Axon**. \n",
    "- Die Länge des Axons kann einigen oder bis zu Zehntausenden Längen des Zellkörpers entsprechen. \n",
    "- Am Ende teilt sich das Axon in feine Verästelungen, die **Telodendria** auf. \n",
    "- An der Spitze dieser Verästelungen befinden sich winzige Strukturen, die **synaptischen Verbindungen** (oder einfach\n",
    "**Synapsen**), die mit den Dendriten (oder Zellkörpern) anderer Neuronen verbunden sind. \n",
    "\n",
    "Biologische Neuronen erhalten über die Synapsen kurze elektrische Impulse oder Signale von anderen Neuronen. Erhält ein Neuron innerhalb weniger Millisekunden eine ausreichende Anzahl Signale, feuert es sein eigenes Signal ab.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e723bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/Complete_neuron_cell_diagram_de.svg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee0e35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "[Quelle: Wikipedia](https://de.wikipedia.org/wiki/Nervenzelle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd2c49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Eine Kurze Geschichte des Deep Learning\n",
    "\n",
    "### Warren McCulloch und Walter Pitts Neuron\n",
    "Überraschenderweise gibt es ANNs schon eine ganze Weile: Das erste Mal wurden Sie 1943 vom Neurophysiologen **Warren McCulloch** und vom Mathematiker **Walter Pitts** erwähnt. \n",
    "In ihrem wegweisenden Artikel (https://goo.gl/Ul4mxW) *»A Logical Calculus of Ideas Immanent in Nervous Activity«* stellen McCulloch und Pitts ein vereinfachtes rechnerisches Modell vor, nach dem biologische Neuronen im Gehirn von Tieren zusammenarbeiten könnten, um komplexe Berechnungen mithilfe von Aussagenlogik durchzuführen. Dies war die erste Architektur eines künstlichen neuronalen Netzwerks. Seitdem wurden viele weitere Architekturen erfunden, wie wir noch sehen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133ff55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Das Perzeptron\n",
    "Das **1957** von **Frank Rosenblatt** erfundene *Perzeptron* gehört zu den einfachsten Architekturen neuronaler Netze. Es baut auf einem leicht unterschiedlichen künstlichen Neuron auf, der **Linear Threshold Unit (LTU):**\n",
    "- Die Ein- und Ausgaben sind nun Zahlen (anstatt binäre Ein-/Aus-Werte), und zu jeder Eingabeleitung gehört ein Gewicht. Die LTU berechnet eine gewichtete Summe ihrer Eingaben \n",
    "\n",
    "$$ z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\mathbf{w}^T \\cdot \\mathbf{x}$$\n",
    "\n",
    "und wendet dann eine Aktivierungsfunktion\n",
    "auf diese Summe an und gibt das Ergebnis aus:\n",
    "$$Z=h_w(x) = \\mathrm{LTU} \\left(\\mathbf{w}^T \\cdot \\mathbf{x}\\right).$$\n",
    "\n",
    "Quelle: https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/rosenblatt_main.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/Rosenblatt_Perceptron_1960.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ed9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/ArtificialNeuronModel_english.png\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e25f4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Diese frühen Modelle bestehen nur aus einer sehr kleinen Menge virtueller Neuronen, die mit einer Zufallszahl, den Gewichten, verbunden werden. \n",
    "- Diese Gewichte bestimmen, wie jedes simulierte Neuron Informationen zwischen ihnen überträgt, d. h. wie jedes Neuron reagiert, mit einem Wert zwischen 0 und 1. \n",
    "- Mit dieser mathematischen Darstellung kann die neuronale Ausgabe eine Kante oder eine Form aus dem Bild oder ein bestimmtes Energieniveau bei einer Frequenz in einem Phonem erkennen. \n",
    "- Die vorherige Abbildung zeigt ein mathematisch formuliertes künstliches Neuron, bei dem der Eingang den Dendriten entspricht, eine **Aktivierungsfunktion** steuert, ob das Neuron feuert, wenn ein **Schwellenwert** erreicht wird, und der Ausgang entspricht dem *Axon*. \n",
    "- Frühe neuronale Netze konnten jedoch nur eine sehr begrenzte Anzahl von Neuronen auf einmal simulieren, so dass mit einer solch einfachen Architektur nicht viele Muster erkannt werden können. Diese Modelle blieben bis in die 1970er Jahre bestehen\n",
    "\n",
    "Im Unterschied dazu, sind biologische Modelle von Neuronen viel komplizierter, siehe https://en.wikipedia.org/wiki/Biological_neuron_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bb1ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation\n",
    "\n",
    "**1960**: Henry J. Kelley wird das Verdienst zugeschrieben, 1960 die Grundlagen für ein **kontinuierliches Back-Propagation-Modell** entwickelt zu haben. Im Jahr 1962 wurde eine einfachere Version, die nur auf der Kettenregel basiert, von **Stuart Dreyfus** entwickelt. Das Konzept der Back-Propagation (die Rückwärtspropagierung von Fehlern zu Trainingszwecken) existierte zwar schon in den frühen 1960er Jahren, war aber unhandlich und ineffizient und wurde erst 1985 nützlich.\n",
    "\n",
    "Die ersten Bemühungen zur Entwicklung von Deep Learning-Algorithmen stammen von **Alexey Grigoryevich Ivakhnenko** (entwickelte die Gruppenmethode der Datenverarbeitung) und Valentin Grigorʹevich Lapa (Autor von Cybernetics and Forecasting Techniques) im Jahr 1965. Sie verwendeten Modelle mit polynomialen Aktivierungsfunktionen, die dann statistisch ausgewertet wurden. Von jeder Schicht wurden dann die besten statistisch ausgewählten Merkmale an die nächste Schicht weitergeleitet (ein langsamer, manueller Prozess)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b27de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Erster KI-Winter\n",
    "\n",
    "**1970**: In den 1970er Jahren setzte der erste **KI-Winter** ein, das Ergebnis von Versprechungen, die nicht eingehalten werden konnten. Die Auswirkungen dieser fehlenden Finanzierung schränkten sowohl die DL- als auch die KI-Forschung ein. Glücklicherweise gab es Einzelpersonen, die die Forschung ohne Finanzierung weiterführten.\n",
    "\n",
    "In einer Monografie aus dem Jahr 1969 mit dem Titel *Perceptrons* hoben **Marvin Minsky** und **Seymour Papert** eine Reihe ernster Nachteile von Perzeptrons hervor, insbesondere ihr Versagen bei einer Reihe trivialer Probleme (z.B. das exklusive OR\n",
    "(XOR) als Klassifikationsaufgabe; . Natürlich gilt dies auch für jedes andere lineare Klassifikationsmodell (wie\n",
    "die logistische Regression), aber die Wissenschaft hatte wesentlich grössere Hoffnungen in Perzeptrons gesetzt. Daher war die Enttäuschung groß, und in der Folge wandten sich viele Wissenschaftler vom Konnektionismus insgesamt ab (d.h. dem\n",
    "Studium neuronaler Netze), um sich übergeordneten Aufgabenstellungen wie Logik, Problemlösung und Suche zuzuwenden.\n",
    "\n",
    "Es stellt sich aber heraus, dass sich einige dieser Beschränkungen aufheben lassen,\n",
    "indem man **mehrere Perzeptrons in Reihe schaltet**. Das dabei entstehende ANN bezeichnet man als *mehrschichtiges Perzeptron (MLP)*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23614cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/XOR.png\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4112ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quelle: Géron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b9bbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Erstes Faltungsnetzwerk von Kunihiko Fukushima\n",
    "\n",
    "Die ersten \"Faltungsneuronalen Netzwerke\" wurden von [**Kunihiko Fukushima**](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf) eingesetzt. Fukushima entwarf neuronale Netzwerke mit mehreren Pooling- und Faltungsschichten. Im Jahr 1979 entwickelte er ein künstliches neuronales Netzwerk namens **Neocognitron**, das ein hierarchisches, mehrschichtiges Design verwendete. Dieses Design ermöglichte es dem Computer zu \"lernen\", visuelle Muster zu erkennen. Die Netzwerke ähnelten modernen Versionen, wurden aber mit einer Verstärkungsstrategie trainiert, bei der wiederkehrende Aktivierungen in mehreren Schichten vorgenommen wurden, die mit der Zeit an Stärke gewannen. Zusätzlich erlaubte Fukushimas Design, wichtige Merkmale manuell anzupassen, indem das \"Gewicht\" bestimmter Verbindungen erhöht wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80337f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/Kunihiko_Fukushima.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662de231",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/The-architecture-of-the-neocognitron.png\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4983099",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Viele der Konzepte von Neocognitron werden weiterhin verwendet. \n",
    "- Die Verwendung von **Top-Down-Verbindungen** und neue Lernmethoden haben es ermöglicht, eine Vielzahl von neuronalen Netzwerken zu realisieren. \n",
    "- Wenn mehr als ein Muster gleichzeitig präsentiert wird, kann das Modell der **selektiven Aufmerksamkeit** einzelne Muster trennen und erkennen, indem es seine Aufmerksamkeit von einem zum anderen verschiebt. (Der gleiche Prozess, den viele von uns beim Multitasking verwenden). \n",
    "- Ein modernes Neocognitron kann nicht nur Muster mit fehlenden Informationen erkennen (z. B. eine unvollständige Zahl 5), sondern kann das Bild auch vervollständigen, indem es die fehlenden Informationen hinzufügt. Dies könnte man als **Inferenz** bezeichnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520cf8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Langsam entwickelte sich die Backpropagation in den 1970er Jahren deutlich weiter, wurde aber erst 1985 auf neuronale Netze angewendet. Mitte der 1980er Jahre trugen Hinton und andere dazu bei, das Interesse an neuronalen Netzen mit so genannten tiefen Modellen wiederzubeleben, die viele Schichten von Neuronen besser nutzen, d. h. mit mehr als zwei versteckten Schichten. Zu diesem Zeitpunkt demonstrierten **Geoffrey Hinton und seine Co-Autoren** (https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf), dass Backpropagation in einem neuronalen Netzwerk zu einer interessanten repräsentativen Verteilung führen kann. \n",
    "\n",
    "Im Jahr 1989 demonstrierte **Yann LeCun** (http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf) in den Bell Labs die erste praktische Anwendung der Backpropagation. Er setzte Backpropagation in faltbaren neuronalen Netzen ein, um handgeschriebene Ziffern zu verstehen, und seine Idee entwickelte sich schließlich zu einem System, das die Nummern von handgeschriebenen Schecks liest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24198d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/nn_timeline.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f71fb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Zweiter KI-Winter\n",
    "\n",
    "Dies ist auch die Zeit des **2. KI-Winters (1985-1990)**. Im Jahr 1984 warnten die beiden führenden KI-Forscher Roger Schank und **Marvin Minsky** die Wirtschaft, dass die Begeisterung für KI ausser Kontrolle geraten war. Obwohl mehrschichtige Netzwerke komplizierte Aufgaben lernen konnten, war ihre Geschwindigkeit sehr langsam und die Ergebnisse waren nicht sehr beeindruckend. Als daher andere, einfachere, aber effektivere Methoden, wie z. B. Support-Vektor-Maschinen, erfunden wurden, stellten Regierung und Risikokapitalgeber ihre Unterstützung für neuronale Netzwerke ein. Nur drei Jahre später fiel die milliardenschwere KI-Industrie auseinander."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af5b0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die frühen Erfolge von ANNs bis zu den 1960ern führten zur verbreiteten Annahme,\n",
    "dass wir uns schon bald mit wirklich intelligenten Maschinen unterhalten\n",
    "würden. Als klar wurde, dass dieses Versprechen nicht eingelöst werden würde (zumindest\n",
    "für eine lange Zeit), flossen Forschungsgelder in andere Richtungen, und\n",
    "für ANNs brach ein langes, dunkles Zeitalter an. In den frühen 1980ern lebte das\n",
    "Interesse an ANNs mit der Entdeckung neuer Netzwerkarchitekturen wieder auf.\n",
    "Aber in den 1990ern bevorzugten die meisten Forscher andere mächtige Machine-\n",
    "Learning-Verfahren wie Support Vector Machines, da diese bessere\n",
    "Ergebnisse und ein solideres theoretisches Fundament versprachen. Schließlich\n",
    "erleben wir heute eine weitere Renaissance der ANNs. Wird auch diese Welle\n",
    "wie die vorigen wieder verebben? Es gibt gute Gründe anzunehmen, dass es diesmal\n",
    "anders ist und der Einfluss auf unseren Alltag beträchtlich sein wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/DeepLearningPipeline_TowardsDataScience.png\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba0670c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vanishing Gradient Problem\n",
    "\n",
    "**2000**: Um das Jahr 2000 erschien das **Vanishing Gradient Problem**. \n",
    "- Es wurde entdeckt, dass \"Features\" (Lektionen), die in unteren Schichten gebildet wurden, von den oberen Schichten nicht gelernt wurden, weil kein Lernsignal diese Schichten erreichte. \n",
    "- Dies war kein grundsätzliches Problem für alle neuronalen Netze, sondern nur für solche mit gradientenbasierten Lernmethoden. - Die Quelle des Problems stellte sich als **bestimmte Aktivierungsfunktionen** heraus. Eine Reihe von Aktivierungsfunktionen verdichteten ihre Eingabe, was wiederum den Ausgabebereich in einer etwas chaotischen Weise reduzierte.\n",
    "- Dadurch entstanden grosse Bereiche der Eingabe, die auf einen extrem kleinen Bereich abgebildet wurden. In diesen Bereichen der Eingabe wird eine grosse Änderung auf eine kleine Änderung in der Ausgabe reduziert, was zu einem verschwindenden Gradienten führt. \n",
    "- Zwei Lösungen, die zur Lösung dieses Problems verwendet wurden, waren das **schichtweise Vortraining** und die Entwicklung des **LSTM-Netzen** (Langzeitgedächtnis).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05badbdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Big Data\n",
    "\n",
    "Im Jahr 2001 beschrieb ein Forschungsbericht der META Group (jetzt Gartner) die Herausforderungen und Chancen des Datenwachstums als dreidimensional. Der Bericht beschrieb das zunehmende Datenvolumen und die steigende Geschwindigkeit der Daten als zunehmende Vielfalt der Datenquellen und -typen. Dies war ein Aufruf, sich auf den Ansturm von **Big Data** vorzubereiten, der gerade erst begann.\n",
    "\n",
    "Im Jahr 2009 rief **Fei-Fei Li**, eine KI-Professorin in Stanford, **ImageNet** ins Leben, eine kostenlose Datenbank mit mehr als 14 Millionen beschrifteten Bildern. Das Internet ist und war voll von unbeschrifteten Bildern. Beschriftete Bilder wurden benötigt, um neuronale Netze zu \"trainieren\". Professor Li sagte: \"Unsere Vision war, dass Big Data die Art und Weise, wie maschinelles Lernen funktioniert, verändern würde. Daten treiben das Lernen an.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa713e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPUs (NVIDIA)\n",
    "\n",
    "Im Jahr 2011 hatte sich die Geschwindigkeit von GPUs deutlich erhöht, so dass es möglich war, faltige neuronale Netze \"ohne\" das schichtweise Vortraining zu trainieren. Mit der gesteigerten Rechengeschwindigkeit wurde es offensichtlich, dass Deep Learning erhebliche Vorteile in Bezug auf Effizienz und Geschwindigkeit hatte. Ein Beispiel ist **AlexNet**, ein faltungsneuronales Netz, dessen Architektur in den Jahren 2011 und 2012 mehrere internationale Wettbewerbe gewann. Es wurden gleichgerichtete Lineareinheiten verwendet, um die Geschwindigkeit und den Dropout zu verbessern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f16145",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81b16a86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Warum jetzt?\n",
    "\n",
    "Viele der Kernkonzepte für Deep Learning waren bereits in den 80er oder 90er Jahren vorhanden, was ist also in den letzten 5-7 Jahren passiert, das die Dinge verändert hat? Obwohl es viele Faktoren gibt, scheinen die beiden wichtigsten Komponenten die Verfügbarkeit von massiven gelabelten Datensätzen und GPU-Computing zu sein. Hier eine Übersicht über die Faktoren, die bei der Deep-Learning-Revolution eine Rolle gespielt zu haben scheinen:\n",
    "\n",
    "1. Das Auftauchen **grosser, qualitativ hochwertiger gelabelter Datensätze** - Daten zusammen mit GPUs erklären wahrscheinlich den Großteil der Verbesserungen, die wir gesehen haben. Deep Learning ist ein Ofen, der eine Menge Brennstoff braucht, um weiter zu brennen, und wir haben endlich genug Brennstoff.\n",
    "2. **Massiv paralleles Rechnen mit GPUs** - Es hat sich herausgestellt, dass neuronale Netze eigentlich nur ein Haufen von Fliesskommaberechnungen sind, die man parallel ausführen kann. Es hat sich auch herausgestellt, dass GPUs für diese Art von Berechnungen hervorragend geeignet sind. Die Umstellung von CPU-basiertem Training auf GPU-basiertes Training hat zu massiven Geschwindigkeitssteigerungen für diese Modelle geführt, was es uns ermöglicht hat, größere und tiefere Berechnungen mit mehr Daten durchzuführen.\n",
    "3. **Backprop-freundliche Aktivierungsfunktionen** - Der Übergang weg von sättigenden Aktivierungsfunktionen wie tanh und der logistischen Funktion hin zu Dingen wie ReLU hat das Problem des verschwindenden Gradienten gemildert.\n",
    "4. **Verbesserte Architekturen** - Resnets, Inception-Module und Highway-Netze sorgen für einen reibungslosen Ablauf der Gradienten und ermöglichen es uns, die Tiefe und Flexibilität des Netzes zu erhöhen.\n",
    "5. **Software-Plattformen** - Frameworks wie Tensorflow, Theano, Chainer und mxnet, die eine automatische Differenzierung ermöglichen, erlauben eine nahtlose GPU-Berechnung und machen das Protoyping schneller und weniger fehleranfällig. So können Sie sich auf Ihre Modellstruktur konzentrieren, ohne sich um Low-Level-Details wie Gradienten und GPU-Management kümmern zu müssen.\n",
    "5. **Neue Regularisierungstechniken** - Techniken wie Dropout, Batch-Normalisierung und Datenaugmentation ermöglichen es uns, immer größere Netzwerke ohne (oder mit weniger) Overfitting zu trainieren Robuste Optimierer - Modifikationen der `SGD`-Prozedur, einschliesslich `Momentum, RMSprop` und `ADAM`, haben dazu beigetragen, jedes letzte Prozent aus Ihrer Verlustfunktion herauszuholen.\n",
    "6. **Lokale Optima** - Einige theoretische Einschränkungen von ANNs haben sich als Vorteile herausgestellt. Beispielsweise ging man davon aus, dass die Algorithmen zum Trainieren von ANNs wegen ihrer Neigung zu **lokalen Optima** zum Scheitern verurteilt wären. Diese sind aber in der Praxis selten (oder liegen zumindest nahe genug am globalen Optimum).\n",
    "7. **Finanzierung** - ANNs befinden sich in einem förderlichen Kreislauf von finanzieller Unterstützung und Fortschritt. Faszinierende auf ANNs basierende Produkte schaffen es regelmässig in die Schlagzeilen, wodurch sie weitere Aufmerksamkeit und Förderung anziehen, was wiederum weitere Fortschritte und noch faszinierende Produkte nach sich zieht.\n",
    "\n",
    "Die meisten dieser Verbesserungen wurden durch die empirische Leistung bei einer Reihe von Standard-Benchmarks erzielt. Es gibt nicht viele theoretische Begründungen (obwohl es einige gibt) für viele dieser Verfahren, was zu der folgenden Hypothese führt:\n",
    "\n",
    "**Deep-Learning-Hypothese: Der Erfolg von Deep Learning ist grösstenteils ein Erfolg der Technik.**\n",
    "\n",
    "Ich denke nicht, dass dies eine kontroverse Position ist, und es soll den Erfolg von Deep Learning nicht schmälern, aber ich denke, es ist eine faire Charakterisierung, wie der Stand der Technik vorangetrieben wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ffa46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/ChronologieNN.png\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7eb3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Top Deep Learning Researchers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc6aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/top-12-deeplearning-leaders-sept2019.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cd601",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "- [Quelle: Matthew Dearing: 12 Deep Learning Researchers and Leaders, KDnuggets](https://www.kdnuggets.com/2019/09/12-deep-learning-research-leaders.html)\n",
    "- https://laptrinhx.com/top-10-deep-learning-researchers-who-are-re-defining-its-application-areas-1508484677/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abafa7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Andrew Ng, Gründer und CEO von Landing AI, Gründer von deeplearning.ai.\n",
    "</strong>(<a href=\"https://twitter.com/andrewyng\" target=\"_blank\" rel=\"noopener\">@andrewyng</a> 435K | <a href=\"https://www.linkedin.com/in/andrewyng/detail/recent-activity/\" target=\"_blank\" rel=\"noopener\">LinkedIn Activity</a> | <a href=\"https://www.facebook.com/andrew.ng.96\" target=\"_blank\" rel=\"noopener\">Facebook</a> | <a href=\"https://scholar.google.com/citations?hl=en&amp;user=mG4imMEAAAAJ\" target=\"_blank\" rel=\"noopener\">Google Scholar</a> | <a href=\"https://arxiv.org/search/?query=%22Andrew+Ng%22&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50\" target=\"_blank\" rel=\"noopener\">arXiv</a>)</li>\n",
    "\n",
    "Andrew war Mitbegründer und Leiter von Google Brain und war VP und Chief Scientist bei Baidu. Im Jahr 2011 leitete er die Entwicklung der wichtigsten MOOC-Plattform (Massive Open Online Courses) der Stanford University und unterrichtete einen Online-Kurs für maschinelles Lernen, der über 100.000 Studenten angeboten wurde, was zur Gründung von Coursera führte, deren Co-Vorsitzender und Mitbegründer er ist, sowie ein ausserordentlicher Professor an der Stanford University.\n",
    "\n",
    "### Fei-Fei Li, Professor für Computerwissenschaften an der Stanford University.\n",
    "</strong>(<a href=\"https://twitter.com/drfeifei\" target=\"_blank\" rel=\"noopener\">@drfeifei</a> 324K | <a href=\"https://scholar.google.com/citations?hl=en&amp;user=rDfyQnIAAAAJ\" target=\"_blank\" rel=\"noopener\">Google Scholar</a> | <a href=\"https://arxiv.org/search/?query=%22Fei-Fei+Li%22&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50\" target=\"_blank\" rel=\"noopener\">arXiv</a>)</li>\n",
    "\n",
    "Als erste Sequoia-Professorin im Fachbereich Informatik an der Stanford University und Co-Direktorin von Stanfords Human-Centered AI Institute war Dr. Li von 2013 bis 2018 Direktorin des Stanford AI Lab. Während eines Sabbaticals in den Jahren 2017 und 2018 war sie Vizepräsidentin bei Google und arbeitete als Chief Scientist of AI/ML bei Google Cloud.  Dr. Lis Forschung umfasst maschinelles Lernen, Deep Learning, Computer Vision und kognitive und computergestützte Neurowissenschaften mit fast 200 wissenschaftlichen Artikeln, die in hochrangigen Fachzeitschriften und Konferenzen veröffentlicht wurden. Dr. Li ist auch der Erfinder von ImageNet und der ImageNet Challenge und ist eine führende nationale Stimme für die Förderung von Vielfalt in MINT und KI. Watch this [Ted Talk](https://www.youtube.com/embed/40riCqvRoMs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871d7e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo(\"40riCqvRoMs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a84c20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Andrej Karpathy, Senior Director of Artifical Intelligence bei Tesla.\n",
    "</strong>(<a href=\"https://twitter.com/karpathy\" target=\"_blank\" rel=\"noopener\">@karpathy</a> 231K | <a href=\"https://scholar.google.com/citations?hl=en&amp;user=l8WuQJgAAAAJ\" target=\"_blank\" rel=\"noopener\">Google Scholar</a> | <a href=\"https://arxiv.org/search/?query=%22Andrej+Karpathy%22&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50\" target=\"_blank\" rel=\"noopener\">arXiv</a>)</li>\n",
    "\n",
    "Bei Tesla leitet Andrej das Team, das für alle neuronalen Netzwerke des Autopiloten verantwortlich ist. Zuvor war er Research Scientist bei OpenAI und arbeitete an Deep Learning in den Bereichen Computer Vision, generative Modellierung und Reinforcement Learning. Während er in Stanford unter Fei-Fei Li promovierte, absolvierte Andrej zwei Praktika bei Google, um an gross angelegtem Feature-Learning über YouTube-Videos zu arbeiten, sowie ein Praktikum bei DeepMind, wo er an Deep Reinforcement Learning arbeitete.\n",
    "\n",
    "Watch on Youtube](https://youtu.be/_au3yw46lcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b87c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"Ucp0TTmvqOE?t=6565\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce36ef8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demis Hassabis, Gründer und CEO von DeepMind.\n",
    "(<a href=\"https://twitter.com/demishassabis\" target=\"_blank\" rel=\"noopener\">@demishassabis</a> 171K | <a href=\"https://scholar.google.com/citations?hl=en&amp;user=dYpPMQEAAAAJ\" target=\"_blank\" rel=\"noopener\">Google Scholar</a> | <a href=\"https://arxiv.org/search/?query=%22Demis+Hassabis%22&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50\" target=\"_blank\" rel=\"noopener\">arXiv</a>)</li>\n",
    "\n",
    "Demis gründete DeepMind im Jahr 2010 nach erfolgreichen Karrieren in der Wissenschaft und der Entwicklung von Computerspielen mit. Als Schach-Wunderkind entwarf er im Alter von 17 Jahren das millionenfach verkaufte und preisgekrönte Spiel Theme Park. Nach seinem Abschluss an der Universität Cambridge gründete er die bahnbrechende Videospielfirma Elixir Studios und promovierte in kognitiven Neurowissenschaften am UCL. Heute ist Demis Fellow der Royal Society, der Royal Academy of Engineering und der Royal Society of Arts und wurde 2017 in die Time 100-Liste der einflussreichsten Menschen aufgenommen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e4ae45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ian Goodfellow, Director of Machine Learning bei Apple.\n",
    "(<a href=\"https://twitter.com/goodfellow_ian\" target=\"_blank\" rel=\"noopener\">@goodfellow_ian</a> 157K | <a href=\"https://www.facebook.com/ia3n.goodfellow\" target=\"_blank\" rel=\"noopener\">Facebook</a> | <a href=\"https://scholar.google.com/citations?hl=en&amp;user=iYN86KEAAAAJ\" target=\"_blank\" rel=\"noopener\">Google Scholar</a> | <a href=\"https://arxiv.org/search/?query=%22Ian+Goodfellow%22&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50\" target=\"_blank\" rel=\"noopener\">arXiv</a>)</li>\n",
    "\n",
    "Ian Goodfellow erwarb seinen B.S. und M.S. in Informatik an der Stanford University bei Andrew Ng, gefolgt von einem Ph.D. in Maschinellem Lernen an der Université de Montréal bei Yoshua Bengio und Aaron Courville. Anschliessend arbeitete er bei Google im Forschungsteam von Google Brain und wechselte dann zum neu gegründeten OpenAI-Institut, bevor er 2017 zu Google Research zurückkehrte. Bekannt für die Erfindung generativer [adversarialer Netzwerke](https://youtu.be/9JpdAg6uMXs), ist Ian auch einer der Hauptautoren des Lehrbuchs Deep Learning, wurde in MIT Technology Review's 35 Innovators Under 35 zitiert und wurde in die Liste der 100 Global Thinkers von Foreign Policy aufgenommen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/Ian-1024x341.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8546334a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Yann LeCun, Vice President und Chief AI Scientist bei Facebook.\n",
    "(<a href=\"https://twitter.com/ylecun\" target=\"_blank\" rel=\"noopener\">@ylecun</a> 156K | <a href=\"https://www.linkedin.com/in/yann-lecun-0b999/detail/recent-activity/\" target=\"_blank\" rel=\"noopener\">LinkedIn activity</a> | <a href=\"https://www.facebook.com/yann.lecun\" target=\"_blank\" rel=\"noopener\">Facebook</a> | <a href=\"https://scholar.google.com/citations?hl=en&amp;user=WLN3QrAAAAAJ\" target=\"_blank\" rel=\"noopener\">Google Scholar</a> | <a href=\"https://arxiv.org/search/?query=%22Yann+LeCun%22&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50\" target=\"_blank\" rel=\"noopener\">arXiv</a>)</li>\n",
    "\n",
    "Als Professor, Forscher und F&E-Manager mit akademischer und industrieller Erfahrung in den Bereichen KI, maschinelles Lernen, Deep Learning, Computer Vision, intelligente Datenanalyse, Data Mining, Datenkomprimierung, digitale Bibliothekssysteme und Robotik ist Yann als Mitbegründer der Faltungsnetze sowie für seine Arbeiten zur optischen Zeichenerkennung und Computer Vision unter Verwendung von Faltungsneuronalen Netzen bekannt. Yann ist ausserdem Miterfinder der DjVu-Bildkompressionstechnologie und hat die Programmiersprache Lush mitentwickelt. Er war Gründungsdirektor des NYU Center for Data Science, wo er das unüberwachte Lernen revolutionierte, und 2018 war Yann zusammen mit Geoffrey Hinton und Yoshua Benigo Co-Preisträger des ACM A.M. Turing Award für ihre Arbeit im Bereich Deep Learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e771c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/Yann-1024x341.png\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9246adb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Jeremy P. Howard, Founding Researcher bei fast.ai, \n",
    "Distinguished Research Scientist an der University of San Francisco.\n",
    "(<a href=\"https://twitter.com/jeremyphoward\" target=\"_blank\" rel=\"noopener\">@jeremyphoward</a> 69K | <a href=\"https://arxiv.org/search/?query=%22Jeremy+Howard%22&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50\" target=\"_blank\" rel=\"noopener\">arXiv</a>)</li>\n",
    "\n",
    "Als Unternehmer, Geschäftsstratege, Entwickler und Pädagoge ist Jeremy Howard Gründungsforscher bei fast.ai, einem Forschungsinstitut, das sich dafür einsetzt, Deep Learning zugänglicher zu machen. Ausserdem ist er Distinguished Research Scientist an der University of San Francisco, Fakultätsmitglied an der Singularity University und Young Global Leader beim World Economic Forum. Jeremys jüngstes Startup, Enlitic, war das erste Unternehmen, das Deep Learning in der Medizin einsetzte und wurde von MIT Tech Review zweimal zu einem der 50 intelligentesten Unternehmen der Welt gewählt. Zuvor war Jeremy Präsident und Chief Scientist der Data-Science-Plattform Kaggle sowie ein unternehmerischer Investor mit mehreren Startups, ein Berater und Mitwirkender bei vielen Open-Source-Projekten.  Jeremy ist regelmässiger Gast in Australiens höchstbewertetem Frühstücksnachrichtenprogramm, hielt einen beliebten Vortrag auf TED.com und erstellte Tutorials und Diskussionen zu Data Science und Webentwicklung.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea64ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Jürgen Schmidhuber\n",
    "Co-Direktor des Dalle Molle Institute for Artificial Intelligence Research in Manno, Schweiz\n",
    "\n",
    "Zusammen mit seinen Studenten Sepp Hochreiter, Fred Cummins, Alex Graves und anderen ist er als Vater des modernen Deep Learning bekannt. Er war der erste, der eine Arbeit über das Langzeitgedächtnis (Long short-term memory, LSTM), eine hochentwickelte Version rekurrenter neuronaler Netzwerke, veröffentlichte. LSTM wird mittlerweile in der Spracherkennung eingesetzt, z. B. in Googles Software für Smartphones, dem smarten Assistenten Allo, Apples Siri, Amazons Alexa und anderen. Schmidhuber ist Co-Direktor des Dalle Molle Institute for Artificial Intelligence Research in Manno, Schweiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864fe337",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"-Y7PLaxXUrs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471dbea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Yoshua Bengio \n",
    "\n",
    "Yoshua Bengio ist Professor am Department of Computer Science and Operations Research an der Université de Montréal. Er ist außerdem Mitbegründer von Element AI, einem in Montreal ansässigen Business-Inkubator, der versucht, KI-Forschung in reale Geschäftsanwendungen zu verwandeln.\n",
    "\n",
    "Yoshua ist bekannt für seine Arbeit an künstlichen neuronalen Netzwerken und Deep Learning in den 1980er und 1990er Jahren. Gemeinsam mit Yann LeCun hat er die renommierte ICLR-Konferenz ins Leben gerufen. Er ist einer der meistzitierten Informatiker in den Bereichen Deep Learning, rekurrente Netzwerke, probabilistisches Lernen und natürliche Sprache.\n",
    "\n",
    "Es gibt wahrscheinlich kein Thema im Bereich Deep Learning, das Yoshua nicht berührt hat, und deshalb ist sein Beitrag zum Gebiet des Deep Learning im Vergleich zu seinen Zeitgenossen recht vielfältig.\n",
    "\n",
    "Er hat die prestigeträchtige Auszeichnung des Canada Research Chair in Statistical Learning Algorithms erhalten und wurde außerdem mit dem Turing Award 2018 ausgezeichnet. Sehen Sie sich seinen Vortrag über Deep Learning unten an:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3965ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/Yosh-1024x341.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d155c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ruslan Salakhutdinov\n",
    "ausserordentlicher Professor an der Carnegie Mellon University, Direktor der KI-Forschung bei Apple.\n",
    "(<a href=\"https://twitter.com/rsalakhu\" target=\"_blank\" rel=\"noopener\">@rsalakhu</a> 69K | <a href=\"https://www.linkedin.com/in/ruslan-salakhutdinov-53a0b610/detail/recent-activity/\" target=\"_blank\" rel=\"noopener\">LinkedIn activity</a> | <a href=\"https://scholar.google.com/citations?hl=en&amp;user=ITZ1e7MAAAAJ\" target=\"_blank\" rel=\"noopener\">Google Scholar</a> | <a href=\"https://arxiv.org/search/?query=%22Ruslan+Salakhutdinov%22&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50\" target=\"_blank\" rel=\"noopener\">arXiv</a>)</li>\n",
    "\n",
    "Als Professor für Informatik in der Abteilung für maschinelles Lernen, School of Computer Science an der CMU, arbeitet Ruslan im Bereich des statistischen maschinellen Lernens. Sein Doktorvater war Geoffrey Hinton und mit vielen veröffentlichten Arbeiten über maschinelles Lernen umfassen Ruslans Interessen Deep Learning, probabilistische grafische Modelle und gross angelegte Optimierung. Er ist bekannt für die Entwicklung des Bayes'schen Programmlernens und war der erste Direktor der KI-Forschung, der bei Apple eingestellt wurde.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c3940",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/Ruslan-1024x341.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113235d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Geoffrey Hinton\n",
    "Professor für Informatik an der Universität von Toronto, VP und Engineering Fellow bei Google\n",
    "(<a href=\"https://twitter.com/geoffreyhinton\" target=\"_blank\" rel=\"noopener\">@geoffreyhinton</a> 47K | <a href=\"https://scholar.google.com/citations?hl=en&amp;user=JicYPdAAAAAJ\" target=\"_blank\" rel=\"noopener\">Google Scholar</a> | <a href=\"https://arxiv.org/search/?query=%22Geoffrey+Hinton%22&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50\" target=\"_blank\" rel=\"noopener\">arXiv</a>)</li>\n",
    "\n",
    "Geoffrey Hinton ist ein berühmter KI-Forscher und Mitbegründer der Idee des Deep Learning, da er 1986 die Backpropagation für das Training von mehrschichtigen neuronalen Netzwerken populär gemacht hat. Geoffrey gilt als der \"Pate des Deep Learning\" und teilt seine Zeit mit Google Brain und der Universität von Toronto. Seine Erfindungen sind der Kern von Algorithmen, die Spracherkennung, fahrerlose Autos und Bilderkennung antreiben, und er wurde zusammen mit Yoshua Benigo und Yann LeCun mit dem Turning Prize 2018 ausgezeichnet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/master/ANN01/Bilder/Geoffrey-1024x341.jpg\"\n",
    "display(Image(url=url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f585b91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Daphne Koller\n",
    "Gründerin und CEO von insitro, Mitbegründerin von Coursera, ausserplanmässige Professorin für Informatik und Pathologie in Stanford.\n",
    "(<a href=\"https://twitter.com/DaphneKoller\" target=\"_blank\" rel=\"noopener\">@DaphneKoller</a> 16K | <a href=\"https://scholar.google.com/citations?hl=en&amp;user=5Iqe53IAAAAJ\" target=\"_blank\" rel=\"noopener\">Google Scholar</a> | <a href=\"https://arxiv.org/search/?query=%22Daphne+Koller%22&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50\" target=\"_blank\" rel=\"noopener\">arXiv</a>)</li>\n",
    "\n",
    "Daphne hat an der Grenze zwischen maschinellem Lernen und Biomedizin gearbeitet. Im Jahr 2004 wurde sie zum MacArthur Fellow ernannt und erhielt den allerersten ACM-Infosys Foundation Award in Computing Sciences. Später gründete Daphne zusammen mit Andrew Ng Coursera. Für ihre Beiträge zur Online-Bildung wurde sie 2010 von Newsweek als eine der 10 wichtigsten Personen, 2012 vom Time Magazine als eine der 100 einflussreichsten Personen und 2014 von Fast Company als eine der kreativsten Personen ausgezeichnet. Nachdem sie 2016 Chief Computing Officer bei Calico wurde, wechselte Daphne als Nächstes zu insitro, einem Startup für Arzneimittelforschung. Daphne ist Autorin von über 200 begutachteten Publikationen, die in Zeitschriften wie Science, Cell und Nature Genetics erschienen sind, und Co-Autorin eines Lehrbuchs über probabilistische grafische Modelle, das in einen Online-Kurs umgewandelt wurde.\n",
    "\n",
    "### Alex Smola, Direktor, Amazon Web Services.\n",
    "(<a href=\"https://twitter.com/smolix\" target=\"_blank\" rel=\"noopener\">@smolix</a> 14K | <a href=\"https://www.facebook.com/smolix\" target=\"_blank\" rel=\"noopener\">Facebook</a> | <a href=\"https://scholar.google.com/citations?hl=en&amp;user=Tb0ZrYwAAAAJ\" target=\"_blank\" rel=\"noopener\">Google Scholar</a> | <a href=\"https://arxiv.org/search/?query=%22Alex+Smola%22&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50\" target=\"_blank\" rel=\"noopener\">arXiv</a>)</li>\n",
    "\n",
    "Alex arbeitet an maschinellem Lernen und statistischer Datenanalyse, einschliesslich Anwendungen von der Dokumentenanalyse, Bioinformatik und Computer Vision bis hin zur Analyse von Internetdaten. Nachdem er zahlreiche Doktoranden und Forscher betreut hat, hat Alex über 200 Papers und ein Buch verfasst und fünf Bücher herausgegeben. Seine Spezialgebiete sind Deep Learning, Kernel-Methoden, unüberwachte Modelle, schnelle Sampler und Benutzermodelle, einschliesslich hochskalierbarer Modelle, die viele Terabytes an Daten und Hunderte von Millionen von Benutzern umfassen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e720b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5508b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "nbTranslate": {
   "displayLangs": [
    "ger",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ger",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
