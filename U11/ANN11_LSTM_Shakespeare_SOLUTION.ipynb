{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U11/ANN11_LSTM_Shakespeare_SOLUTION.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Dichten wie Shakespeare ...\n",
    "\n",
    "## Rekurrente Netze - LSTMs\n",
    "\n",
    "- LSTMs führen das Konzept eines Zustands für jede Schicht in einem rekurrenten Netz ein. \n",
    "- Der Zustand fungiert als sein Speicher. Man kann sich das wie das Hinzufügen von Attributen zu einer Klasse in der objektorientierten Programmierung vorstellen. \n",
    "- Die Attribute des Speicherzustands werden mit jedem Trainingsbeispiel aktualisiert.\n",
    "\n",
    "- In LSTMs sind die Regeln, welche die im State (Speicher) gespeicherten Informationen bestimmen, trainierte neuronale Netze - darin liegt der Magie (die Intelligenz?). Sie können trainiert werden, um zu lernen, was sie sich merken sollen, während gleichzeitig der Rest des rekurrenten Netzes lernt, das Ziel-Label vorherzusagen! \n",
    "- Mit der Einführung eines Speichers (memory) und eines Zustands (state) ist das Modell in der Lage, Abhängigkeiten zu lernen, die sich nicht nur über ein oder zwei Token, sondern über die die Gesamtheit jeder Datenprobe erstrecken. \n",
    "\n",
    "Mit diesen langfristigen Abhängigkeiten in der Hand, kann man über die Wörter selbst hinausgehen und etwas Tieferes über die Sprache herausfinden. Mit LSTMs stehen dem Modell Muster zur Verfügung, die der Mensch als selbstverständlich ansieht und auf einer unterbewussten Ebene verarbeitet. Und mit diesen Mustern können Sie nicht nur\n",
    "Muster genauer vorhersagen, sondern auch **neue Texte generieren**. Der Stand der Technik in diesem Bereich ist noch lange nicht perfekt, aber die aber die Ergebnisse, die Sie sehen werden, selbst in Ihren Spielzeugbeispielen, sind beeindruckend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Modellierung von Sprache auf Zeichenebene\n",
    "\n",
    "Worte haben eine Bedeutung - da sind wir uns alle einig. Die Modellierung natürlicher Sprache mit diesen\n",
    "Grundbausteinen zu modellieren, erscheint daher nur natürlich (WordVectors). Die Verwendung dieser Modelle zur Beschreibung von Bedeutung, Gefühle, Absichten und alles andere in Form dieser atomaren Strukturen zu beschreiben, scheint\n",
    "ebenfalls natürlich. \n",
    "\n",
    "Aber natürlich sind Wörter überhaupt nicht atomar. Wie Sie vorhin gesehen haben, bestehen sie aus kleineren Wörtern, Wortstämmen, Phonemen und so weiter. Aber sie sind auch, und das ist noch grundlegender, eine **Folge von Zeichen**.\n",
    "\n",
    "\n",
    "- Bei der *Modellierung von Sprache* ist ein Grossteil der Bedeutung *auf der Zeichenebene* verborgen.\n",
    "- *Intonationen* in der Stimme, *Alliterationen*, *Reime* - all das kann modelliert werden, wenn man wenn man die Dinge bis auf die Zeichenebene herunterbricht. \n",
    "\n",
    "Viele dieser Muster sind in einem Text enthalten, wenn man den Text daraufhin untersucht, welches Zeichen $x_k$ nach welchem folgt, unter Berücksichtigung (conditional probability) der Zeichen, die vorangegangen sind.\n",
    "\n",
    "$$ p(x_k \\vert x_{k-1}, x_{k-1}, \\dots , x_{k-m})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "In diesem Paradigma wird ein Leerzeichen, ein Komma oder ein Punkt einfach zu einem weiteren Zeichen.\n",
    "Und da das LSTM-Netz die **Bedeutung von Sequenzen lernt**, ist es gezwungen, Muster auf niedrigerer Ebene zu finden.\n",
    "Wenn das Modell nach einer bestimmten Anzahl von Silben ein sich wiederholendes Suffix erkennt, das sich\n",
    "reimt, könnte dies ein Hinweis auf eine Bedeutung wie vielleicht Heiterkeit oder Spott sein.\n",
    "\n",
    "- Mit einer ausreichend grossen Trainingsmenge beginnen sich diese Muster herauszukristallisieren. Und da es in der englischen Sprache viel weniger unterschiedliche Buchstaben als Wörter gibt, muss sich das Modell um eine eine geringere Vielfalt an Eingabevektoren kümmern (*Dimensionsreduktion* des Inputs).\n",
    "- **Das Trainieren eines Modells auf Buchstabenebene ist jedoch schwierig**. Die Muster und langfristigen Abhängigkeiten, die auf der Zeichenebene zu finden sind, können von Text zu Text sehr unterschiedlich sein. Sie können diese Muster finden, aber sie lassen sich möglicherweise nicht so gut verallgemeinern. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### (a) Herunterladen und Speichern des Datensatzes \n",
    "\n",
    "1. **Herunterladen des Textes:**\n",
    "   - Es wird eine Verbindung zu einer URL aufgebaut, wo sich ein Text mit Werken von Shakespeare befindet.\n",
    "   - Der komplette Text wird aus dem Internet geladen und im Arbeitsspeicher gespeichert.\n",
    "\n",
    "2. **Speichern auf dem Computer:**\n",
    "   - Der geladene Text wird in eine neue Datei namens `shakespeare.txt` geschrieben.\n",
    "   - Diese Datei liegt dann lokal auf deinem Rechner unter `/data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the text file\n",
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/678ae4873e82f52ae1b563e32c12c6837fd5ae78/data/Gutenberg/shakespeare.txt\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "shakespeare_text = response.text\n",
    "\n",
    "# Save the text to a local file (optional)\n",
    "with open(\"./data/shakespeare.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(shakespeare_text)\n",
    "\n",
    "print(\"Text file downloaded and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/shakespeare.txt\") as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### (b) Generieren Sie ein `dictionary`, das jedem Character einen Index zuweist und umgekehrt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(char_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### (c) Eine Textstelle anzeigen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### (d) PyTorch Dataset class: Aufteilen der Textsequenzen in Teilstücke\n",
    "\n",
    "Generieren Sie mit Hilfe eines LLM (ChatGPT, Gemini, ...) ein PyToch-Dataset, das folgende Aufgaben erfüllt:\n",
    "- es genieriert einen Datensatz für die Sprachmodellierung auf Zeichenebene bei Shakespeare-Texten.\n",
    "- dazu erstellt es ein Vokabular der Zeichen und codiert the Zeichenketten in Indizes und\n",
    "```python\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char2idx = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.idx2char = {i: c for i, c in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "```\n",
    "- Gibt Sequenzen von Zeichenindizes und Beschriftungen des nächsten Zeichens zurück.\n",
    "- Teilt den Input-Korpus in Zeichenketten der maximalen Länge `maxlen=40` auf. \n",
    "- Verwendet einen Stride von 3 Zeichen, sodass eine semi-redundante Repräsentation der Input-Sequenz entsteht.\n",
    "- Erstellt gleichzeitig eine Liste `next_chars`, welche das nächste, folgende Zeichen nach dieser Zeichenkette speichert. Dies sind die zugehörigen Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for character-level language modeling on Shakespeare texts.\n",
    "    Returns sequences of character indices and next-character labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir: str, maxlen: int = 40, step: int = 3):\n",
    "        text = ''\n",
    "        for path in pathlib.Path(data_dir).iterdir():\n",
    "            if path.is_file():\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    text += f.read().lower()\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char2idx = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.idx2char = {i: c for i, c in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = maxlen\n",
    "        self.step = step\n",
    "        self.sequences = []\n",
    "        self.next_chars = []\n",
    "        for i in range(0, len(text) - maxlen, step):\n",
    "            self.sequences.append(text[i: i + maxlen])\n",
    "            self.next_chars.append(text[i + maxlen])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        # Return sequence of indices\n",
    "        x = torch.tensor([self.char2idx[ch] for ch in seq], dtype=torch.long)\n",
    "        # Target index\n",
    "        y = torch.tensor(self.char2idx[self.next_chars[idx]], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### (e) Pytorch DataLoader\n",
    "\n",
    "Das `ShakespeareDataModule` ist ein **PyTorch Lightning**-Mechanismus zum Laden, Vorverarbeiten und Batchen eines Shakespeare-Textkorpus für ein Zeichen-basiertes LSTM-Modell. \n",
    "Es:\n",
    "\n",
    "* Liest alle Shakespeare-Dateien aus einem Verzeichnis\n",
    "* Zerlegt den Text in Sequenzen fester Länge\n",
    "* Teilt die Daten in **Training**- und **Validierungs**-Sets\n",
    "* Stellt jeweils passende `DataLoader` bereit\n",
    "\n",
    "\n",
    "**Parameter im Konstruktor**\n",
    "\n",
    "| Parameter    | Typ     | Beschreibung                                                                                     |\n",
    "| ------------ | ------- | ------------------------------------------------------------------------------------------------ |\n",
    "| `data_dir`   | `str`   | Pfad zum Verzeichnis mit den Shakespeare-Textdateien                                             |\n",
    "| `batch_size` | `int`   | Anzahl der Sequenzen pro Batch beim Training/Validierung                                         |\n",
    "| `val_split`  | `float` | Anteil (z. B. `0.05` = 5 %) für das Validierungs-Set                                             |\n",
    "| `maxlen`     | `int`   | Länge jeder Eingabesequenz (Anzahl Zeichen)                                                      |\n",
    "| `step`       | `int`   | Schrittweite beim Verschieben des Fensters über den Text (z. B. `3` für semi-redundante Samples) |\n",
    "\n",
    "\n",
    "\n",
    "**Hinweis:** Dieses Modul trennt die Daten sauber vom Modell-Code. So könnt ihr verschiedene Modelle (LSTM, Transformer, …) einsetzen, ohne das Preprocessing anzupassen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ShakespeareDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 data_dir: str = 'data',\n",
    "                 batch_size: int = 128,\n",
    "                 val_split: float = 0.05,\n",
    "                 maxlen: int = 40,\n",
    "                 step: int = 3):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.maxlen = maxlen\n",
    "        self.step = step\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = CharDataset(self.data_dir, self.maxlen, self.step)\n",
    "        total = len(dataset)\n",
    "        val_size = int(total * self.val_split)\n",
    "        train_size = total - val_size\n",
    "        self.train_dataset, self.val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        self.vocab_size = dataset.vocab_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### (f) Definieren Sie ein einfaches LSTM-Modell mit PyTorch Lightning\n",
    "\n",
    "Implementieren Sie mit Hilfe eines LLM (ChatGPT, Gemini, ...) ein LSTM-Modell für die Vorhersage des nächsten Zeichens.\n",
    "\n",
    "- Zu Beginn werden die Eingabeindizes über eine **Embedding-Schicht** in *dichte Vektoren* überführt, was gegenüber grossen, spärlichen One‑Hot-Repräsentationen deutlich speichereffizienter ist und es dem Modell erlaubt, semantische Ähnlichkeiten zwischen Zeichen zu lernen. \n",
    "- Anschliessend verarbeitet ein **Long Short‑Term Memory (LSTM)** diese eingebetteten Sequenzen, um zeitliche Abhängigkeiten im Text zu erfassen und langfristige Kontextinformationen im verborgenen Zustand zu speichern.\n",
    "- Der letzte verborgene (hidden) Zustand des LSTM dient als **komprimierte Repräsentation der gesamten Eingabesequenz**. \n",
    "- Eine **Dropout-Schicht** aktiviert sich danach, um Überanpassung zu verhindern, indem sie zufällig Teile des neuronalen Netzwerks deaktiviert. \n",
    "- Ein **dichtes Zwischennetzwerk** mit nichtlinearer Aktivierung (ReLU) sorgt für zusätzliche Modellkapazität und stellt sicher, dass komplexe Zusammenhänge zwischen den Merkmalen gelernt werden können. \n",
    "- Ein erneutes **Dropout** verbessert die Generalisierungsfähigkeit weiter. \n",
    "- Abschliessend projiziert eine finale vollverknüpfte Schicht dieses Merkmal auf so viele Ausgabeneinheiten, wie Zeichen im Vokabular vorhanden sind, um für jedes mögliche Folgezeichen einen Rohwert (Logit) zu erzeugen.\n",
    "\n",
    "Verwenden Sie folgende Parameter:\n",
    "```python\n",
    "class LitCharRNN(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                vocab_size: int,\n",
    "                embed_dim: int = 64,\n",
    "                hidden_size: int = 128,\n",
    "                dropout: float = 0.2,\n",
    "                lr: float = 5e-3):\n",
    "```\n",
    "\n",
    "Für die **Optimierung** kommt die Kreuzentropie als Verlustfunktion zum Einsatz. Sie kombiniert implizit Softmax und Negative Log-Likelihood und ist der Standard bei Mehrklassen-Klassifikationsaufgaben; die fehlende Notwendigkeit einer manuellen Softmax-Operation im Netzwerkaufbau erhöht die numerische Stabilität und hält den Trainingscode schlank. RMSprop dient als Optimierer, da er durch adaptive Lernraten und eine gewichtete Mittelung vergangener Gradienten gut mit den dynamischen Gradienten von rekursiven Netzen zurechtkommt.\n",
    "\n",
    "Während des Trainings und der Validierung wird die Modellgenauigkeit mittels einer **Multiclass-Accuracy-Metrik** verfolgt. Dieser Ansatz ermöglicht eine präzise Überwachung, wie oft das Modell das nächste Zeichen korrekt vorhersagt, und liefert direkt interpretierbare Kennzahlen zur Modellgüte. Insgesamt bildet die Kombination aus Embedding, LSTM, Dropout, nichtlinearer Zwischenschicht und Kreuzentropie-Loss eine robuste Architektur für Zeichen-basiertes Sprachmodellieren, die sowohl Effizienz als auch Leistungsfähigkeit gewährleistet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitCharRNN(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embed_dim: int = 64,\n",
    "                 hidden_size: int = 128,\n",
    "                 dropout: float = 0.2,\n",
    "                 lr: float = 5e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocab_size)\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len) of indices\n",
    "        emb = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        output, (h_n, c_n) = self.lstm(emb)\n",
    "        last_hidden = h_n[-1]\n",
    "        x = self.dropout(last_hidden)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.train_accuracy(preds, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.val_accuracy(preds, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### (g) Softmax-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducible logits\n",
    "np.random.seed(42)\n",
    "logits = np.random.randn(10)\n",
    "\n",
    "# Convert logits to base probabilities\n",
    "base_probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "\n",
    "# Softmax-with-temperature function (returns distribution)\n",
    "def tempered_softmax(preds: np.ndarray, temperature: float = 1.0) -> np.ndarray:\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    return exp_preds / np.sum(exp_preds)\n",
    "\n",
    "# Temperatures to visualize\n",
    "temperatures = [0.1, 2, 5]\n",
    "# Create a single figure with 3 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, T in zip(axes, temperatures):\n",
    "    dist = tempered_softmax(base_probs, T)\n",
    "    ax.bar(range(10), dist)\n",
    "    ax.set_title(f\"T = {T}\")\n",
    "    ax.set_xlabel(\"Index\")\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    ax.set_ylim(0, dist.max() + 0.1)\n",
    "    ax.grid(True)\n",
    "    ax.set_xticks(range(10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "- Now, we generate out of a standard softmax-output a tempered_softmax output and sample from it. \n",
    "- The temperature parameter T controls the sharpness of the distribution. \n",
    "- A low T (e.g., 0.1) makes the distribution sharper, while a high T (e.g., 5) makes it flatter. \n",
    "- This allows us to control the randomness of the sampling process.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds: np.ndarray, temperature: float = 1.0) -> int:\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### (h) Modell trainiren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "data_module = ShakespeareDataModule(data_dir='data', batch_size=128)\n",
    "data_module.setup()\n",
    "\n",
    "model = LitCharRNN(vocab_size=data_module.vocab_size)\n",
    "logger = TensorBoardLogger('tb_logs', name='shakes_rnn')\n",
    "early_stop = EarlyStopping(monitor='train_loss', patience=5, mode='min')\n",
    "trainer = pl.Trainer(max_epochs=20, logger=logger, callbacks=[early_stop])\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "torch.save(model.state_dict(), 'shakes_lstm_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### (i) Neue Texte generieren:\n",
    "\n",
    "Sie mit der Temperatur oder den Startsequenzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "text_data = ''\n",
    "for path in pathlib.Path('data').iterdir():\n",
    "    if path.is_file():\n",
    "        text_data += open(path, 'r').read().lower()\n",
    "start_index = random.randint(0, len(text_data) - data_module.maxlen - 1)\n",
    "seed = text_data[start_index: start_index + data_module.maxlen]\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print(f\"\\n----- temperature: {diversity}\")\n",
    "    print(f\"----- Generating with seed: '{seed}'\")\n",
    "    sentence = seed\n",
    "    generated = seed\n",
    "    for _ in range(400):\n",
    "        x_pred = torch.tensor([[data_module.train_dataset.dataset.char2idx[ch] for ch in sentence]], dtype=torch.long).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(x_pred)                         # preds is a tensor\n",
    "            tensor_probs = F.softmax(preds, dim=-1)       # softmax on tensor\n",
    "            probs = tensor_probs.cpu().numpy().squeeze()  # then to NumPy\n",
    "            #probs = F.softmax(logits, dim=-1).cpu().numpy().squeeze()\n",
    "        next_idx = sample(probs, diversity)\n",
    "        next_char = data_module.train_dataset.dataset.idx2char[next_idx]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
