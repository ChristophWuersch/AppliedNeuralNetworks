{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/U11/ANN11_LSTM_Shakespeare_SOLUTION_pl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Dichten wie Shakespeare ...\n",
    "\n",
    "## Rekurrente Netze - LSTMs\n",
    "\n",
    "- LSTMs führen das Konzept eines Zustands für jede Schicht in einem rekurrenten Netz ein. \n",
    "- Der Zustand fungiert als sein Speicher. Man kann sich das wie das Hinzufügen von Attributen zu einer Klasse in der objektorientierten Programmierung vorstellen. \n",
    "- Die Attribute des Speicherzustands werden mit jedem Trainingsbeispiel aktualisiert.\n",
    "\n",
    "- In LSTMs sind die Regeln, welche die im State (Speicher) gespeicherten Informationen bestimmen, trainierte neuronale Netze - darin liegt der Magie (die Intelligenz?). Sie können trainiert werden, um zu lernen, was sie sich merken sollen, während gleichzeitig der Rest des rekurrenten Netzes lernt, das Ziel-Label vorherzusagen! \n",
    "- Mit der Einführung eines Speichers (memory) und eines Zustands (state) ist das Modell in der Lage, Abhängigkeiten zu lernen, die sich nicht nur über ein oder zwei Token, sondern über die die Gesamtheit jeder Datenprobe erstrecken. \n",
    "\n",
    "Mit diesen langfristigen Abhängigkeiten in der Hand, kann man über die Wörter selbst hinausgehen und etwas Tieferes über die Sprache herausfinden. Mit LSTMs stehen dem Modell Muster zur Verfügung, die der Mensch als selbstverständlich ansieht und auf einer unterbewussten Ebene verarbeitet. Und mit diesen Mustern können Sie nicht nur\n",
    "Muster genauer vorhersagen, sondern auch **neue Texte generieren**. Der Stand der Technik in diesem Bereich ist noch lange nicht perfekt, aber die aber die Ergebnisse, die Sie sehen werden, selbst in Ihren Spielzeugbeispielen, sind beeindruckend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Modellierung von Sprache auf Zeichenebene\n",
    "\n",
    "Worte haben eine Bedeutung - da sind wir uns alle einig. Die Modellierung natürlicher Sprache mit diesen\n",
    "Grundbausteinen zu modellieren, erscheint daher nur natürlich (WordVectors). Die Verwendung dieser Modelle zur Beschreibung von Bedeutung, Gefühle, Absichten und alles andere in Form dieser atomaren Strukturen zu beschreiben, scheint\n",
    "ebenfalls natürlich. \n",
    "\n",
    "Aber natürlich sind Wörter überhaupt nicht atomar. Wie Sie vorhin gesehen haben, bestehen sie aus kleineren Wörtern, Wortstämmen, Phonemen und so weiter. Aber sie sind auch, und das ist noch grundlegender, eine **Folge von Zeichen**.\n",
    "\n",
    "\n",
    "- Bei der *Modellierung von Sprache* ist ein Grossteil der Bedeutung *auf der Zeichenebene* verborgen.\n",
    "- *Intonationen* in der Stimme, *Alliterationen*, *Reime* - all das kann modelliert werden, wenn man wenn man die Dinge bis auf die Zeichenebene herunterbricht. \n",
    "\n",
    "Viele dieser Muster sind in einem Text enthalten, wenn man den Text daraufhin untersucht, welches Zeichen $x_k$ nach welchem folgt, unter Berücksichtigung (conditional probability) der Zeichen, die vorangegangen sind.\n",
    "\n",
    "$$ p(x_k \\vert x_{k-1}, x_{k-1}, \\dots , x_{k-m})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "In diesem Paradigma wird ein Leerzeichen, ein Komma oder ein Punkt einfach zu einem weiteren Zeichen.\n",
    "Und da das LSTM-Netz die **Bedeutung von Sequenzen lernt**, ist es gezwungen, Muster auf niedrigerer Ebene zu finden.\n",
    "Wenn das Modell nach einer bestimmten Anzahl von Silben ein sich wiederholendes Suffix erkennt, das sich\n",
    "reimt, könnte dies ein Hinweis auf eine Bedeutung wie vielleicht Heiterkeit oder Spott sein.\n",
    "\n",
    "- Mit einer ausreichend grossen Trainingsmenge beginnen sich diese Muster herauszukristallisieren. Und da es in der englischen Sprache viel weniger unterschiedliche Buchstaben als Wörter gibt, muss sich das Modell um eine eine geringere Vielfalt an Eingabevektoren kümmern (*Dimensionsreduktion* des Inputs).\n",
    "- **Das Trainieren eines Modells auf Buchstabenebene ist jedoch schwierig**. Die Muster und langfristigen Abhängigkeiten, die auf der Zeichenebene zu finden sind, können von Text zu Text sehr unterschiedlich sein. Sie können diese Muster finden, aber sie lassen sich möglicherweise nicht so gut verallgemeinern. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA (GPU) is available, otherwise use CPU\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# (a) Herunterladen und Speichern des Datensatzes \n",
    "\n",
    "1. **Herunterladen des Textes:**\n",
    "   - Es wird eine Verbindung zu einer URL aufgebaut, wo sich ein Text mit Werken von Shakespeare befindet.\n",
    "   - Der komplette Text wird aus dem Internet geladen und im Arbeitsspeicher gespeichert.\n",
    "\n",
    "2. **Speichern auf dem Computer:**\n",
    "   - Der geladene Text wird in eine neue Datei namens `shakespeare.txt` geschrieben.\n",
    "   - Diese Datei liegt dann lokal auf deinem Rechner.\n",
    "\n",
    "3. **Rückmeldung:**\n",
    "   - Nach dem erfolgreichen Speichern gibt das Skript eine einfache Bestätigung aus:\n",
    "     ```\n",
    "     Text file downloaded and saved.\n",
    "     ```\n",
    "\n",
    "**Ziel dieses Skripts:**\n",
    "Der heruntergeladene Text kann später verwendet werden – z. B. zum Trainieren eines LSTM-Modells für Textgenerierung (wie bei KI-Schreibassistenten oder automatischem Dichten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the text file\n",
    "url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/678ae4873e82f52ae1b563e32c12c6837fd5ae78/data/Gutenberg/shakespeare.txt\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "shakespeare_text = response.text\n",
    "\n",
    "# Save the text to a local file (optional)\n",
    "with open(\"shakespeare.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(shakespeare_text)\n",
    "\n",
    "print(\"Text file downloaded and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# (b) Datensatz lesen\n",
    "In diesem Code wird die Datei `shakespeare.txt` geöffnet und ihr gesamter Inhalt wird eingelesen. Das passiert in einem sogenannten `with`-Block.\n",
    "\n",
    "### Schritt-für-Schritt:\n",
    "\n",
    "1. **Datei öffnen:**\n",
    "   Die Textdatei `shakespeare.txt` wird geöffnet. Der `with`-Block sorgt dafür, dass die Datei nach dem Lesen automatisch wieder geschlossen wird – auch wenn ein Fehler auftritt.\n",
    "\n",
    "2. **Inhalt lesen:**\n",
    "   Mit `f.read()` wird der gesamte Textinhalt der Datei eingelesen und in der Variable `text` gespeichert. Der Inhalt liegt danach als ein langer String vor.\n",
    "\n",
    "## Ergebnis\n",
    "\n",
    "Der gesamte Text aus der Datei befindet sich nun in der Variable `text` und kann weiterverarbeitet werden, z. B. zur Tokenisierung oder für ein LSTM-Sprachmodell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shakespeare.txt\") as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# (c) Zeichenbasierte Kodierung des Textes\n",
    "\n",
    "Dieser Code bereitet den Shakespeare-Text für die zeichenweise Verarbeitung vor (z. B. für ein LSTM-Modell).\n",
    "\n",
    "## Schritt-für-Schritt-Erklärung\n",
    "\n",
    "1. **`all_char = set(text)`**  \n",
    "   → Erstellt eine Menge aller **einzigartigen Zeichen** im Text (z. B. Buchstaben, Satzzeichen, Leerzeichen).\n",
    "\n",
    "2. **`n_unique_char = len(all_char)`**  \n",
    "   → Zählt, wie viele **unterschiedliche Zeichen** es gibt (z. B. 65 oder 80).\n",
    "\n",
    "3. **`decoder = dict(enumerate(all_char))`**  \n",
    "   → Erstellt ein Wörterbuch, das jedem Index ein Zeichen zuordnet.  \n",
    "   → Wird später verwendet, um Zahlen wieder in Text umzuwandeln (**Decodierung**).\n",
    "\n",
    "4. **`encoder = {char: ind for ind, char in decoder.items()}`**  \n",
    "   → Erstellt das Gegenstück zum Decoder: ein Wörterbuch, das jedem Zeichen einen Index zuordnet (**Encodierung**).\n",
    "\n",
    "5. **`encoded_text = np.array([encoder[char] for char in text])`**  \n",
    "   → Wandelt den gesamten Text in eine Folge von Ganzzahlen um.  \n",
    "   → Jedes Zeichen wird durch seine entsprechende Zahl aus dem `encoder` ersetzt.\n",
    "\n",
    "## Ergebnis\n",
    "\n",
    "Der Text ist jetzt numerisch kodiert – als NumPy-Array mit Ganzzahlen – und bereit für das Training eines Modells wie LSTM oder GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_char = set(text)\n",
    "n_unique_char = len(all_char)\n",
    "decoder = dict(enumerate(all_char))\n",
    "encoder = {char: ind for ind, char in decoder.items()}\n",
    "encoded_text = np.array([encoder[char] for char in text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# (d) One-Hot-Encoding-Funktion\n",
    "\n",
    "## Funktion: `one_hot_encoder(encoded_text, n_unique_char)`\n",
    "\n",
    "Diese Funktion wandelt eine Folge von ganzzahligen Zeichen-IDs in ein **One-Hot-Encoded Format** um.\n",
    "\n",
    "### Was passiert?\n",
    "\n",
    "- Erstellt eine Null-Matrix der Form `(Anzahl_Zeichen, Anzahl_einzigartiger_Zeichen)`.\n",
    "- Setzt für jedes Zeichen an der passenden Stelle den Wert auf `1.0`.\n",
    "- Formt das Ergebnis zurück zur Originalform, ergänzt um die One-Hot-Dimension.\n",
    "\n",
    "### Eingabe:\n",
    "- `encoded_text`: 1D- oder 2D-Array mit ganzzahligen Zeichen-IDs.\n",
    "- `n_unique_char`: Anzahl aller unterschiedlichen Zeichen.\n",
    "\n",
    "### Ausgabe:\n",
    "- Ein 3D-Array mit One-Hot-Vektoren (z. B. `[10000, 65] → [10000, 65]` oder `[B, T] → [B, T, C]`).\n",
    " Wird häufig für zeichenbasiertes Training von RNNs oder LSTMs genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, n_unique_char):\n",
    "    one_hot = np.zeros((encoded_text.size, n_unique_char)).astype(np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, n_unique_char))\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# (e) Funktion: `generate_batches`\n",
    "\n",
    "Diese Funktion erstellt **Mini-Batches** für das Training eines RNN-/LSTM-Modells auf Zeichenebene.\n",
    "\n",
    "## Zweck\n",
    "Sie teilt den kodierten Text in Trainingsbeispiele mit fester Sequenzlänge auf – inklusive der passenden Zielsequenz (shifted by one character).\n",
    "\n",
    "## Parameter\n",
    "- `encoded_text`: Der numerisch kodierte Text (als NumPy-Array).\n",
    "- `sample_per_batch`: Wie viele Sequenzen pro Batch erzeugt werden.\n",
    "- `seq_len`: Länge jeder Sequenz (in Zeichen).\n",
    "\n",
    "## Ausgabe (via `yield`)\n",
    "- `x`: Eingabesequenzen (Größe: `[batch_size, seq_len]`)\n",
    "- `y`: Zielsequenzen (jeweils um 1 Zeichen nach rechts verschoben)\n",
    "\n",
    "## Besonderheiten\n",
    "- Wenn am Ende nicht genug Zeichen für eine vollständige Sequenz vorhanden sind, wird zyklisch der Anfang verwendet.\n",
    "- Die Funktion ist ein Generator – ideal für die sequentielle Datenverarbeitung im Training.\n",
    "\n",
    "Perfekt für zeichenbasierte Textgenerierung mit LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, sample_per_batch=10, seq_len=50):\n",
    "    char_per_batch = sample_per_batch * seq_len\n",
    "    avail_batch = int(len(encoded_text) / char_per_batch)\n",
    "    encoded_text = encoded_text[: char_per_batch * avail_batch]\n",
    "    encoded_text = encoded_text.reshape((sample_per_batch, -1))\n",
    "\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        x = encoded_text[:, n : n + seq_len]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, n + seq_len]\n",
    "        # for the very last case\n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "        yield x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# (f) Klasse: `LSTM` (zeichenbasiertes Textgenerierungsmodell)\n",
    "\n",
    "Dieses LSTM-Modell ist für zeichenweise Textgenerierung konzipiert (z. B. mit Shakespeare-Texten).\n",
    "\n",
    "## Initialisierung (`__init__`)\n",
    "- **`all_char`**: Liste aller einzigartigen Zeichen im Text.\n",
    "- **`num_hidden`**: Anzahl der Neuronen im versteckten Zustand.\n",
    "- **`num_layers`**: Anzahl der gestapelten LSTM-Schichten.\n",
    "- **`drop_prob`**: Dropout-Rate zur Regularisierung.\n",
    "\n",
    "### Bestandteile:\n",
    "- **`encoder / decoder`**: Mapping zwischen Zeichen und Indizes.\n",
    "- **`self.lstm`**: Mehrschichtiges LSTM mit `batch_first=True`.\n",
    "- **`self.fc_linear`**: Lineare Schicht zur Projektion auf den Zeichenvorrat.\n",
    "- **`self.dropout`**: Dropout-Schicht zur Vermeidung von Overfitting.\n",
    "\n",
    "## Vorwärtsdurchlauf (`forward`)\n",
    "- Führt den Input durch das LSTM und Dropout.\n",
    "- Formt die Ausgabe in 2D um und leitet sie durch die lineare Schicht.\n",
    "- Gibt Vorhersagen (`final_out`) und den neuen Hidden State zurück.\n",
    "\n",
    "## Hidden-State-Initialisierung (`init_hidden`)\n",
    "- Erstellt initiale Hidden States (Hidden + Cell) mit Nullen.\n",
    "- Achtet auf Gerätekompatibilität (`to(device)`).\n",
    "\n",
    "Dieses Modell kann verwendet werden, um Zeichenfolgen zu generieren, basierend auf trainierten Wahrscheinlichkeiten für Zeichenübergänge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, all_char, num_hidden=512, num_layers=3, drop_prob=0.5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.all_char = all_char\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.decoder = dict(enumerate(all_char))\n",
    "        self.encoder = {char: ind for ind, char in decoder.items()}\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            len(self.all_char),\n",
    "            num_hidden,\n",
    "            num_layers,\n",
    "            dropout=drop_prob,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_char))\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        drop_out = self.dropout(lstm_out)\n",
    "        drop_out = drop_out.contiguous().view(-1, self.num_hidden)\n",
    "        final_out = self.fc_linear(drop_out)\n",
    "        return final_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = (\n",
    "            torch.zeros(self.num_layers, batch_size, self.num_hidden).to(device),\n",
    "            torch.zeros(self.num_layers, batch_size, self.num_hidden).to(device),\n",
    "        )\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Hier wird das Modell trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(all_char, num_hidden=1024, num_layers=4, drop_prob=0.6).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# (g) Trainingseinstellungen und Datenaufteilung\n",
    "\n",
    "Dieser Code bereitet alles für das Training eines LSTM-Modells zur Zeichen-Textgenerierung vor.\n",
    "\n",
    "## Optimierung & Verlustfunktion\n",
    "- **`optimizer`**: Verwendet den Adam-Optimizer mit Lernrate 0.001.\n",
    "- **`criterion`**: Nutzt `CrossEntropyLoss`, passend für Klassifikation über Zeichen.\n",
    "\n",
    "## Trainings-/Validierungsaufteilung\n",
    "- **`train_percent = 0.9`**: 90 % der Daten werden für das Training verwendet.\n",
    "- **`train_data`**: Enthält den vorderen Teil des kodierten Textes (Training).\n",
    "- **`val_data`**: Der hintere Teil wird für die Validierung genutzt.\n",
    "\n",
    "## Trainingsparameter\n",
    "- **`num_epoch = 10`**: Das Modell wird über 10 Epochen trainiert.\n",
    "- **`batch_size = 100`**: Jeweils 100 Zeichen-Sequenzen pro Batch.\n",
    "- **`seq_len = 100`**: Jede Eingabesequenz hat eine Länge von 100 Zeichen.\n",
    "- **`tracker = 0`**: Zähler (vermutlich für späteres Tracking wie Verlust).\n",
    "- **`num_char`**: Anzahl der einzigartigen Zeichen (für Output-Größe).\n",
    "\n",
    "## Modell in Trainingsmodus\n",
    "- **`model.train()`**: Schaltet das Modell in den Trainingsmodus (z. B. für Dropout aktiv).\n",
    "\n",
    "Damit ist das Modell bereit für den Trainingsloop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_percent = 0.9\n",
    "train_ind = int(len(encoded_text) * (train_percent))\n",
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]\n",
    "num_epoch = 10\n",
    "batch_size = 100\n",
    "seq_len = 100\n",
    "tracker = 0\n",
    "num_char = max(encoded_text) + 1\n",
    "# set the model on training mode\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# (h) LSTM-Trainingsloop zur Zeichen-Textgenerierung\n",
    "\n",
    "Dieser Code führt das Training des LSTM-Modells über mehrere Epochen durch und validiert regelmäßig den Fortschritt.\n",
    "\n",
    "## Vorbereitung\n",
    "- **`torch.cuda.empty_cache()`**: Gibt nicht verwendeten GPU-Speicher frei.\n",
    "- **`model.train()`**: Aktiviert den Trainingsmodus (z. B. für Dropout).\n",
    "- **Batch-Größe & Sequenzlänge**: 100 Sequenzen à 100 Zeichen pro Batch.\n",
    "\n",
    "---\n",
    "\n",
    "## Epochenschleife (`for epoch in range(num_epoch)`)\n",
    "### Pro Epoche:\n",
    "1. **Hidden-State zurücksetzen**  \n",
    "   Initialisiert den versteckten Zustand für das LSTM.\n",
    "\n",
    "2. **Durch Batch-Generator iterieren**  \n",
    "   Holt Eingabe- und Zielsequenzen (`x`, `y`) aus `generate_batches`.\n",
    "\n",
    "3. **Vorverarbeitung und Training**\n",
    "   - Leert GPU-Speicher erneut.\n",
    "   - Setzt Gradienten auf 0.\n",
    "   - One-Hot-Encodiert die Eingaben.\n",
    "   - Wandelt Eingaben und Ziele in Torch-Tensoren und überträgt sie aufs Gerät.\n",
    "   - Führt einen Vorwärtsdurchlauf durchs Modell.\n",
    "   - Berechnet den Verlust (`CrossEntropyLoss`).\n",
    "   - Führt Backpropagation durch.\n",
    "   - Clipped Gradienten zur Stabilisierung.\n",
    "   - Optimierungsschritt mit Adam.\n",
    "\n",
    "---\n",
    "\n",
    "## Validierung (alle 25 Schritte)\n",
    "- Modell wird auf `eval()` gesetzt (kein Dropout).\n",
    "- Führt Vorhersagen auf Trainingsdaten aus.\n",
    "- Berechnet und speichert Validierungsverluste.\n",
    "- Gibt aktuelle Metriken aus:\n",
    "  ```text\n",
    "  epoch : <nr> tracker : <nr> loss : <wert>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Clear unused GPU memory\n",
    "model.train()\n",
    "\n",
    "# Reduce batch size and sequence length to lower memory usage\n",
    "batch_size = 100  # Reduced from 100\n",
    "seq_len = 100  # Reduced from 100\n",
    "\n",
    "# For every epoch\n",
    "for epoch in range(num_epoch):\n",
    "    # Reset hidden state\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    # Go through every x, y in batch_gen obj\n",
    "    for x, y in generate_batches(val_data, batch_size, seq_len):\n",
    "        tracker += 1\n",
    "        # Clear unused GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        # Zero_grad\n",
    "        model.zero_grad()\n",
    "        # Create input and target\n",
    "        x = one_hot_encoder(x, num_char)\n",
    "        inputs = torch.tensor(x).to(device)\n",
    "        targets = torch.tensor(y).long().to(device)\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        # Now pass through model\n",
    "        lstm_out, hidden = model.forward(inputs, hidden)\n",
    "        # Calculate loss and backprop\n",
    "        loss = criterion(lstm_out, targets.view(batch_size * seq_len))\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "        # For every 25 steps do validation\n",
    "        if tracker % 25 == 0:\n",
    "            # Put model in eval mode\n",
    "            model.eval()\n",
    "            val_hidden = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            for x, y in generate_batches(train_data, batch_size, seq_len):\n",
    "                x = one_hot_encoder(x, num_char)\n",
    "                inputs = torch.tensor(x).to(device)\n",
    "                targets = torch.tensor(y).long().to(device)\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                lstm_out, val_hidden = model.forward(inputs, val_hidden)\n",
    "                loss = criterion(lstm_out, targets.view(batch_size * seq_len))\n",
    "                val_losses.append(loss.item())\n",
    "            print(f\"epoch : {epoch + 1} tracker : {tracker} loss : {loss.item()}\")\n",
    "            model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# (i) Funktion: `pred_next_char`\n",
    "\n",
    "Diese Funktion sagt das **nächste Zeichen** vorher, basierend auf einem gegebenen Zeichen und dem aktuellen Zustand des LSTM-Modells.\n",
    "\n",
    "## Zweck\n",
    "Sie dient zur **zeichengenauen Textgenerierung**, z. B. um ein neues Shakespeare-ähnliches Gedicht zu erzeugen.\n",
    "\n",
    "## Schritt-für-Schritt\n",
    "\n",
    "1. **Eingabe-Zeichen kodieren:**\n",
    "   - Das Zeichen wird in einen Integer-Index umgewandelt (`encoder`).\n",
    "   - Danach wird es in ein One-Hot-Vektor umgewandelt.\n",
    "\n",
    "2. **Tensor erstellen:**\n",
    "   - Der One-Hot-Vektor wird in einen Torch-Tensor umgewandelt und aufs richtige Gerät (GPU/CPU) verschoben.\n",
    "\n",
    "3. **Hidden-State vorbereiten:**\n",
    "   - Der versteckte Zustand wird aktualisiert und auf `.data` gesetzt (für detach von Autograd).\n",
    "\n",
    "4. **Vorhersage machen:**\n",
    "   - Der Tensor wird durch das LSTM-Modell geschickt.\n",
    "   - Die Ausgabe wird mit `softmax` in Wahrscheinlichkeiten umgewandelt.\n",
    "\n",
    "5. **Top-k Sampling:**\n",
    "   - Es werden die `k` wahrscheinlichsten Zeichen gewählt.\n",
    "   - Eins davon wird zufällig (aber gewichtet durch die Wahrscheinlichkeiten) ausgewählt.\n",
    "\n",
    "6. **Ausgabe:**\n",
    "   - Gibt das vorhergesagte Zeichen (als Symbol) zurück sowie den aktualisierten Hidden-State.\n",
    "\n",
    "## Parameter\n",
    "- `model`: Das trainierte LSTM-Modell.\n",
    "- `char`: Das aktuelle Zeichen.\n",
    "- `hidden`: Der aktuelle versteckte Zustand des Modells.\n",
    "- `k`: Top-k-Sampling – wie viele mögliche Zeichen zur Auswahl stehen.\n",
    "\n",
    "**Anwendung:** Diese Funktion wird wiederholt aufgerufen, um Zeichen für Zeichen neuen Text zu generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_next_char(model, char, hidden=None, k=1):\n",
    "    encoded_text = model.encoder[char]\n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "    encoded_text = one_hot_encoder(encoded_text, len(model.all_char))\n",
    "    # create input by encoding and one_hotting the char\n",
    "    inputs = torch.tensor(encoded_text).to(device)\n",
    "    # create hidden state\n",
    "    hidden = tuple([state.data for state in hidden])\n",
    "    # make prediction\n",
    "    lstm_out, hidden = model.forward(inputs, hidden)\n",
    "    # get probabilities\n",
    "    probs = torch.nn.functional.softmax(lstm_out, dim=1).data\n",
    "    probs = probs.cpu()\n",
    "    probs, index_position = probs.topk(k)\n",
    "    index_position = index_position.numpy().squeeze()\n",
    "    probs = probs.numpy().flatten()\n",
    "    probs = probs / probs.sum()\n",
    "    # choose a char from top k\n",
    "    char = np.random.choice(index_position, p=probs)\n",
    "    return model.decoder[char], hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# (k) Funktion: `generate_text`\n",
    "\n",
    "Diese Funktion **generiert neuen Text** Zeichen für Zeichen mithilfe eines trainierten LSTM-Modells.\n",
    "\n",
    "## Schritt-für-Schritt-Erklärung\n",
    "\n",
    "1. **Modell vorbereiten:**\n",
    "   - Das Modell wird auf das richtige Gerät (`CPU` oder `GPU`) verschoben.\n",
    "   - Es wird in den Evaluierungsmodus (`eval()`) versetzt, um Dropout & Co. zu deaktivieren.\n",
    "\n",
    "2. **Initialisierung:**\n",
    "   - Das Startwort (`seed`) wird in eine Liste von Zeichen (`output_char`) zerlegt.\n",
    "   - Der Hidden-State des LSTM wird für Batch-Größe 1 initialisiert.\n",
    "\n",
    "3. **Seed \"verarbeiten\":**\n",
    "   - Für jedes Zeichen im Seed wird `pred_next_char` aufgerufen, um den Hidden-State schrittweise aufzubauen.\n",
    "   - Das letzte vorhergesagte Zeichen wird zur Ausgabe hinzugefügt.\n",
    "\n",
    "4. **Textgenerierung:**\n",
    "   - Es werden `size` weitere Zeichen generiert.\n",
    "   - In jedem Schritt wird das zuletzt erzeugte Zeichen als Eingabe verwendet.\n",
    "   - Das neue Zeichen wird an die Ausgabeliste angehängt.\n",
    "\n",
    "5. **Rückgabe:**\n",
    "   - Die generierten Zeichen werden durch Leerzeichen getrennt und als String zurückgegeben.\n",
    "\n",
    "## Parameter\n",
    "\n",
    "- `model`: Das trainierte LSTM-Modell.\n",
    "- `size`: Anzahl der zu generierenden Zeichen.\n",
    "- `seed`: Startzeichen oder -wort für den Text.\n",
    "- `k`: Wie viele Zeichen für Top-k-Sampling berücksichtigt werden.\n",
    "\n",
    "**Ziel:** Automatische Texterzeugung, z. B. in Shakespeare-Stil oder basierend auf einem anderen Textkorpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed=\"the\", k=1):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    output_char = [c for c in seed]\n",
    "    hidden = model.init_hidden(1)\n",
    "    for char in seed:\n",
    "        char, hidden = pred_next_char(model, char, hidden, k=k)\n",
    "    output_char.append(char)\n",
    "    for i in range(size):\n",
    "        char, hidden = pred_next_char(model, output_char[-1], hidden, k=k)\n",
    "        output_char.append(char)\n",
    "    return \" \".join(output_char)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# (l) Vorhersage \n",
    "\n",
    "## Was passiert hier?\n",
    "\n",
    "Dieser Befehl ruft die Funktion `generate_text(...)` auf und gibt das Ergebnis direkt in der Konsole aus.\n",
    "\n",
    "## Bedeutung der Parameter:\n",
    "\n",
    "- `model`: Das trainierte LSTM-Modell zur Textgenerierung.\n",
    "- `200`: Anzahl der zu generierenden Zeichen.\n",
    "- `\"The \"`: Start-Seed, mit dem der Text beginnt.\n",
    "- `k=2`: **Top-2-Sampling** – bei jeder Vorhersage werden die 2 wahrscheinlichsten nächsten Zeichen berücksichtigt, eines davon wird zufällig gewählt (mit gewichteter Wahrscheinlichkeit).\n",
    "\n",
    "## Ausgabe:\n",
    "\n",
    "Das Ergebnis ist ein automatisch generierter Text, der mit `\"The \"` beginnt und dann 200 Zeichen lang fortgesetzt wird – im Stil des Trainingskorpus (z. B. Shakespeare).  \n",
    "Der generierte Text sieht z. B. so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, 200, seed=\"The \", k=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "https://medium.com/@adi.joshi2018/building-a-character-level-language-model-from-scratch-with-pytorch-9fe248525f1c\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
