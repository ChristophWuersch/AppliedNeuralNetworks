{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> ¬© Christoph W√ºrsch, Fran√ßois Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN13/13.1-neural_style_transfer_with_gif_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f√ºr Ausf√ºhrung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural style transfer\n",
    "\n",
    "\n",
    "Dieses Notizbuch enth√§lt die Codebeispiele aus Kapitel 8, Abschnitt 3 von [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Neben DeepDream gibt es noch eine weitere bedeutende Entwicklung bei auf Deep Learning beruhenden Bildverarbeitungsverfahren: den von *Leon Gatys et al.* im Sommer 2015 vorgestellten **Neural-Style-Algorithmus** zur Stil√ºbertragung [1]. Der Algorithmus wurde kontinuierlich verfeinert, und seit der Erstver√∂ffentlichung wurden viele Varianten entwickelt. Zudem gibt es diverse Smartphone-Apps, die ihn verwenden. Der Einfachheit halber beschr√§nkt sich dieser Abschnitt auf die in der urspr√ºnglichen Arbeit vorgestellte Variante.\n",
    "\n",
    "Der Algorithmus √ºbertr√§gt den Stil eines Referenzbilds auf ein Zielbild und erh√§lt dabei dessen Inhalt. \n",
    "\n",
    "[1] [Leon A. Gatys, Alexander S. Ecker und Matthias Bethge, A Neural Algorithm of Artistic Style,\n",
    "arXiv (2015)](https://arxiv.org/abs/1508.06576)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stil und Inhalt\n",
    "\n",
    "- Der **Stil** beschreibt in diesem Zusammenhang im Wesentlichen *Texturen, Farben und visuelle Muster auf verschiedenen r√§umlichen Skalen* eines Bilds. \n",
    "- Der **Inhalt** ezieht sich auf die allgemeinere Makrostruktur des Bilds. Inder obigen Abbildung geh√∂ren die blauen und gelben kreisf√∂rmigen Pinselstriche in Vincent van Goghs Gem√§lde Sternennacht zum Stil, die Geb√§ude in dem in T√ºbingen aufgenommenen Foto stellen hingegen den Inhalt dar.\n",
    "\n",
    "\n",
    "Die eng mit der Texterzeugung verwandte Idee der Stil√ºbertragung hat in der Bildverarbeitung eine lange Vorgeschichte, die viel weiter zur√ºckreicht als die des erst 2015 entwickelten Neural-Style-Algorithmus. Wie sich herausstellte, lassen sich\n",
    "mit auf Deep Learning beruhenden Implementierungen der Stil√ºbertragung Ergebnisse erzielen, die alles, was mit klassischen Methoden des maschinellen Sehens erreicht werden kann, in den Schatten stellen. \n",
    "\n",
    "Und das l√∂ste eine erstaunliche Renaissance kreativer Anwendungen des maschinellen Sehens aus. Die entscheidende Idee bei der Implementierung der Stil√ºbertragung liegt allen Deep-Learning-Algorithmen zugrunde: Man definiert eine Verlustfunktion, die festlegt, was erreicht werden soll, und minimiert sie. Im vorliegenden Fall ist klar, was erreicht werden soll: Der Inhalt des urspr√ºnglichen Bilds soll erhalten werden, w√§hrend gleichzeitig der Stil eines Referenzbilds √ºbernommen wird. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Der Prozess des Style Transfers ist in Abbildung 7 dargestellt. Um den Style Transfer durchzuf√ºhren,\n",
    "werden drei Bilder ben√∂tigt. Das erste Bild enth√§lt den gew√ºnschten **Stil** $S$ *(Style)*,\n",
    "das zweite enth√§lt den **Inhalt** $C$ *(Content)* und bei dem dritten Bild handelt es sich um das\n",
    "generierte Bild $x$ aus dem Stil-Bild und Inhalt-Bild. \n",
    "\n",
    "Das VGG19 dient beim Style Transfer als Funktion $f_w$, um Feature Maps zu generieren, die entweder dem `style` oder dem `content` entsprechen. Da der **Inhalt** (`content`) des Bildes erst in den letzten Schichten des Netzes erkannt wird, muss f√ºr die Berechnung der Verlustfunktion f√ºr den `content` ein Convolutional Layer verwendet werden, das sich im letzten Layer-Block des VGG19 befindet. \n",
    "\n",
    "Bei der Extraktion des **Stils** f√ºr kleine Muster werden Feature Maps aus den vorderen Schichten verwendet und Feature Maps aus den hinteren Schichten f√ºr grosse Muster. Das Netz wird nicht trainiert, die Gewichte des vortrainierten Netzes werden anfangs eingelesen und anschliessend nicht mehr ver√§ndert. Somit sind die Gewichte $w$ statisch. Um das zu generierende Bild $x$ zu berechnen, muss dieses zun√§chst initialisiert werden. In der vorliegenden Implementierung\n",
    "wurde dies mit dem Inhalt-Bild $C$ gemacht, jedoch w√§re auch weisses Rauschen m√∂glich. Nachdem ein Bild mit VGG19 analysiert wurde, werden verschiedene Feature Maps von Convolutional Layern abgespeichert, die anschliessend f√ºr die Optimierung verwendet werden. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Zielfunktion $\\mathcal{L}=\\alpha\\cdot \\mathcal{L}_C+\\beta \\cdot \\mathcal{L}_S$\n",
    "\n",
    "Wenn wir Inhalt und Stil mathematisch definieren k√∂nnten, s√§he eine passende zu minimierende\n",
    "Verlustfunktion folgenderma√üen aus:\n",
    "\n",
    "```\n",
    "loss = distance(style(reference_image) - style(generated_image)) +\n",
    "       distance(content(original_image) - content(generated_image))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`distance` ist hier eine Normierungsfunktion wie die L2-Norm, `content` ist eine\n",
    "Funktion, die eine Repr√§sentation des Bildinhalts errechnet, und `style` ist eine\n",
    "Funktion, die eine Repr√§sentation des Stils liefert. \n",
    "\n",
    "- Die Minimierung dieser Verlustfunktion sorgt daf√ºr, dass `style(generated_image)` und `style(reference_image)` bzw. `content(generated_image)` und `content(original_image)` von jeweils gleicher Gr√∂ssenordnung sind, und erzielt so die definierte Stil√ºbertragung.\n",
    "- Gatys et al. machten die entscheidende Beobachtung, dass **tiefe neuronale Netze eine M√∂glichkeit bieten, die Funktionen `style` und `content` mathematisch zu definieren**. \n",
    "\n",
    "Sehen wir uns also an, wie das funktioniert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The content loss $\\mathcal{L}_C$: Verlustfunktion f√ºr den Inhalt\n",
    "\n",
    "\n",
    "Wie Sie bereits wissen, enthalten die Aktivierungen der ersten Layer eines NNs lokale Informationen √ºber das Bild, die h√∂her gelegenen Layer dagegen zunehmend *globale* und *abstrakte* Informationen. \n",
    "\n",
    "Mit anderen Worten: **Die Aktivierungen der verschiedenen Layer eines CNNs stellen eine Zerlegung des Inhalts eines\n",
    "Bilds auf unterschiedlichen r√§umlichen Skalen dar.** Deshalb w√ºrde man erwarten, dass der eher globale und abstrakte Inhalt eines Bilds durch die Repr√§sentationen in den oberen Layern eines CNNs erfasst wird.\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aus diesem Grund ist die L2-Norm f√ºr den Abstand zwischen den Aktivierungen eines oberen Layers eines vortrainierten CNNs, berechnet f√ºr das Zielbild, und den Aktivierungen des gleichen Layers, berechnet f√ºr das erzeugte Bild, ein guter\n",
    "Kandidat f√ºr die Verlustfunktion. Auf diese Weise ist sichergestellt, dass das erzeugte Bild aus Perspektive des oberen Layers √Ñhnlichkeit mit dem urspr√ºnglichen Zielinhaltbild besitzt. Wenn die Annahme stimmt, dass die oberen Layer\n",
    "eines CNNs tats√§chlich den Inhalt ihrer Eingabebilder repr√§sentieren, sollte der Inhalt eines Bilds auf diese Weise erhalten bleiben.\n",
    "\n",
    "Um die Optimierung durchzuf√ºhren, wird der Fehler berechnet, welcher zwischen dem generierten Bild $x$ und den vorgegebenen Bilder $C$ und $S$ entsteht. Der **Content-Loss** $\\mathcal{L}_C$ wird *pixelweise* durch die Methode der kleinsten Quadrate nach folgender Formel berechnet.\n",
    "\n",
    "\n",
    "$$\\boxed{ \\mathcal{L}_C = \\sum_i^n \\left[ f_w(C)-f_w(x) \\right]^2} \\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The style loss $\\mathcal{L}_S$: Verlustfunktion f√ºr den Stil\n",
    "\n",
    "Der Stil ist durch lokale Strukturen definiert. Da diese auf dem ganzen Bild vorkommen k√∂nnen,\n",
    "muss der Stil *translationsinvariant* sein. Um dies zu erreichen, wird die **Gram-Matrix**\n",
    "$G$ f√ºr das generierte Bild $x$ und das Stil-Bild $S$ aus den extrahierten Schichten berechnet.\n",
    "\n",
    "Die Verlustfunktion f√ºr den Inhalt verwendet nur einen einzelnen oberen Layer,\n",
    "die von Gatys et al. definierte Verlustfunktion f√ºr den Stil $\\mathcal{L}_S$ verwendet jedoch mehrere Layer eines CNNs: Es wird versucht, das Aussehen des Referenzstilbilds nicht nur auf einer einzigen Skala, sondern auf allen vom CNN extrahierten r√§umlichen Skalen zu erfassen. Gatys et al. verwenden als Verlustfunktion die **Gramsche Matrix** $G$ der Aktivierungen eines Layers, also das Skalarprodukt der Feature-Maps eines Layers. Dieses Skalarprodukt repr√§sentiert gewisserma√üen die Korrelationen zwischen den Merkmalen eines Layers. Diese Korrelationen erfassen eine Statistik der Muster auf einer bestimmten r√§umlichen Skala, die empirisch dem Erscheinungsbild der auf dieser Skala sichtbaren Texturen entsprechen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Um die **Gram-Matrix** zu berechnen, werden die Feature Maps in hochdimensionale Vektoren\n",
    "gewandelt, das heisst s√§mtliche Feature Maps einer Schicht werden zu einer Matrix zusammengef√ºgt.\n",
    "Diese Matrizen werden verwendet, um den mittleren quadratischen Abstand f√ºr\n",
    "jede Schicht gem√§ss Formel (2) zu berechnen\n",
    "\n",
    "$$\\boxed{E_l=\\frac{1}{4N_l^2\\cdot M_l^2} \\sum_{i,j} \\left[ G(S)_{ij}-G(x)_{ij}\\right]^2} \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dabei ist $N_l$ die Anzahl der Feature Maps und $M_l$ die Gr√∂sse der jeweiligen Feature Map. Der\n",
    "Style-Loss $\\mathcal{L}_S$ kann anschliessend durch Aufsummieren von $E_l$ unter Ber√ºcksichtigung der Gewichte $w_l$ berechnet werden:\n",
    "\n",
    "$$\\mathcal{L}_S = \\sum_{l=0}^L w_l \\cdot E_l \\tag{3}$$\n",
    "\n",
    "Die Verlustfunktion f√ºr den Stil hat also zum Ziel, √§hnliche interne Korrelationen zwischen den Aktivierungen verschiedener Layer im Referenzstilbild beim erzeugten Bild zu erhalten. Umgekehrt ist dadurch gew√§hrleistet, dass die auf verschiedenen\n",
    "r√§umlichen Skalen erkennbaren Texturen des Referenzstilbilds im erzeugten Bild √§hnlich aussehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Der Gesamtverlust $\\mathcal{L}$\n",
    "\n",
    "Aus den beiden Formeln $\\mathcal{L}_C$ (1) und $\\mathcal{L}_S$ (2) kann nun der totale Fehler  berechnet werden:\n",
    "\n",
    "## $$ \\boxed{\\mathcal{L} = \\alpha \\cdot \\mathcal{L}_C + \\beta \\cdot \\mathcal{L}_S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir k√∂nnen somit **ein vortrainiertes CNN verwenden, um eine Verlustfunktion zu definieren**, die Folgendes leistet:\n",
    "- Sie erh√§lt den Inhalt, indem sie √§hnliche Aktivierungen allgemeiner Layer des Zielinhaltbilds im erzeugten Bild √ºbernimmt. Das CNN sollte feststellen, dass das Zielinhaltbild und das erzeugte Bild die gleichen Objekte enthalten.\n",
    "- Sie erh√§lt den Stil, indem sie √§hnliche *Korrelationen der Aktivierungen* sowohl in tiefer liegenden als auch in h√∂her liegenden Layern √ºbernimmt. Merkmalskorrelationen erfassen Texturen: Das erzeugte Bild und das Referenzstilbild sollten auf verschiedenen r√§umlichen Skalen die jeweils gleichen Texturen enthalten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"Bilder/FastStyleTransfer_Johnson.PNG\" width=\"840\" align=\"center\"/>\n",
    "\n",
    "[3] [Johnson, J., Alahi, A. und Li, F.-F., ‚ÄúPerceptual Losses for Real-Time Style Transfer\n",
    "and Super-Resolution‚Äù, CoRR, Jg. abs/1603.08155, 2016.](http://web.eecs.umich.edu/~justincj/papers/eccv16/JohnsonECCV16.pdf)\n",
    "\n",
    "[4] [Johnson, J., Alahi, A. und Li, F.-F., ‚ÄúPerceptual Losses for Real-Time Style Transfer\n",
    "and Super-Resolution: Supplementary Material‚Äù, CoRR, 2016.](http://web.eecs.umich.edu/~justincj/papers/eccv16/JohnsonECCV16Supplementary.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Style Transfer mit PyTorch\n",
    "\n",
    "Beim Neural Style Transfer (NST) wird der **Inhalt eines Bildes** mit dem **k√ºnstlerischen Stil eines anderen Bildes** kombiniert. Die zugrunde liegende Idee stammt aus der Arbeit von *Gatys et al.* und nutzt ein vortrainiertes Convolutional Neural Network (CNN), in der Regel **VGG19**, um Stil- und Inhaltsinformationen aus Bildern zu extrahieren.\n",
    "\n",
    "Das Verfahren nutzt nicht die Klassifikationsf√§higkeit des Netzwerks, sondern die **Aktivierungen seiner Zwischenlayer**, um sowohl die **semantischen Inhalte** als auch die **stilistischen Eigenschaften** eines Bildes zu erfassen.\n",
    "\n",
    "### Vorgehensweise in drei Schritten:\n",
    "\n",
    "1. **Feature-Extraktion mit VGG19**  \n",
    "   Ein tiefes CNN (hier: VGG19) wird verwendet, um die Aktivierungen der Zwischenlayer f√ºr drei Bilder zu berechnen:\n",
    "   - das Content-Bild (Zielinhalt)\n",
    "   - das Style-Bild (Stilvorlage)\n",
    "   - das generierte Bild (wird trainiert)\n",
    "\n",
    "2. **Definition der Verlustfunktion**  \n",
    "   Die sogenannte Perceptual Loss setzt sich zusammen aus:\n",
    "   - **Content Loss**: Vergleich der Feature-Maps zwischen generiertem und Content-Bild\n",
    "   - **Style Loss**: Vergleich der Gram-Matrizen zwischen generiertem und Style-Bild\n",
    "   - **TV Loss (optional)**: Gl√§ttung des Bildes durch Minimierung lokaler Unterschiede\n",
    "\n",
    "3. **Optimierung des Bildes**  \n",
    "   Das Zielbild wird pixelweise durch **Gradientenabstieg** so ver√§ndert, dass es gleichzeitig dem Inhalt des Content-Bildes und dem Stil des Style-Bildes entspricht.\n",
    "\n",
    "\n",
    "> Damit das Verfahren gut funktioniert, sollten Content- und Style-Bild in √§hnlicher Gr√∂√üe vorliegen. In der Regel wird die H√∂he der Bilder auf z.‚ÄØB. **400 Pixel** skaliert, um Rechenzeit zu sparen und bessere Ergebnisse zu erzielen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Neural Style Transfer mit VGG19 (PyTorch)\n",
    "# Basierend auf der offiziellen PyTorch-Tutorial-Implementierung\n",
    "# -----------------------------------------------\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torchvision.transforms.functional import InterpolationMode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuerst wird gepr√ºft, ob eine CUDA-f√§hige GPU verf√ºgbar ist. Wenn ja, wird \"cuda\" als Ger√§t gew√§hlt, sonst \"cpu\". Dieses Ger√§t wird dann als Standardger√§t f√ºr alle folgenden Tensoroperationen gesetzt, sodass man nicht st√§ndig .to(device) angeben muss. Zum Schluss wird das gew√§hlte Ger√§t zur Kontrolle ausgegeben. Das sorgt f√ºr flexiblen, ger√§teunabh√§ngigen Code, der sowohl auf GPU als auch CPU l√§uft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Ger√§teeinstellung & Default\n",
    "# -----------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "print(\"Verwendetes Ger√§t:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bildpfade & Download\n",
    "\n",
    "### Was passiert hier?\n",
    "\n",
    "1. **Ordner anlegen**  \n",
    "   ```python\n",
    "   os.makedirs(\"data/Bilder\", exist_ok=True)\n",
    "   ```\n",
    "   Es wird ein Ordner `data/Bilder` erstellt, falls er noch nicht existiert. So wird sichergestellt, dass der Speicherort f√ºr die Bilder verf√ºgbar ist.\n",
    "\n",
    "2. **URLs und Pfade definieren**  \n",
    "   Es werden zwei Bild-URLs festgelegt:\n",
    "   - Ein **Content-Bild** (Portr√§tfoto einer Frau)\n",
    "   - Ein **Style-Bild** (Gem√§lde von Ljubow Popowa)\n",
    "\n",
    "   Zus√§tzlich werden die lokalen Speicherpfade definiert, unter denen diese Bilder abgelegt werden sollen:\n",
    "   - `data/Bilder/portrait.jpg`\n",
    "   - `data/Bilder/popova_style.jpg`\n",
    "\n",
    "3. **User-Agent setzen**  \n",
    "   Einige Server blockieren Anfragen ohne User-Agent. Daher wird ein Header gesetzt, um sich als normaler Browser auszugeben:\n",
    "   ```python\n",
    "   headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "   ```\n",
    "\n",
    "4. **Download-Funktion definieren**  \n",
    "   Die Funktion `download_image(url, path)` l√§dt ein Bild von einer URL herunter und speichert es unter dem angegebenen Pfad.\n",
    "\n",
    "5. **Bilder nur bei Bedarf herunterladen**  \n",
    "   Vor dem Herunterladen wird √ºberpr√ºft, ob die Dateien bereits vorhanden sind. Falls nicht, werden sie heruntergeladen:\n",
    "   ```python\n",
    "   if not os.path.exists(content_path):\n",
    "       download_image(content_url, content_path)\n",
    "   ```\n",
    "\n",
    "6. **Statusmeldung**  \n",
    "   Am Ende wird eine Best√§tigung ausgegeben:\n",
    "   ```\n",
    "   ‚úÖ Bilder erfolgreich heruntergeladen und gespeichert.\n",
    "   ```\n",
    "\n",
    "### Zweck:\n",
    "Dieser Code stellt sicher, dass die ben√∂tigten Bilder (Content & Style) lokal verf√ºgbar sind, ohne sie jedes Mal neu herunterladen zu m√ºssen. Das ist besonders hilfreich f√ºr Anwendungen wie **Neural Style Transfer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Bild-Download\n",
    "# -----------------------------------------------\n",
    "os.makedirs(\"data/Bilder\", exist_ok=True)\n",
    "\n",
    "content_url = \"https://www.myposter.de/magazin/wp-content/uploads/2016/06/Portrait-Frau-lachend-shutterstock_381113020_kl.jpg\"\n",
    "style_url = (\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/7/78/Popova_Air_Man_Space.jpg\"\n",
    ")\n",
    "content_path_default = \"data/Bilder/portrait.jpg\"\n",
    "style_path_default = \"data/Bilder/popova_style.jpg\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "\n",
    "def download_image(url, path):\n",
    "    req = urllib.request.Request(url, headers=headers)\n",
    "    with urllib.request.urlopen(req) as response, open(path, \"wb\") as out_file:\n",
    "        out_file.write(response.read())\n",
    "\n",
    "\n",
    "if not os.path.exists(content_path_default):\n",
    "    download_image(content_url, content_path_default)\n",
    "if not os.path.exists(style_path_default):\n",
    "    download_image(style_url, style_path_default)\n",
    "\n",
    "print(\"‚úÖ Portrait- und Popova-Style-Bild erfolgreich geladen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# StyleTransfer-Bilder herunterladen\n",
    "# -----------------------------------------------\n",
    "target_dir = \"data/Bilder/StyleTransfer\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "filenames = [\n",
    "    \"Klimt2.jpg\",\n",
    "    \"Kopenhagen.jpg\",\n",
    "    \"Mosaic2.jpg\",\n",
    "    \"Sonnenblumen.jpg\",\n",
    "    \"Van_Gogh.jpg\",\n",
    "    \"Vassily_Kandinsky7.jpg\",\n",
    "    \"candy.jpg\",\n",
    "    \"composition-with-figures_popova.jpg\",\n",
    "    \"composition.jpg\",\n",
    "    \"graffiti.jpg\",\n",
    "    \"popova.png\",\n",
    "    \"portrait.jpg\",\n",
    "    \"portrait_mann.jpg\",\n",
    "    \"portrait_women.jpg\",\n",
    "    \"simpsons.jpg\",\n",
    "    \"udnie.jpg\",\n",
    "]\n",
    "\n",
    "base_url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/data/StyleTransfer/\"\n",
    "\n",
    "for name in filenames:\n",
    "    save_path = os.path.join(target_dir, name)\n",
    "    if os.path.exists(save_path):\n",
    "        continue\n",
    "    try:\n",
    "        urllib.request.urlretrieve(base_url + name, save_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Bildpfade\n",
    "# -----------------------------------------------\n",
    "content_images = {\n",
    "    \"portrait\": \"data/Bilder/StyleTransfer/portrait.jpg\",\n",
    "    \"Kopenhagen\": \"data/Bilder/StyleTransfer/Kopenhagen.jpg\",\n",
    "    \"portrait_mann\": \"data/Bilder/StyleTransfer/portrait_mann.jpg\",\n",
    "    \"portrait_women\": \"data/Bilder/StyleTransfer/portrait_women.jpg\",\n",
    "    \"simpsons\": \"data/Bilder/StyleTransfer/simpsons.jpg\",\n",
    "}\n",
    "\n",
    "style_images = {\n",
    "    \"popova\": \"data/Bilder/StyleTransfer/popova.png\",\n",
    "    \"candy\": \"data/Bilder/StyleTransfer/candy.jpg\",\n",
    "    \"Van_Gogh\": \"data/Bilder/StyleTransfer/Van_Gogh.jpg\",\n",
    "    \"udnie\": \"data/Bilder/StyleTransfer/udnie.jpg\",\n",
    "    \"Vassily_Kandinsky7\": \"data/Bilder/StyleTransfer/Vassily_Kandinsky7.jpg\",\n",
    "    \"composition\": \"data/Bilder/StyleTransfer/composition.jpg\",\n",
    "    \"composition_popova\": \"data/Bilder/StyleTransfer/composition-with-figures_popova.jpg\",\n",
    "    \"graffiti\": \"data/Bilder/StyleTransfer/graffiti.jpg\",\n",
    "    \"Mosaic2\": \"data/Bilder/StyleTransfer/Mosaic2.jpg\",\n",
    "    \"Sonnenblumen\": \"data/Bilder/StyleTransfer/Sonnenblumen.jpg\",\n",
    "    \"Klimt2\": \"data/Bilder/StyleTransfer/Klimt2.jpg\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Dropdown-Men√ºs mit ipywidgets\n",
    "# -----------------------------------------------\n",
    "content_dropdown = widgets.Dropdown(\n",
    "    options=content_images,\n",
    "    description=\"Content:\",\n",
    "    layout=widgets.Layout(width=\"60%\"),\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "style_dropdown = widgets.Dropdown(\n",
    "    options=style_images,\n",
    "    description=\"Style:\",\n",
    "    layout=widgets.Layout(width=\"60%\"),\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "# Ausgabe-Label\n",
    "output_widget = widgets.Output()  # Umbenennung, damit kein Konflikt entsteht\n",
    "\n",
    "# Globale Variablen f√ºr die aktuellen Pfade\n",
    "selected_content_path = content_dropdown.value\n",
    "selected_style_path = style_dropdown.value\n",
    "\n",
    "\n",
    "def update_paths(change=None):\n",
    "    global selected_content_path, selected_style_path\n",
    "    output_widget.clear_output()\n",
    "    with output_widget:\n",
    "        selected_content_path = content_dropdown.value\n",
    "        selected_style_path = style_dropdown.value\n",
    "        print(\"‚úÖ Ausgew√§hlte Pfade:\")\n",
    "        print(\"Content:\", selected_content_path)\n",
    "        print(\"Style:\", selected_style_path)\n",
    "\n",
    "\n",
    "# Verbindung der Dropdowns mit dem Callback\n",
    "content_dropdown.observe(update_paths, names=\"value\")\n",
    "style_dropdown.observe(update_paths, names=\"value\")\n",
    "\n",
    "# Anzeige\n",
    "display(content_dropdown, style_dropdown, output_widget)\n",
    "\n",
    "# Initiales Anzeigen der Auswahl\n",
    "update_paths()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bildverarbeitung\n",
    "\n",
    "1. **Bildgr√∂√üe festlegen**  \n",
    "   Die Zielgr√∂√üe f√ºr die Bilder wird abh√§ngig vom Ger√§t gew√§hlt:\n",
    "   - **512 Pixel**, wenn eine GPU verf√ºgbar ist (schnelleres Rechnen)\n",
    "   - **128 Pixel**, wenn nur eine CPU verf√ºgbar ist (ressourcenschonender)\n",
    "\n",
    "2. **Transformationen definieren**  \n",
    "   `loader` ist eine Pipeline zur Bildvorverarbeitung:\n",
    "   - **Resize** auf die Zielgr√∂√üe mit BICUBIC-Interpolation\n",
    "   - **CenterCrop**, um das Bild quadratisch zuzuschneiden\n",
    "   - **ToTensor**, um das PIL-Bild in ein PyTorch-Tensor (mit Werten in [0, 1]) umzuwandeln\n",
    "\n",
    "3. **`unloader`**  \n",
    "   Eine Umkehrfunktion: Wandelt ein Tensor-Bild wieder zur√ºck in ein PIL-Bild zur Visualisierung.\n",
    "\n",
    "4. **Funktion: `image_loader(image_path)`**  \n",
    "   - √ñffnet ein Bild von Pfad `image_path`\n",
    "   - Konvertiert es zu RGB\n",
    "   - Wendet die Transformationspipeline `loader` an\n",
    "   - F√ºgt eine Batch-Dimension hinzu (`unsqueeze(0)`)\n",
    "   - Gibt das Bild als Float-Tensor auf dem richtigen Ger√§t (`cpu` oder `cuda`) zur√ºck\n",
    "\n",
    "5. **Funktion: `imshow(tensor, title=None)`**  \n",
    "   - Kopiert das Tensorbild zur√ºck auf die CPU\n",
    "   - Entfernt die Batch-Dimension\n",
    "   - Wandelt es mit `unloader` in ein PIL-Bild um\n",
    "   - Zeigt es mit `matplotlib` an\n",
    "   - Optional kann ein Titel angegeben werden\n",
    "   - Achsen werden ausgeblendet, und ein kurzes `pause()` sorgt daf√ºr, dass das Bild korrekt dargestellt wird\n",
    "\n",
    "### Zweck:\n",
    "Dieser Block stellt sicher, dass Bilder korrekt geladen, skaliert und f√ºr die Verarbeitung (z.‚ÄØB. im Style Transfer) als Tensor vorbereitet werden ‚Äì und sp√§ter wieder anschaulich dargestellt werden k√∂nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Bildverarbeitung\n",
    "# -----------------------------------------------\n",
    "imsize = 512 if torch.cuda.is_available() else 128\n",
    "loader = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(imsize, interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(imsize),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "\n",
    "def image_loader(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone().squeeze(0)\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.pause(0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilder laden\n",
    "\n",
    "1. **Content- und Style-Bilder laden**  \n",
    "   Mit der Funktion `image_loader(...)` werden die beiden vorbereiteten Bilder (Content und Style) eingelesen und in passende PyTorch-Tensoren umgewandelt:\n",
    "   ```python\n",
    "   content_img = image_loader(content_path)\n",
    "   style_img = image_loader(style_path)\n",
    "   ```\n",
    "   Diese Bilder enthalten nun eine zus√§tzliche Batch-Dimension und liegen als 4D-Tensoren vor: `(1, 3, H, W)`.\n",
    "\n",
    "2. **Gr√∂√üenvergleich**  \n",
    "   Es wird gepr√ºft, ob beide Bilder die gleiche Gr√∂√üe haben:\n",
    "   ```python\n",
    "   if content_img.size() != style_img.size():\n",
    "       raise ValueError(\"Style und Content m√ºssen gleiche Gr√∂sse haben!\")\n",
    "   ```\n",
    "   Das ist notwendig, da der Style-Transfer-Prozess voraussetzt, dass Content- und Style-Bild dieselben Dimensionen besitzen ‚Äì sonst funktionieren die Operationen (z.‚ÄØB. Feature-Extraktion oder Loss-Berechnung) nicht korrekt.\n",
    "\n",
    "### Zweck:\n",
    "Dieser Block l√§dt die beiden Hauptbilder f√ºr den Style Transfer und stellt sicher, dass sie korrekt und gleich gro√ü sind ‚Äì eine wichtige Voraussetzung f√ºr die Weiterverarbeitung im Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Bilder laden\n",
    "# -----------------------------------------------\n",
    "content_img = image_loader(selected_content_path)\n",
    "style_img = image_loader(selected_style_path)\n",
    "\n",
    "if content_img.size() != style_img.size():\n",
    "    raise ValueError(\"Style und Content m√ºssen gleiche Gr√∂sse haben!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verlustfunktionen f√ºr Neural Style Transfer\n",
    "\n",
    "In diesem Abschnitt werden zwei zentrale Verlustfunktionen definiert: eine f√ºr den **Content (Inhalt)** und eine f√ºr den **Style (Stil)**. Diese messen, wie gut das optimierte Bild dem Content- bzw. Style-Ziel entspricht.\n",
    "\n",
    "### √úbersicht der Funktionen und Klassen\n",
    "\n",
    "| Name              | Typ         | Zweck                                                                                     |\n",
    "|-------------------|--------------|--------------------------------------------------------------------------------------------|\n",
    "| `ContentLoss`     | Klasse       | Misst den Unterschied zwischen den **Feature-Maps** des Content-Bildes und des generierten Bildes mithilfe von **MSE-Loss**. Nur der Inhalt wird ber√ºcksichtigt. |\n",
    "| `gram_matrix`     | Funktion     | Berechnet die **Gram-Matrix**, welche die Korrelationen zwischen den Kan√§len eines Features beschreibt. Sie ist die Grundlage zur Berechnung des Style-Loss. |\n",
    "| `StyleLoss`       | Klasse       | Vergleicht die **Gram-Matrix** des Style-Bildes mit der des generierten Bildes mittels **MSE-Loss**. Dadurch wird der visuelle Stil √ºbertragen. |\n",
    "\n",
    "\n",
    "### Details zu den Komponenten\n",
    "\n",
    "#### `ContentLoss`\n",
    "- Speichert ein \"Ziel\"-Feature (`target`), das vom Content-Bild stammt.\n",
    "- Im Forward-Pass wird der MSE-Loss zwischen dem aktuellen Input (aus dem generierten Bild) und dem Ziel berechnet.\n",
    "- Gibt den Input unver√§ndert zur√ºck, damit das Bild weiter durch das Netz flie√üen kann.\n",
    "\n",
    "#### `gram_matrix(input)`\n",
    "- Formt das Input-Feature (Tensor der Form `[batch, channels, height, width]`) um in ein 2D-Feature.\n",
    "- Multipliziert das Feature mit seiner Transponierten, um die Korrelationen zu berechnen.\n",
    "- Normalisiert die Gram-Matrix, damit die Werte unabh√§ngig von der Bildgr√∂√üe sind.\n",
    "\n",
    "#### `StyleLoss`\n",
    "- Wandelt das Ziel-Feature des Style-Bildes in eine Gram-Matrix um.\n",
    "- Im Forward-Pass wird die Gram-Matrix des Inputs berechnet und mit dem Ziel verglichen (MSE-Loss).\n",
    "- Gibt ebenfalls den Input unver√§ndert zur√ºck.\n",
    "\n",
    "\n",
    "### Zweck:\n",
    "Diese Verlustfunktionen sind essenziell f√ºr den Style Transfer:\n",
    "- Der **Content-Loss** bewahrt die Struktur und Formen des Ausgangsbildes.\n",
    "- Der **Style-Loss** √ºbertr√§gt die Textur, Farbverteilung und Muster des Stilbildes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Verlustfunktionen\n",
    "# -----------------------------------------------\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super().__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "\n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()\n",
    "    features = input.view(a * b, c * d)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super().__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Normalisierung (f√ºr VGG)\n",
    "# -----------------------------------------------\n",
    "cnn = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = mean.view(-1, 1, 1)\n",
    "        self.std = std.view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisierung (f√ºr VGG)\n",
    "\n",
    "Beim Style Transfer wird ein vortrainiertes VGG19-Netzwerk verwendet. Dieses erwartet Eingabebilder, die mit bestimmten Mittelwerten und Standardabweichungen normalisiert wurden.\n",
    "\n",
    "\n",
    "### Erkl√§rung der Bestandteile\n",
    "\n",
    "| Name                         | Typ       | Zweck                                                                                   |\n",
    "|------------------------------|------------|------------------------------------------------------------------------------------------|\n",
    "| `cnn`                        | Modell     | Enth√§lt die **Feature-Extraktionsschichten** des vortrainierten VGG19-Modells (ohne Klassifikationskopf). |\n",
    "| `cnn_normalization_mean`     | Tensor     | Mittelwerte der Farbkan√§le (RGB), mit denen die Bilder f√ºr VGG normalisiert werden.     |\n",
    "| `cnn_normalization_std`      | Tensor     | Standardabweichungen der Farbkan√§le (RGB), f√ºr die Normalisierung der Eingabebilder.    |\n",
    "| `Normalization`              | Klasse     | Modul zur Durchf√ºhrung der Normalisierung im Forward-Pass des Netzwerks.                |\n",
    "\n",
    "\n",
    "### Details\n",
    "\n",
    "#### VGG-Modell\n",
    "```python\n",
    "cnn = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n",
    "```\n",
    "- L√§dt das **vortrainierte VGG19-Netzwerk** (ImageNet-Gewichte).\n",
    "- `.features` extrahiert nur den **Feature-Teil** des Netzwerks (ohne die Klassifizierungsschichten).\n",
    "- `.eval()` versetzt das Modell in den **Inferenzmodus**, d.h. ohne Dropout oder BatchNorm-Updates.\n",
    "\n",
    "#### Mittelwert & Standardabweichung\n",
    "```python\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "```\n",
    "- Diese Werte stammen aus der **ImageNet-Datenverteilung** (Basis f√ºr das Training von VGG19).\n",
    "- Sie werden ben√∂tigt, damit die Eingabebilder in der gleichen Weise normalisiert werden wie beim Training.\n",
    "\n",
    "#### `Normalization`-Klasse\n",
    "- Erbt von `nn.Module`.\n",
    "- Die RGB-Kanal-Mittelwerte und Standardabweichungen werden so reshaped, dass sie auf Tensoren im Format `[B, C, H, W]` anwendbar sind.\n",
    "- Im `forward(...)` wird das Bild kanalweise normalisiert\n",
    "\n",
    "\n",
    "### Zweck:\n",
    "Diese Normalisierung ist **essentiell**, damit die Bilder mit dem VGG-Netz korrekt verarbeitet werden. Ohne diese Anpassung w√ºrden die Features falsch interpretiert, was zu schlechten Ergebnissen beim Style Transfer f√ºhren w√ºrde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Modellaufbau mit Verlustschichten\n",
    "# -----------------------------------------------\n",
    "content_layers_default = [\"conv_5\", \"conv_6\"]\n",
    "style_layers_default = [\"conv_1\", \"conv_2\", \"conv_3\", \"conv_4\"]\n",
    "\n",
    "\n",
    "def get_style_model_and_losses(\n",
    "    cnn,\n",
    "    norm_mean,\n",
    "    norm_std,\n",
    "    style_img,\n",
    "    content_img,\n",
    "    content_layers=content_layers_default,\n",
    "    style_layers=style_layers_default,\n",
    "):\n",
    "    normalization = Normalization(norm_mean, norm_std).to(device)\n",
    "    content_losses, style_losses = [], []\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = f\"conv_{i}\"\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = f\"relu_{i}\"\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = f\"pool_{i}\"\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = f\"bn_{i}\"\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unrecognized layer: {layer.__class__.__name__}\")\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # Modell k√ºrzen\n",
    "    for j in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[j], ContentLoss) or isinstance(model[j], StyleLoss):\n",
    "            break\n",
    "    model = model[: (j + 1)]\n",
    "    return model, style_losses, content_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimierung vorbereiten\n",
    "\n",
    "Dieser Abschnitt definiert die Optimierungsmethode, mit der das Bild f√ºr den Style Transfer angepasst wird.\n",
    "\n",
    "\n",
    "### Funktion: `get_input_optimizer(input_img)`\n",
    "\n",
    "| Bestandteil                 | Bedeutung                                                                 |\n",
    "|----------------------------|---------------------------------------------------------------------------|\n",
    "| `input_img.requires_grad_()` | Aktiviert das **Gradient Tracking** f√ºr das Bild, da es optimiert werden soll. |\n",
    "| `optim.LBFGS([...])`         | Verwendet den **L-BFGS-Optimierer** aus PyTorch, eine effektive Methode f√ºr kleine Probleme mit wenigen Parametern. |\n",
    "\n",
    "\n",
    "### Zweck:\n",
    "Beim Neural Style Transfer wird **nicht** das Netzwerk trainiert, sondern das **Eingabebild** (das zu stylende Bild) direkt optimiert.\n",
    "\n",
    "Die Funktion gibt einen Optimierer zur√ºck, der das `input_img` schrittweise ver√§ndert, sodass der **Style-Loss** und der **Content-Loss** minimiert werden.\n",
    "\n",
    "Der **L-BFGS-Optimierer** eignet sich besonders gut f√ºr Style Transfer, da er konvergiert, ohne dass eine hohe Anzahl an Iterationen n√∂tig ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Optimierung vorbereiten\n",
    "# -----------------------------------------------\n",
    "def get_input_optimizer(input_img):\n",
    "    return optim.LBFGS([input_img.requires_grad_()], lr=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Transfer ausf√ºhren\n",
    "\n",
    "In dieser Funktion wird der eigentliche **Style Transfer** durchgef√ºhrt: Das Eingabebild wird so optimiert, dass es den Inhalt des Content-Bildes und den Stil des Style-Bildes kombiniert.\n",
    "\n",
    "\n",
    "### Funktion: `run_style_transfer(...)`\n",
    "\n",
    "| Parameter         | Bedeutung                                                                 |\n",
    "|-------------------|---------------------------------------------------------------------------|\n",
    "| `cnn`             | Vortrainiertes VGG19-Netz (nur Feature-Teil)                             |\n",
    "| `norm_mean/std`   | Mittelwert & Standardabweichung f√ºr VGG-Normalisierung                   |\n",
    "| `content_img`     | Bild, dessen Inhalt beibehalten werden soll                              |\n",
    "| `style_img`       | Bild, dessen Stil √ºbertragen werden soll                                 |\n",
    "| `input_img`       | Das zu optimierende Bild (initial meist eine Kopie von `content_img`)    |\n",
    "| `num_steps`       | Anzahl der Optimierungsschritte (Standard: 500)                          |\n",
    "| `style_weight`    | Gewichtung des Style-Loss (h√∂her ‚Üí mehr Stil√ºbernahme)                   |\n",
    "| `content_weight`  | Gewichtung des Content-Loss (h√∂her ‚Üí mehr Inhaltstreue)                  |\n",
    "\n",
    "\n",
    "### Ablauf:\n",
    "\n",
    "1. **Modell vorbereiten**  \n",
    "   Die Hilfsfunktion `get_style_model_and_losses(...)` erstellt ein zusammengesetztes Modell mit eingef√ºgten Style- und Content-Loss-Modulen.\n",
    "\n",
    "2. **Optimierer initialisieren**  \n",
    "   Das Eingabebild wird mit `get_input_optimizer(...)` als optimierbares Objekt vorbereitet (L-BFGS-Optimierer).\n",
    "\n",
    "3. **Optimierungsschleife starten**  \n",
    "   F√ºr die festgelegte Anzahl an Schritten (`num_steps`) wird die folgende Funktion wiederholt:\n",
    "\n",
    "   #### `closure()`:\n",
    "   - Clamped (`input_img.clamp_(0, 1)`): Bild wird auf g√ºltige Pixelwerte (0‚Äì1) begrenzt.\n",
    "   - Optimierer-Zustand wird zur√ºckgesetzt.\n",
    "   - Forward-Pass durch das Modell ‚Üí Verluste werden automatisch berechnet.\n",
    "   - Style- und Content-Loss werden aufsummiert und gewichtet.\n",
    "   - Backward-Pass: Gradienten werden berechnet.\n",
    "   - Alle 50 Schritte wird der aktuelle Loss ausgegeben.\n",
    "\n",
    "4. **Letztes Clamping**  \n",
    "   Nach der Optimierung wird das finale Bild nochmals auf g√ºltige Werte beschr√§nkt.\n",
    "\n",
    "5. **Ergebnis**  \n",
    "   Die Funktion gibt das transformierte Bild zur√ºck ‚Äì eine Kombination aus Stil und Inhalt.\n",
    "\n",
    "\n",
    "### Zweck:\n",
    "Dies ist die zentrale Schleife, die den **Neural Style Transfer** durchf√ºhrt. Sie ver√§ndert schrittweise das Eingabebild, bis es sowohl den gew√ºnschten Stil als auch den gew√ºnschten Inhalt bestm√∂glich wiedergibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def run_style_transfer(\n",
    "    cnn,\n",
    "    norm_mean,\n",
    "    norm_std,\n",
    "    content_img,\n",
    "    style_img,\n",
    "    input_img,\n",
    "    num_steps=1000,\n",
    "    style_weight=1e6,\n",
    "    content_weight=1,\n",
    "    show_every=100,  # Anzeige-Intervall (Bildanzeige)\n",
    "    gif_every=20,  # GIF-Frame-Intervall (GIF speichern)\n",
    "    content_path=None,\n",
    "    style_path=None,\n",
    "):\n",
    "    print(\"Modell wird erstellt...\")\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(\n",
    "        cnn, norm_mean, norm_std, style_img, content_img\n",
    "    )\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Optimierung l√§uft...\")\n",
    "\n",
    "    # Fortschrittsanzeige vorbereiten\n",
    "    progress_bar = widgets.IntProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=num_steps,\n",
    "        description=\"0%\",\n",
    "        bar_style=\"info\",\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    info_box = widgets.HTML(value=\"<b>Initialisierung l√§uft...</b>\")\n",
    "    metrics_box = widgets.HTML()\n",
    "\n",
    "    display(\n",
    "        widgets.VBox(\n",
    "            [\n",
    "                widgets.HTML(\n",
    "                    \"<h4 style='margin:0;'>üé® <b>Style Transfer Fortschritt</b></h4>\"\n",
    "                ),\n",
    "                progress_bar,\n",
    "                info_box,\n",
    "                metrics_box,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Log-Ausgabe vorbereiten\n",
    "    log_output = widgets.Output(\n",
    "        layout={\"border\": \"1px solid #ccc\", \"height\": \"250px\", \"overflow_y\": \"auto\"}\n",
    "    )\n",
    "    log_output.clear_output()\n",
    "    display(\n",
    "        widgets.VBox(\n",
    "            [\n",
    "                widgets.HTML(\n",
    "                    \"<h4 style='margin-top:30px;'>üìú <b>Iteration-Verlauf</b></h4>\"\n",
    "                ),\n",
    "                log_output,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    run = 0\n",
    "    frames = []\n",
    "    last_style_loss = None\n",
    "    last_content_loss = None\n",
    "\n",
    "    while run <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            with torch.no_grad():\n",
    "                input_img.clamp_(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = sum(sl.loss for sl in style_losses)\n",
    "            content_score = sum(cl.loss for cl in content_losses)\n",
    "            loss = style_score * style_weight + content_score * content_weight\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        run += 1\n",
    "        progress_bar.value = run\n",
    "        progress_bar.description = f\"{int((run / num_steps) * 100)}%\"\n",
    "\n",
    "        # GIF-Frame speichern alle gif_every Iterationen\n",
    "        if run % gif_every == 0 or run == num_steps:\n",
    "            with torch.no_grad():\n",
    "                input_img.clamp_(0, 1)\n",
    "                img_clone = input_img.clone()\n",
    "                img_np = img_clone.squeeze().cpu().numpy()\n",
    "                img_np = img_np.transpose(1, 2, 0)\n",
    "                img_uint8 = (img_np * 255).astype(\"uint8\")\n",
    "                frame = Image.fromarray(img_uint8)\n",
    "                frames.append(frame)\n",
    "\n",
    "        # Bildanzeige nur alle show_every Iterationen\n",
    "        if run % show_every == 0 or run == num_steps:\n",
    "            with torch.no_grad():\n",
    "                input_img.clamp_(0, 1)\n",
    "                style_score = sum(sl.loss for sl in style_losses)\n",
    "                content_score = sum(cl.loss for cl in content_losses)\n",
    "\n",
    "                style_diff = (\n",
    "                    style_score.item() - last_style_loss\n",
    "                    if last_style_loss is not None\n",
    "                    else 0\n",
    "                )\n",
    "                content_diff = (\n",
    "                    content_score.item() - last_content_loss\n",
    "                    if last_content_loss is not None\n",
    "                    else 0\n",
    "                )\n",
    "                last_style_loss = style_score.item()\n",
    "                last_content_loss = content_score.item()\n",
    "\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time = elapsed / run\n",
    "                eta = avg_time * (num_steps - run)\n",
    "\n",
    "                # Metrik-Anzeige\n",
    "                metrics_box.value = f\"\"\"\n",
    "                <table style='font-size:14px;'>\n",
    "                    <tr><td><b>üåÄ Iteration:</b></td><td>{run} / {num_steps}</td></tr>\n",
    "                    <tr><td><b>üé≠ Style Loss:</b></td><td>{style_score.item():.2e} ({style_diff:+.2e})</td></tr>\n",
    "                    <tr><td><b>üß† Content Loss:</b></td><td>{content_score.item():.4f} ({content_diff:+.4f})</td></tr>\n",
    "                    <tr><td><b>‚è±Ô∏è Verstrichen:</b></td><td>{time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))}</td></tr>\n",
    "                    <tr><td><b>üìÖ ETA:</b></td><td>{time.strftime(\"%H:%M:%S\", time.gmtime(eta))}</td></tr>\n",
    "                </table>\n",
    "                \"\"\"\n",
    "\n",
    "                info_box.value = f\"<b>üîÅ Optimierung l√§uft ‚Äì Iteration {run}</b>\"\n",
    "\n",
    "                # Log-Verlauf aktualisieren\n",
    "                with log_output:\n",
    "                    display(\n",
    "                        HTML(f\"\"\"\n",
    "                        <div style='font-family:monospace; border-bottom:1px solid #ddd; padding:4px 0;'>\n",
    "                            <b>Iteration {run:>4}:</b> \n",
    "                            Style Loss = {style_score.item():.2e} ({style_diff:+.2e}), \n",
    "                            Content Loss = {content_score.item():.4f} ({content_diff:+.4f}), \n",
    "                            ‚è±Ô∏è {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))}\n",
    "                        </div>\n",
    "                    \"\"\")\n",
    "                    )\n",
    "\n",
    "                print(\n",
    "                    f\"Iteration {run}: Style Loss: {style_score.item():.2e}, Content Loss: {content_score.item():.4f}\"\n",
    "                )\n",
    "\n",
    "                img_clone = input_img.clone()\n",
    "                imshow(img_clone, title=f\"Iteration {run}\")\n",
    "\n",
    "    progress_bar.bar_style = \"success\"\n",
    "    progress_bar.description = \"100%\"\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    info_box.value = \"<b>‚úÖ Style Transfer abgeschlossen!</b>\"\n",
    "    metrics_box.value += f\"<br><b>‚è≥ Gesamtdauer:</b> {time.strftime('%H:%M:%S', time.gmtime(total_time))}\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_img.clamp_(0, 1)\n",
    "\n",
    "    def last_part(path):\n",
    "        return os.path.splitext(os.path.basename(path))[0] if path else \"unknown\"\n",
    "\n",
    "    content_name = last_part(content_path)\n",
    "    style_name = last_part(style_path)\n",
    "    gif_path = f\"style_transfer_content-{content_name}_style-{style_name}.gif\"\n",
    "\n",
    "    if frames:\n",
    "        frames[0].save(\n",
    "            gif_path,\n",
    "            save_all=True,\n",
    "            append_images=frames[1:],\n",
    "            duration=200,\n",
    "            loop=0,\n",
    "        )\n",
    "        print(f\"GIF gespeichert unter {gif_path}\")\n",
    "\n",
    "    return input_img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausf√ºhrung\n",
    "\n",
    "In diesem letzten Schritt wird der gesamte Style-Transfer-Prozess gestartet und das Ergebnis visualisiert.\n",
    "\n",
    "\n",
    "### Erkl√§rung der Schritte:\n",
    "\n",
    "| Codezeile                                      | Bedeutung                                                                 |\n",
    "|------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| `input_img = content_img.clone()`              | Erzeugt eine Kopie des Content-Bildes als Startpunkt f√ºr die Optimierung. Das Eingabebild wird im Laufe des Style Transfers angepasst. |\n",
    "| `output = run_style_transfer(...)`             | Startet den Style Transfer mit allen zuvor definierten Parametern. Das Ergebnis ist das optimierte Bild, das Stil und Inhalt vereint. |\n",
    "| `plt.figure()`                                 | √ñffnet eine neue Matplotlib-Figur f√ºr die Bildanzeige.                   |\n",
    "| `imshow(output, title=\"Output Image\")`         | Zeigt das Ergebnisbild im Plot mit dem Titel ‚ÄûOutput Image‚Äú.             |\n",
    "| `plt.ioff()` / `plt.show()`                    | Deaktiviert interaktive Anzeige und zeigt das Bildfenster.               |\n",
    "\n",
    "\n",
    "### Zweck:\n",
    "Das Bild `output`, das durch den Style Transfer entstanden ist, wird abschlie√üend dargestellt. Es kombiniert den Inhalt des Content-Bildes mit dem Stil des Style-Bildes und stellt somit das Endergebnis des gesamten Prozesses dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Ausf√ºhrung\n",
    "# -----------------------------------------------\n",
    "plt.figure()\n",
    "imshow(content_img, title=\"Content-Bild (Inhalt)\")\n",
    "\n",
    "plt.figure()\n",
    "imshow(style_img, title=\"Style-Bild (Stil)\")\n",
    "input_img = content_img.clone()\n",
    "\n",
    "output = run_style_transfer(\n",
    "    cnn,\n",
    "    cnn_normalization_mean,\n",
    "    cnn_normalization_std,\n",
    "    content_img,\n",
    "    style_img,\n",
    "    input_img,\n",
    "    show_every=100,   # Bildanzeige alle 100 Iterationen\n",
    "    gif_every=20,     # GIF-Frame alle 20 Iterationen\n",
    "    content_path=selected_content_path,\n",
    "    style_path=selected_style_path,\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "imshow(output, title=\"Output Image\")\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die obige Abbildung zeigt das Ergebnis. Was dieses Verfahren leistet, ist im Grunde genommen lediglich eine Form der √úbertragung von Texturen eines Bilds auf ein anderes. \n",
    "\n",
    "- Am besten funktioniert es, wenn die Referenzstilbilder stark strukturiert und hochgradig selbst√§hnlich sind und die Zielinhaltbilder keine feinen Details enthalten m√ºssen, um noch erkennbar zu sein. \n",
    "- Ziemlich abstrakte Merkmale, wie etwa der Stil eines Portr√§ts, lassen sich f√ºr gew√∂hnlich nicht auf andere Bilder √ºbertragen. \n",
    "- Der **Algorithmus geh√∂rt eher zur klassischen Signalverarbeitung als zur KI**, Sie d√ºrfen hier also keine Wunder erwarten.\n",
    "- Dar√ºber hinaus arbeitet der Algorithmus zur Stil√ºbertragung langsam. Die hier ausgef√ºhrte Transformation ist aber so einfach, dass sie auch von einem kleinen, schnellen Feedforward-Netz erlernt werden kann ‚Äì sofern Ihnen geeignete Trainingsdaten zur Verf√ºgung stehen. Schnelle Stil√ºbertragungen k√∂nnen also erzielt werden, indem man zun√§chst einmal den Rechenaufwand investiert, um bei unver√§ndertem Referenzstilbild eine Reihe von Ein-/Ausgabe-Trainingsbeispielen zu erzeugen, mit denen das einfache CNN trainiert wird, damit es diese stilspezifische Transformation erlernt. Sobald das erledigt ist, erfolgt die Stil√ºbertragung auf ein Bild augenblicklich, denn es ist ja nur noch eine Zustandsweitergabe in\n",
    "dem kleinen CNN n√∂tig.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take aways\n",
    "\n",
    "Bei der Stil√ºbertragung wird ein neues Bild erzeugt, das den Inhalt eines Zielinhaltbilds\n",
    "erh√§lt und den Stil eines Referenzstilbilds √ºbernimmt.\n",
    "- Inhalte k√∂nnen von den h√∂her liegenden Aktivierungen eines CNNs erfasst werden.\n",
    "- Der Stil kann durch interne Korrelationen der Aktivierungen verschiedener Layer eines CNNs erfasst werden.\n",
    "- Deep Learning erm√∂glicht es, eine Stil√ºbertragung als Optimierungsaufgabe f√ºr ein vortrainiertes CNN mit Verlustfunktion zu formulieren.\n",
    "- Von diesem grundlegenden Konzept sind viele Varianten und Verfeinerungen m√∂glich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where's the intelligence?\n",
    "\n",
    "- Auch wenn Style-Transfer eher zur klassischen Signalverarbeitung geh√∂rt, gelicngt diese √úbertragung von Mustern auf ein anderes Bild erst durch Deep Learning. Welche Eigenschaften von CNNs sind hier gefragt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
