{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN13/13.1-neural_style_transfer_with_gif_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural style transfer\n",
    "\n",
    "\n",
    "Dieses Notizbuch enthält die Codebeispiele aus Kapitel 8, Abschnitt 3 von [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Neben DeepDream gibt es noch eine weitere bedeutende Entwicklung bei auf Deep Learning beruhenden Bildverarbeitungsverfahren: den von *Leon Gatys et al.* im Sommer 2015 vorgestellten **Neural-Style-Algorithmus** zur Stilübertragung [1]. Der Algorithmus wurde kontinuierlich verfeinert, und seit der Erstveröffentlichung wurden viele Varianten entwickelt. Zudem gibt es diverse Smartphone-Apps, die ihn verwenden. Der Einfachheit halber beschränkt sich dieser Abschnitt auf die in der ursprünglichen Arbeit vorgestellte Variante.\n",
    "\n",
    "Der Algorithmus überträgt den Stil eines Referenzbilds auf ein Zielbild und erhält dabei dessen Inhalt. \n",
    "\n",
    "[1] [Leon A. Gatys, Alexander S. Ecker und Matthias Bethge, A Neural Algorithm of Artistic Style,\n",
    "arXiv (2015)](https://arxiv.org/abs/1508.06576)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stil und Inhalt\n",
    "\n",
    "- Der **Stil** beschreibt in diesem Zusammenhang im Wesentlichen *Texturen, Farben und visuelle Muster auf verschiedenen räumlichen Skalen* eines Bilds. \n",
    "- Der **Inhalt** ezieht sich auf die allgemeinere Makrostruktur des Bilds. Inder obigen Abbildung gehören die blauen und gelben kreisförmigen Pinselstriche in Vincent van Goghs Gemälde Sternennacht zum Stil, die Gebäude in dem in Tübingen aufgenommenen Foto stellen hingegen den Inhalt dar.\n",
    "\n",
    "\n",
    "Die eng mit der Texterzeugung verwandte Idee der Stilübertragung hat in der Bildverarbeitung eine lange Vorgeschichte, die viel weiter zurückreicht als die des erst 2015 entwickelten Neural-Style-Algorithmus. Wie sich herausstellte, lassen sich\n",
    "mit auf Deep Learning beruhenden Implementierungen der Stilübertragung Ergebnisse erzielen, die alles, was mit klassischen Methoden des maschinellen Sehens erreicht werden kann, in den Schatten stellen. \n",
    "\n",
    "Und das löste eine erstaunliche Renaissance kreativer Anwendungen des maschinellen Sehens aus. Die entscheidende Idee bei der Implementierung der Stilübertragung liegt allen Deep-Learning-Algorithmen zugrunde: Man definiert eine Verlustfunktion, die festlegt, was erreicht werden soll, und minimiert sie. Im vorliegenden Fall ist klar, was erreicht werden soll: Der Inhalt des ursprünglichen Bilds soll erhalten werden, während gleichzeitig der Stil eines Referenzbilds übernommen wird. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Der Prozess des Style Transfers ist in Abbildung 7 dargestellt. Um den Style Transfer durchzuführen,\n",
    "werden drei Bilder benötigt. Das erste Bild enthält den gewünschten **Stil** $S$ *(Style)*,\n",
    "das zweite enthält den **Inhalt** $C$ *(Content)* und bei dem dritten Bild handelt es sich um das\n",
    "generierte Bild $x$ aus dem Stil-Bild und Inhalt-Bild. \n",
    "\n",
    "Das VGG19 dient beim Style Transfer als Funktion $f_w$, um Feature Maps zu generieren, die entweder dem `style` oder dem `content` entsprechen. Da der **Inhalt** (`content`) des Bildes erst in den letzten Schichten des Netzes erkannt wird, muss für die Berechnung der Verlustfunktion für den `content` ein Convolutional Layer verwendet werden, das sich im letzten Layer-Block des VGG19 befindet. \n",
    "\n",
    "Bei der Extraktion des **Stils** für kleine Muster werden Feature Maps aus den vorderen Schichten verwendet und Feature Maps aus den hinteren Schichten für grosse Muster. Das Netz wird nicht trainiert, die Gewichte des vortrainierten Netzes werden anfangs eingelesen und anschliessend nicht mehr verändert. Somit sind die Gewichte $w$ statisch. Um das zu generierende Bild $x$ zu berechnen, muss dieses zunächst initialisiert werden. In der vorliegenden Implementierung\n",
    "wurde dies mit dem Inhalt-Bild $C$ gemacht, jedoch wäre auch weisses Rauschen möglich. Nachdem ein Bild mit VGG19 analysiert wurde, werden verschiedene Feature Maps von Convolutional Layern abgespeichert, die anschliessend für die Optimierung verwendet werden. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Zielfunktion $\\mathcal{L}=\\alpha\\cdot \\mathcal{L}_C+\\beta \\cdot \\mathcal{L}_S$\n",
    "\n",
    "Wenn wir Inhalt und Stil mathematisch definieren könnten, sähe eine passende zu minimierende\n",
    "Verlustfunktion folgendermaßen aus:\n",
    "\n",
    "```\n",
    "loss = distance(style(reference_image) - style(generated_image)) +\n",
    "       distance(content(original_image) - content(generated_image))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`distance` ist hier eine Normierungsfunktion wie die L2-Norm, `content` ist eine\n",
    "Funktion, die eine Repräsentation des Bildinhalts errechnet, und `style` ist eine\n",
    "Funktion, die eine Repräsentation des Stils liefert. \n",
    "\n",
    "- Die Minimierung dieser Verlustfunktion sorgt dafür, dass `style(generated_image)` und `style(reference_image)` bzw. `content(generated_image)` und `content(original_image)` von jeweils gleicher Grössenordnung sind, und erzielt so die definierte Stilübertragung.\n",
    "- Gatys et al. machten die entscheidende Beobachtung, dass **tiefe neuronale Netze eine Möglichkeit bieten, die Funktionen `style` und `content` mathematisch zu definieren**. \n",
    "\n",
    "Sehen wir uns also an, wie das funktioniert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The content loss $\\mathcal{L}_C$: Verlustfunktion für den Inhalt\n",
    "\n",
    "\n",
    "Wie Sie bereits wissen, enthalten die Aktivierungen der ersten Layer eines NNs lokale Informationen über das Bild, die höher gelegenen Layer dagegen zunehmend *globale* und *abstrakte* Informationen. \n",
    "\n",
    "Mit anderen Worten: **Die Aktivierungen der verschiedenen Layer eines CNNs stellen eine Zerlegung des Inhalts eines\n",
    "Bilds auf unterschiedlichen räumlichen Skalen dar.** Deshalb würde man erwarten, dass der eher globale und abstrakte Inhalt eines Bilds durch die Repräsentationen in den oberen Layern eines CNNs erfasst wird.\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aus diesem Grund ist die L2-Norm für den Abstand zwischen den Aktivierungen eines oberen Layers eines vortrainierten CNNs, berechnet für das Zielbild, und den Aktivierungen des gleichen Layers, berechnet für das erzeugte Bild, ein guter\n",
    "Kandidat für die Verlustfunktion. Auf diese Weise ist sichergestellt, dass das erzeugte Bild aus Perspektive des oberen Layers Ähnlichkeit mit dem ursprünglichen Zielinhaltbild besitzt. Wenn die Annahme stimmt, dass die oberen Layer\n",
    "eines CNNs tatsächlich den Inhalt ihrer Eingabebilder repräsentieren, sollte der Inhalt eines Bilds auf diese Weise erhalten bleiben.\n",
    "\n",
    "Um die Optimierung durchzuführen, wird der Fehler berechnet, welcher zwischen dem generierten Bild $x$ und den vorgegebenen Bilder $C$ und $S$ entsteht. Der **Content-Loss** $\\mathcal{L}_C$ wird *pixelweise* durch die Methode der kleinsten Quadrate nach folgender Formel berechnet.\n",
    "\n",
    "\n",
    "$$\\boxed{ \\mathcal{L}_C = \\sum_i^n \\left[ f_w(C)-f_w(x) \\right]^2} \\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The style loss $\\mathcal{L}_S$: Verlustfunktion für den Stil\n",
    "\n",
    "Der Stil ist durch lokale Strukturen definiert. Da diese auf dem ganzen Bild vorkommen können,\n",
    "muss der Stil *translationsinvariant* sein. Um dies zu erreichen, wird die **Gram-Matrix**\n",
    "$G$ für das generierte Bild $x$ und das Stil-Bild $S$ aus den extrahierten Schichten berechnet.\n",
    "\n",
    "Die Verlustfunktion für den Inhalt verwendet nur einen einzelnen oberen Layer,\n",
    "die von Gatys et al. definierte Verlustfunktion für den Stil $\\mathcal{L}_S$ verwendet jedoch mehrere Layer eines CNNs: Es wird versucht, das Aussehen des Referenzstilbilds nicht nur auf einer einzigen Skala, sondern auf allen vom CNN extrahierten räumlichen Skalen zu erfassen. Gatys et al. verwenden als Verlustfunktion die **Gramsche Matrix** $G$ der Aktivierungen eines Layers, also das Skalarprodukt der Feature-Maps eines Layers. Dieses Skalarprodukt repräsentiert gewissermaßen die Korrelationen zwischen den Merkmalen eines Layers. Diese Korrelationen erfassen eine Statistik der Muster auf einer bestimmten räumlichen Skala, die empirisch dem Erscheinungsbild der auf dieser Skala sichtbaren Texturen entsprechen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Um die **Gram-Matrix** zu berechnen, werden die Feature Maps in hochdimensionale Vektoren\n",
    "gewandelt, das heisst sämtliche Feature Maps einer Schicht werden zu einer Matrix zusammengefügt.\n",
    "Diese Matrizen werden verwendet, um den mittleren quadratischen Abstand für\n",
    "jede Schicht gemäss Formel (2) zu berechnen\n",
    "\n",
    "$$\\boxed{E_l=\\frac{1}{4N_l^2\\cdot M_l^2} \\sum_{i,j} \\left[ G(S)_{ij}-G(x)_{ij}\\right]^2} \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dabei ist $N_l$ die Anzahl der Feature Maps und $M_l$ die Grösse der jeweiligen Feature Map. Der\n",
    "Style-Loss $\\mathcal{L}_S$ kann anschliessend durch Aufsummieren von $E_l$ unter Berücksichtigung der Gewichte $w_l$ berechnet werden:\n",
    "\n",
    "$$\\mathcal{L}_S = \\sum_{l=0}^L w_l \\cdot E_l \\tag{3}$$\n",
    "\n",
    "Die Verlustfunktion für den Stil hat also zum Ziel, ähnliche interne Korrelationen zwischen den Aktivierungen verschiedener Layer im Referenzstilbild beim erzeugten Bild zu erhalten. Umgekehrt ist dadurch gewährleistet, dass die auf verschiedenen\n",
    "räumlichen Skalen erkennbaren Texturen des Referenzstilbilds im erzeugten Bild ähnlich aussehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Der Gesamtverlust $\\mathcal{L}$\n",
    "\n",
    "Aus den beiden Formeln $\\mathcal{L}_C$ (1) und $\\mathcal{L}_S$ (2) kann nun der totale Fehler  berechnet werden:\n",
    "\n",
    "## $$ \\boxed{\\mathcal{L} = \\alpha \\cdot \\mathcal{L}_C + \\beta \\cdot \\mathcal{L}_S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir können somit **ein vortrainiertes CNN verwenden, um eine Verlustfunktion zu definieren**, die Folgendes leistet:\n",
    "- Sie erhält den Inhalt, indem sie ähnliche Aktivierungen allgemeiner Layer des Zielinhaltbilds im erzeugten Bild übernimmt. Das CNN sollte feststellen, dass das Zielinhaltbild und das erzeugte Bild die gleichen Objekte enthalten.\n",
    "- Sie erhält den Stil, indem sie ähnliche *Korrelationen der Aktivierungen* sowohl in tiefer liegenden als auch in höher liegenden Layern übernimmt. Merkmalskorrelationen erfassen Texturen: Das erzeugte Bild und das Referenzstilbild sollten auf verschiedenen räumlichen Skalen die jeweils gleichen Texturen enthalten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"Bilder/FastStyleTransfer_Johnson.PNG\" width=\"840\" align=\"center\"/>\n",
    "\n",
    "[3] [Johnson, J., Alahi, A. und Li, F.-F., “Perceptual Losses for Real-Time Style Transfer\n",
    "and Super-Resolution”, CoRR, Jg. abs/1603.08155, 2016.](http://web.eecs.umich.edu/~justincj/papers/eccv16/JohnsonECCV16.pdf)\n",
    "\n",
    "[4] [Johnson, J., Alahi, A. und Li, F.-F., “Perceptual Losses for Real-Time Style Transfer\n",
    "and Super-Resolution: Supplementary Material”, CoRR, 2016.](http://web.eecs.umich.edu/~justincj/papers/eccv16/JohnsonECCV16Supplementary.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Style Transfer mit PyTorch\n",
    "\n",
    "Beim Neural Style Transfer (NST) wird der **Inhalt eines Bildes** mit dem **künstlerischen Stil eines anderen Bildes** kombiniert. Die zugrunde liegende Idee stammt aus der Arbeit von *Gatys et al.* und nutzt ein vortrainiertes Convolutional Neural Network (CNN), in der Regel **VGG19**, um Stil- und Inhaltsinformationen aus Bildern zu extrahieren.\n",
    "\n",
    "Das Verfahren nutzt nicht die Klassifikationsfähigkeit des Netzwerks, sondern die **Aktivierungen seiner Zwischenlayer**, um sowohl die **semantischen Inhalte** als auch die **stilistischen Eigenschaften** eines Bildes zu erfassen.\n",
    "\n",
    "### Vorgehensweise in drei Schritten:\n",
    "\n",
    "1. **Feature-Extraktion mit VGG19**  \n",
    "   Ein tiefes CNN (hier: VGG19) wird verwendet, um die Aktivierungen der Zwischenlayer für drei Bilder zu berechnen:\n",
    "   - das Content-Bild (Zielinhalt)\n",
    "   - das Style-Bild (Stilvorlage)\n",
    "   - das generierte Bild (wird trainiert)\n",
    "\n",
    "2. **Definition der Verlustfunktion**  \n",
    "   Die sogenannte Perceptual Loss setzt sich zusammen aus:\n",
    "   - **Content Loss**: Vergleich der Feature-Maps zwischen generiertem und Content-Bild\n",
    "   - **Style Loss**: Vergleich der Gram-Matrizen zwischen generiertem und Style-Bild\n",
    "   - **TV Loss (optional)**: Glättung des Bildes durch Minimierung lokaler Unterschiede\n",
    "\n",
    "3. **Optimierung des Bildes**  \n",
    "   Das Zielbild wird pixelweise durch **Gradientenabstieg** so verändert, dass es gleichzeitig dem Inhalt des Content-Bildes und dem Stil des Style-Bildes entspricht.\n",
    "\n",
    "\n",
    "> Damit das Verfahren gut funktioniert, sollten Content- und Style-Bild in ähnlicher Größe vorliegen. In der Regel wird die Höhe der Bilder auf z. B. **400 Pixel** skaliert, um Rechenzeit zu sparen und bessere Ergebnisse zu erzielen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Neural Style Transfer mit VGG19 (PyTorch)\n",
    "# Basierend auf der offiziellen PyTorch-Tutorial-Implementierung\n",
    "# -----------------------------------------------\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torchvision.transforms.functional import InterpolationMode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuerst wird geprüft, ob eine CUDA-fähige GPU verfügbar ist. Wenn ja, wird \"cuda\" als Gerät gewählt, sonst \"cpu\". Dieses Gerät wird dann als Standardgerät für alle folgenden Tensoroperationen gesetzt, sodass man nicht ständig .to(device) angeben muss. Zum Schluss wird das gewählte Gerät zur Kontrolle ausgegeben. Das sorgt für flexiblen, geräteunabhängigen Code, der sowohl auf GPU als auch CPU läuft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Geräteeinstellung & Default\n",
    "# -----------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "print(\"Verwendetes Gerät:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bildpfade & Download\n",
    "\n",
    "### Was passiert hier?\n",
    "\n",
    "1. **Ordner anlegen**  \n",
    "   ```python\n",
    "   os.makedirs(\"data/Bilder\", exist_ok=True)\n",
    "   ```\n",
    "   Es wird ein Ordner `data/Bilder` erstellt, falls er noch nicht existiert. So wird sichergestellt, dass der Speicherort für die Bilder verfügbar ist.\n",
    "\n",
    "2. **URLs und Pfade definieren**  \n",
    "   Es werden zwei Bild-URLs festgelegt:\n",
    "   - Ein **Content-Bild** (Porträtfoto einer Frau)\n",
    "   - Ein **Style-Bild** (Gemälde von Ljubow Popowa)\n",
    "\n",
    "   Zusätzlich werden die lokalen Speicherpfade definiert, unter denen diese Bilder abgelegt werden sollen:\n",
    "   - `data/Bilder/portrait.jpg`\n",
    "   - `data/Bilder/popova_style.jpg`\n",
    "\n",
    "3. **User-Agent setzen**  \n",
    "   Einige Server blockieren Anfragen ohne User-Agent. Daher wird ein Header gesetzt, um sich als normaler Browser auszugeben:\n",
    "   ```python\n",
    "   headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "   ```\n",
    "\n",
    "4. **Download-Funktion definieren**  \n",
    "   Die Funktion `download_image(url, path)` lädt ein Bild von einer URL herunter und speichert es unter dem angegebenen Pfad.\n",
    "\n",
    "5. **Bilder nur bei Bedarf herunterladen**  \n",
    "   Vor dem Herunterladen wird überprüft, ob die Dateien bereits vorhanden sind. Falls nicht, werden sie heruntergeladen:\n",
    "   ```python\n",
    "   if not os.path.exists(content_path):\n",
    "       download_image(content_url, content_path)\n",
    "   ```\n",
    "\n",
    "6. **Statusmeldung**  \n",
    "   Am Ende wird eine Bestätigung ausgegeben:\n",
    "   ```\n",
    "   ✅ Bilder erfolgreich heruntergeladen und gespeichert.\n",
    "   ```\n",
    "\n",
    "### Zweck:\n",
    "Dieser Code stellt sicher, dass die benötigten Bilder (Content & Style) lokal verfügbar sind, ohne sie jedes Mal neu herunterladen zu müssen. Das ist besonders hilfreich für Anwendungen wie **Neural Style Transfer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Bild-Download\n",
    "# -----------------------------------------------\n",
    "os.makedirs(\"data/Bilder\", exist_ok=True)\n",
    "\n",
    "content_url = \"https://www.myposter.de/magazin/wp-content/uploads/2016/06/Portrait-Frau-lachend-shutterstock_381113020_kl.jpg\"\n",
    "style_url = (\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/7/78/Popova_Air_Man_Space.jpg\"\n",
    ")\n",
    "content_path_default = \"data/Bilder/portrait.jpg\"\n",
    "style_path_default = \"data/Bilder/popova_style.jpg\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "\n",
    "def download_image(url, path):\n",
    "    req = urllib.request.Request(url, headers=headers)\n",
    "    with urllib.request.urlopen(req) as response, open(path, \"wb\") as out_file:\n",
    "        out_file.write(response.read())\n",
    "\n",
    "\n",
    "if not os.path.exists(content_path_default):\n",
    "    download_image(content_url, content_path_default)\n",
    "if not os.path.exists(style_path_default):\n",
    "    download_image(style_url, style_path_default)\n",
    "\n",
    "print(\"✅ Portrait- und Popova-Style-Bild erfolgreich geladen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# StyleTransfer-Bilder herunterladen\n",
    "# -----------------------------------------------\n",
    "target_dir = \"data/Bilder/StyleTransfer\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "filenames = [\n",
    "    \"Klimt2.jpg\",\n",
    "    \"Kopenhagen.jpg\",\n",
    "    \"Mosaic2.jpg\",\n",
    "    \"Sonnenblumen.jpg\",\n",
    "    \"Van_Gogh.jpg\",\n",
    "    \"Vassily_Kandinsky7.jpg\",\n",
    "    \"candy.jpg\",\n",
    "    \"composition-with-figures_popova.jpg\",\n",
    "    \"composition.jpg\",\n",
    "    \"graffiti.jpg\",\n",
    "    \"popova.png\",\n",
    "    \"portrait.jpg\",\n",
    "    \"portrait_mann.jpg\",\n",
    "    \"portrait_women.jpg\",\n",
    "    \"simpsons.jpg\",\n",
    "    \"udnie.jpg\",\n",
    "]\n",
    "\n",
    "base_url = \"https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/data/StyleTransfer/\"\n",
    "\n",
    "for name in filenames:\n",
    "    save_path = os.path.join(target_dir, name)\n",
    "    if os.path.exists(save_path):\n",
    "        continue\n",
    "    try:\n",
    "        urllib.request.urlretrieve(base_url + name, save_path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fehler bei {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Bildpfade\n",
    "# -----------------------------------------------\n",
    "content_images = {\n",
    "    \"portrait\": \"data/Bilder/StyleTransfer/portrait.jpg\",\n",
    "    \"Kopenhagen\": \"data/Bilder/StyleTransfer/Kopenhagen.jpg\",\n",
    "    \"portrait_mann\": \"data/Bilder/StyleTransfer/portrait_mann.jpg\",\n",
    "    \"portrait_women\": \"data/Bilder/StyleTransfer/portrait_women.jpg\",\n",
    "    \"simpsons\": \"data/Bilder/StyleTransfer/simpsons.jpg\",\n",
    "}\n",
    "\n",
    "style_images = {\n",
    "    \"popova\": \"data/Bilder/StyleTransfer/popova.png\",\n",
    "    \"candy\": \"data/Bilder/StyleTransfer/candy.jpg\",\n",
    "    \"Van_Gogh\": \"data/Bilder/StyleTransfer/Van_Gogh.jpg\",\n",
    "    \"udnie\": \"data/Bilder/StyleTransfer/udnie.jpg\",\n",
    "    \"Vassily_Kandinsky7\": \"data/Bilder/StyleTransfer/Vassily_Kandinsky7.jpg\",\n",
    "    \"composition\": \"data/Bilder/StyleTransfer/composition.jpg\",\n",
    "    \"composition_popova\": \"data/Bilder/StyleTransfer/composition-with-figures_popova.jpg\",\n",
    "    \"graffiti\": \"data/Bilder/StyleTransfer/graffiti.jpg\",\n",
    "    \"Mosaic2\": \"data/Bilder/StyleTransfer/Mosaic2.jpg\",\n",
    "    \"Sonnenblumen\": \"data/Bilder/StyleTransfer/Sonnenblumen.jpg\",\n",
    "    \"Klimt2\": \"data/Bilder/StyleTransfer/Klimt2.jpg\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Dropdown-Menüs mit ipywidgets\n",
    "# -----------------------------------------------\n",
    "content_dropdown = widgets.Dropdown(\n",
    "    options=content_images,\n",
    "    description=\"Content:\",\n",
    "    layout=widgets.Layout(width=\"60%\"),\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "style_dropdown = widgets.Dropdown(\n",
    "    options=style_images,\n",
    "    description=\"Style:\",\n",
    "    layout=widgets.Layout(width=\"60%\"),\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "# Ausgabe-Label\n",
    "output_widget = widgets.Output()  # Umbenennung, damit kein Konflikt entsteht\n",
    "\n",
    "# Globale Variablen für die aktuellen Pfade\n",
    "selected_content_path = content_dropdown.value\n",
    "selected_style_path = style_dropdown.value\n",
    "\n",
    "\n",
    "def update_paths(change=None):\n",
    "    global selected_content_path, selected_style_path\n",
    "    output_widget.clear_output()\n",
    "    with output_widget:\n",
    "        selected_content_path = content_dropdown.value\n",
    "        selected_style_path = style_dropdown.value\n",
    "        print(\"✅ Ausgewählte Pfade:\")\n",
    "        print(\"Content:\", selected_content_path)\n",
    "        print(\"Style:\", selected_style_path)\n",
    "\n",
    "\n",
    "# Verbindung der Dropdowns mit dem Callback\n",
    "content_dropdown.observe(update_paths, names=\"value\")\n",
    "style_dropdown.observe(update_paths, names=\"value\")\n",
    "\n",
    "# Anzeige\n",
    "display(content_dropdown, style_dropdown, output_widget)\n",
    "\n",
    "# Initiales Anzeigen der Auswahl\n",
    "update_paths()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bildverarbeitung\n",
    "\n",
    "1. **Bildgröße festlegen**  \n",
    "   Die Zielgröße für die Bilder wird abhängig vom Gerät gewählt:\n",
    "   - **512 Pixel**, wenn eine GPU verfügbar ist (schnelleres Rechnen)\n",
    "   - **128 Pixel**, wenn nur eine CPU verfügbar ist (ressourcenschonender)\n",
    "\n",
    "2. **Transformationen definieren**  \n",
    "   `loader` ist eine Pipeline zur Bildvorverarbeitung:\n",
    "   - **Resize** auf die Zielgröße mit BICUBIC-Interpolation\n",
    "   - **CenterCrop**, um das Bild quadratisch zuzuschneiden\n",
    "   - **ToTensor**, um das PIL-Bild in ein PyTorch-Tensor (mit Werten in [0, 1]) umzuwandeln\n",
    "\n",
    "3. **`unloader`**  \n",
    "   Eine Umkehrfunktion: Wandelt ein Tensor-Bild wieder zurück in ein PIL-Bild zur Visualisierung.\n",
    "\n",
    "4. **Funktion: `image_loader(image_path)`**  \n",
    "   - Öffnet ein Bild von Pfad `image_path`\n",
    "   - Konvertiert es zu RGB\n",
    "   - Wendet die Transformationspipeline `loader` an\n",
    "   - Fügt eine Batch-Dimension hinzu (`unsqueeze(0)`)\n",
    "   - Gibt das Bild als Float-Tensor auf dem richtigen Gerät (`cpu` oder `cuda`) zurück\n",
    "\n",
    "5. **Funktion: `imshow(tensor, title=None)`**  \n",
    "   - Kopiert das Tensorbild zurück auf die CPU\n",
    "   - Entfernt die Batch-Dimension\n",
    "   - Wandelt es mit `unloader` in ein PIL-Bild um\n",
    "   - Zeigt es mit `matplotlib` an\n",
    "   - Optional kann ein Titel angegeben werden\n",
    "   - Achsen werden ausgeblendet, und ein kurzes `pause()` sorgt dafür, dass das Bild korrekt dargestellt wird\n",
    "\n",
    "### Zweck:\n",
    "Dieser Block stellt sicher, dass Bilder korrekt geladen, skaliert und für die Verarbeitung (z. B. im Style Transfer) als Tensor vorbereitet werden – und später wieder anschaulich dargestellt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Bildverarbeitung\n",
    "# -----------------------------------------------\n",
    "imsize = 512 if torch.cuda.is_available() else 128\n",
    "loader = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(imsize, interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(imsize),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "\n",
    "def image_loader(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone().squeeze(0)\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.pause(0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilder laden\n",
    "\n",
    "1. **Content- und Style-Bilder laden**  \n",
    "   Mit der Funktion `image_loader(...)` werden die beiden vorbereiteten Bilder (Content und Style) eingelesen und in passende PyTorch-Tensoren umgewandelt:\n",
    "   ```python\n",
    "   content_img = image_loader(content_path)\n",
    "   style_img = image_loader(style_path)\n",
    "   ```\n",
    "   Diese Bilder enthalten nun eine zusätzliche Batch-Dimension und liegen als 4D-Tensoren vor: `(1, 3, H, W)`.\n",
    "\n",
    "2. **Größenvergleich**  \n",
    "   Es wird geprüft, ob beide Bilder die gleiche Größe haben:\n",
    "   ```python\n",
    "   if content_img.size() != style_img.size():\n",
    "       raise ValueError(\"Style und Content müssen gleiche Grösse haben!\")\n",
    "   ```\n",
    "   Das ist notwendig, da der Style-Transfer-Prozess voraussetzt, dass Content- und Style-Bild dieselben Dimensionen besitzen – sonst funktionieren die Operationen (z. B. Feature-Extraktion oder Loss-Berechnung) nicht korrekt.\n",
    "\n",
    "### Zweck:\n",
    "Dieser Block lädt die beiden Hauptbilder für den Style Transfer und stellt sicher, dass sie korrekt und gleich groß sind – eine wichtige Voraussetzung für die Weiterverarbeitung im Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Bilder laden\n",
    "# -----------------------------------------------\n",
    "content_img = image_loader(selected_content_path)\n",
    "style_img = image_loader(selected_style_path)\n",
    "\n",
    "if content_img.size() != style_img.size():\n",
    "    raise ValueError(\"Style und Content müssen gleiche Grösse haben!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verlustfunktionen für Neural Style Transfer\n",
    "\n",
    "In diesem Abschnitt werden zwei zentrale Verlustfunktionen definiert: eine für den **Content (Inhalt)** und eine für den **Style (Stil)**. Diese messen, wie gut das optimierte Bild dem Content- bzw. Style-Ziel entspricht.\n",
    "\n",
    "### Übersicht der Funktionen und Klassen\n",
    "\n",
    "| Name              | Typ         | Zweck                                                                                     |\n",
    "|-------------------|--------------|--------------------------------------------------------------------------------------------|\n",
    "| `ContentLoss`     | Klasse       | Misst den Unterschied zwischen den **Feature-Maps** des Content-Bildes und des generierten Bildes mithilfe von **MSE-Loss**. Nur der Inhalt wird berücksichtigt. |\n",
    "| `gram_matrix`     | Funktion     | Berechnet die **Gram-Matrix**, welche die Korrelationen zwischen den Kanälen eines Features beschreibt. Sie ist die Grundlage zur Berechnung des Style-Loss. |\n",
    "| `StyleLoss`       | Klasse       | Vergleicht die **Gram-Matrix** des Style-Bildes mit der des generierten Bildes mittels **MSE-Loss**. Dadurch wird der visuelle Stil übertragen. |\n",
    "\n",
    "\n",
    "### Details zu den Komponenten\n",
    "\n",
    "#### `ContentLoss`\n",
    "- Speichert ein \"Ziel\"-Feature (`target`), das vom Content-Bild stammt.\n",
    "- Im Forward-Pass wird der MSE-Loss zwischen dem aktuellen Input (aus dem generierten Bild) und dem Ziel berechnet.\n",
    "- Gibt den Input unverändert zurück, damit das Bild weiter durch das Netz fließen kann.\n",
    "\n",
    "#### `gram_matrix(input)`\n",
    "- Formt das Input-Feature (Tensor der Form `[batch, channels, height, width]`) um in ein 2D-Feature.\n",
    "- Multipliziert das Feature mit seiner Transponierten, um die Korrelationen zu berechnen.\n",
    "- Normalisiert die Gram-Matrix, damit die Werte unabhängig von der Bildgröße sind.\n",
    "\n",
    "#### `StyleLoss`\n",
    "- Wandelt das Ziel-Feature des Style-Bildes in eine Gram-Matrix um.\n",
    "- Im Forward-Pass wird die Gram-Matrix des Inputs berechnet und mit dem Ziel verglichen (MSE-Loss).\n",
    "- Gibt ebenfalls den Input unverändert zurück.\n",
    "\n",
    "\n",
    "### Zweck:\n",
    "Diese Verlustfunktionen sind essenziell für den Style Transfer:\n",
    "- Der **Content-Loss** bewahrt die Struktur und Formen des Ausgangsbildes.\n",
    "- Der **Style-Loss** überträgt die Textur, Farbverteilung und Muster des Stilbildes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Verlustfunktionen\n",
    "# -----------------------------------------------\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super().__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "\n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()\n",
    "    features = input.view(a * b, c * d)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super().__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Normalisierung (für VGG)\n",
    "# -----------------------------------------------\n",
    "cnn = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = mean.view(-1, 1, 1)\n",
    "        self.std = std.view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisierung (für VGG)\n",
    "\n",
    "Beim Style Transfer wird ein vortrainiertes VGG19-Netzwerk verwendet. Dieses erwartet Eingabebilder, die mit bestimmten Mittelwerten und Standardabweichungen normalisiert wurden.\n",
    "\n",
    "\n",
    "### Erklärung der Bestandteile\n",
    "\n",
    "| Name                         | Typ       | Zweck                                                                                   |\n",
    "|------------------------------|------------|------------------------------------------------------------------------------------------|\n",
    "| `cnn`                        | Modell     | Enthält die **Feature-Extraktionsschichten** des vortrainierten VGG19-Modells (ohne Klassifikationskopf). |\n",
    "| `cnn_normalization_mean`     | Tensor     | Mittelwerte der Farbkanäle (RGB), mit denen die Bilder für VGG normalisiert werden.     |\n",
    "| `cnn_normalization_std`      | Tensor     | Standardabweichungen der Farbkanäle (RGB), für die Normalisierung der Eingabebilder.    |\n",
    "| `Normalization`              | Klasse     | Modul zur Durchführung der Normalisierung im Forward-Pass des Netzwerks.                |\n",
    "\n",
    "\n",
    "### Details\n",
    "\n",
    "#### VGG-Modell\n",
    "```python\n",
    "cnn = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n",
    "```\n",
    "- Lädt das **vortrainierte VGG19-Netzwerk** (ImageNet-Gewichte).\n",
    "- `.features` extrahiert nur den **Feature-Teil** des Netzwerks (ohne die Klassifizierungsschichten).\n",
    "- `.eval()` versetzt das Modell in den **Inferenzmodus**, d.h. ohne Dropout oder BatchNorm-Updates.\n",
    "\n",
    "#### Mittelwert & Standardabweichung\n",
    "```python\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "```\n",
    "- Diese Werte stammen aus der **ImageNet-Datenverteilung** (Basis für das Training von VGG19).\n",
    "- Sie werden benötigt, damit die Eingabebilder in der gleichen Weise normalisiert werden wie beim Training.\n",
    "\n",
    "#### `Normalization`-Klasse\n",
    "- Erbt von `nn.Module`.\n",
    "- Die RGB-Kanal-Mittelwerte und Standardabweichungen werden so reshaped, dass sie auf Tensoren im Format `[B, C, H, W]` anwendbar sind.\n",
    "- Im `forward(...)` wird das Bild kanalweise normalisiert\n",
    "\n",
    "\n",
    "### Zweck:\n",
    "Diese Normalisierung ist **essentiell**, damit die Bilder mit dem VGG-Netz korrekt verarbeitet werden. Ohne diese Anpassung würden die Features falsch interpretiert, was zu schlechten Ergebnissen beim Style Transfer führen würde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Modellaufbau mit Verlustschichten\n",
    "# -----------------------------------------------\n",
    "content_layers_default = [\"conv_5\", \"conv_6\"]\n",
    "style_layers_default = [\"conv_1\", \"conv_2\", \"conv_3\", \"conv_4\"]\n",
    "\n",
    "\n",
    "def get_style_model_and_losses(\n",
    "    cnn,\n",
    "    norm_mean,\n",
    "    norm_std,\n",
    "    style_img,\n",
    "    content_img,\n",
    "    content_layers=content_layers_default,\n",
    "    style_layers=style_layers_default,\n",
    "):\n",
    "    normalization = Normalization(norm_mean, norm_std).to(device)\n",
    "    content_losses, style_losses = [], []\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = f\"conv_{i}\"\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = f\"relu_{i}\"\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = f\"pool_{i}\"\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = f\"bn_{i}\"\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unrecognized layer: {layer.__class__.__name__}\")\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # Modell kürzen\n",
    "    for j in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[j], ContentLoss) or isinstance(model[j], StyleLoss):\n",
    "            break\n",
    "    model = model[: (j + 1)]\n",
    "    return model, style_losses, content_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimierung vorbereiten\n",
    "\n",
    "Dieser Abschnitt definiert die Optimierungsmethode, mit der das Bild für den Style Transfer angepasst wird.\n",
    "\n",
    "\n",
    "### Funktion: `get_input_optimizer(input_img)`\n",
    "\n",
    "| Bestandteil                 | Bedeutung                                                                 |\n",
    "|----------------------------|---------------------------------------------------------------------------|\n",
    "| `input_img.requires_grad_()` | Aktiviert das **Gradient Tracking** für das Bild, da es optimiert werden soll. |\n",
    "| `optim.LBFGS([...])`         | Verwendet den **L-BFGS-Optimierer** aus PyTorch, eine effektive Methode für kleine Probleme mit wenigen Parametern. |\n",
    "\n",
    "\n",
    "### Zweck:\n",
    "Beim Neural Style Transfer wird **nicht** das Netzwerk trainiert, sondern das **Eingabebild** (das zu stylende Bild) direkt optimiert.\n",
    "\n",
    "Die Funktion gibt einen Optimierer zurück, der das `input_img` schrittweise verändert, sodass der **Style-Loss** und der **Content-Loss** minimiert werden.\n",
    "\n",
    "Der **L-BFGS-Optimierer** eignet sich besonders gut für Style Transfer, da er konvergiert, ohne dass eine hohe Anzahl an Iterationen nötig ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Optimierung vorbereiten\n",
    "# -----------------------------------------------\n",
    "def get_input_optimizer(input_img):\n",
    "    return optim.LBFGS([input_img.requires_grad_()], lr=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Transfer ausführen\n",
    "\n",
    "In dieser Funktion wird der eigentliche **Style Transfer** durchgeführt: Das Eingabebild wird so optimiert, dass es den Inhalt des Content-Bildes und den Stil des Style-Bildes kombiniert.\n",
    "\n",
    "\n",
    "### Funktion: `run_style_transfer(...)`\n",
    "\n",
    "| Parameter         | Bedeutung                                                                 |\n",
    "|-------------------|---------------------------------------------------------------------------|\n",
    "| `cnn`             | Vortrainiertes VGG19-Netz (nur Feature-Teil)                             |\n",
    "| `norm_mean/std`   | Mittelwert & Standardabweichung für VGG-Normalisierung                   |\n",
    "| `content_img`     | Bild, dessen Inhalt beibehalten werden soll                              |\n",
    "| `style_img`       | Bild, dessen Stil übertragen werden soll                                 |\n",
    "| `input_img`       | Das zu optimierende Bild (initial meist eine Kopie von `content_img`)    |\n",
    "| `num_steps`       | Anzahl der Optimierungsschritte (Standard: 500)                          |\n",
    "| `style_weight`    | Gewichtung des Style-Loss (höher → mehr Stilübernahme)                   |\n",
    "| `content_weight`  | Gewichtung des Content-Loss (höher → mehr Inhaltstreue)                  |\n",
    "\n",
    "\n",
    "### Ablauf:\n",
    "\n",
    "1. **Modell vorbereiten**  \n",
    "   Die Hilfsfunktion `get_style_model_and_losses(...)` erstellt ein zusammengesetztes Modell mit eingefügten Style- und Content-Loss-Modulen.\n",
    "\n",
    "2. **Optimierer initialisieren**  \n",
    "   Das Eingabebild wird mit `get_input_optimizer(...)` als optimierbares Objekt vorbereitet (L-BFGS-Optimierer).\n",
    "\n",
    "3. **Optimierungsschleife starten**  \n",
    "   Für die festgelegte Anzahl an Schritten (`num_steps`) wird die folgende Funktion wiederholt:\n",
    "\n",
    "   #### `closure()`:\n",
    "   - Clamped (`input_img.clamp_(0, 1)`): Bild wird auf gültige Pixelwerte (0–1) begrenzt.\n",
    "   - Optimierer-Zustand wird zurückgesetzt.\n",
    "   - Forward-Pass durch das Modell → Verluste werden automatisch berechnet.\n",
    "   - Style- und Content-Loss werden aufsummiert und gewichtet.\n",
    "   - Backward-Pass: Gradienten werden berechnet.\n",
    "   - Alle 50 Schritte wird der aktuelle Loss ausgegeben.\n",
    "\n",
    "4. **Letztes Clamping**  \n",
    "   Nach der Optimierung wird das finale Bild nochmals auf gültige Werte beschränkt.\n",
    "\n",
    "5. **Ergebnis**  \n",
    "   Die Funktion gibt das transformierte Bild zurück – eine Kombination aus Stil und Inhalt.\n",
    "\n",
    "\n",
    "### Zweck:\n",
    "Dies ist die zentrale Schleife, die den **Neural Style Transfer** durchführt. Sie verändert schrittweise das Eingabebild, bis es sowohl den gewünschten Stil als auch den gewünschten Inhalt bestmöglich wiedergibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def run_style_transfer(\n",
    "    cnn,\n",
    "    norm_mean,\n",
    "    norm_std,\n",
    "    content_img,\n",
    "    style_img,\n",
    "    input_img,\n",
    "    num_steps=1000,\n",
    "    style_weight=1e6,\n",
    "    content_weight=1,\n",
    "    show_every=100,  # Anzeige-Intervall (Bildanzeige)\n",
    "    gif_every=20,  # GIF-Frame-Intervall (GIF speichern)\n",
    "    content_path=None,\n",
    "    style_path=None,\n",
    "):\n",
    "    print(\"Modell wird erstellt...\")\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(\n",
    "        cnn, norm_mean, norm_std, style_img, content_img\n",
    "    )\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Optimierung läuft...\")\n",
    "\n",
    "    # Fortschrittsanzeige vorbereiten\n",
    "    progress_bar = widgets.IntProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=num_steps,\n",
    "        description=\"0%\",\n",
    "        bar_style=\"info\",\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    info_box = widgets.HTML(value=\"<b>Initialisierung läuft...</b>\")\n",
    "    metrics_box = widgets.HTML()\n",
    "\n",
    "    display(\n",
    "        widgets.VBox(\n",
    "            [\n",
    "                widgets.HTML(\n",
    "                    \"<h4 style='margin:0;'>🎨 <b>Style Transfer Fortschritt</b></h4>\"\n",
    "                ),\n",
    "                progress_bar,\n",
    "                info_box,\n",
    "                metrics_box,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Log-Ausgabe vorbereiten\n",
    "    log_output = widgets.Output(\n",
    "        layout={\"border\": \"1px solid #ccc\", \"height\": \"250px\", \"overflow_y\": \"auto\"}\n",
    "    )\n",
    "    log_output.clear_output()\n",
    "    display(\n",
    "        widgets.VBox(\n",
    "            [\n",
    "                widgets.HTML(\n",
    "                    \"<h4 style='margin-top:30px;'>📜 <b>Iteration-Verlauf</b></h4>\"\n",
    "                ),\n",
    "                log_output,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    run = 0\n",
    "    frames = []\n",
    "    last_style_loss = None\n",
    "    last_content_loss = None\n",
    "\n",
    "    while run <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            with torch.no_grad():\n",
    "                input_img.clamp_(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = sum(sl.loss for sl in style_losses)\n",
    "            content_score = sum(cl.loss for cl in content_losses)\n",
    "            loss = style_score * style_weight + content_score * content_weight\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        run += 1\n",
    "        progress_bar.value = run\n",
    "        progress_bar.description = f\"{int((run / num_steps) * 100)}%\"\n",
    "\n",
    "        # GIF-Frame speichern alle gif_every Iterationen\n",
    "        if run % gif_every == 0 or run == num_steps:\n",
    "            with torch.no_grad():\n",
    "                input_img.clamp_(0, 1)\n",
    "                img_clone = input_img.clone()\n",
    "                img_np = img_clone.squeeze().cpu().numpy()\n",
    "                img_np = img_np.transpose(1, 2, 0)\n",
    "                img_uint8 = (img_np * 255).astype(\"uint8\")\n",
    "                frame = Image.fromarray(img_uint8)\n",
    "                frames.append(frame)\n",
    "\n",
    "        # Bildanzeige nur alle show_every Iterationen\n",
    "        if run % show_every == 0 or run == num_steps:\n",
    "            with torch.no_grad():\n",
    "                input_img.clamp_(0, 1)\n",
    "                style_score = sum(sl.loss for sl in style_losses)\n",
    "                content_score = sum(cl.loss for cl in content_losses)\n",
    "\n",
    "                style_diff = (\n",
    "                    style_score.item() - last_style_loss\n",
    "                    if last_style_loss is not None\n",
    "                    else 0\n",
    "                )\n",
    "                content_diff = (\n",
    "                    content_score.item() - last_content_loss\n",
    "                    if last_content_loss is not None\n",
    "                    else 0\n",
    "                )\n",
    "                last_style_loss = style_score.item()\n",
    "                last_content_loss = content_score.item()\n",
    "\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time = elapsed / run\n",
    "                eta = avg_time * (num_steps - run)\n",
    "\n",
    "                # Metrik-Anzeige\n",
    "                metrics_box.value = f\"\"\"\n",
    "                <table style='font-size:14px;'>\n",
    "                    <tr><td><b>🌀 Iteration:</b></td><td>{run} / {num_steps}</td></tr>\n",
    "                    <tr><td><b>🎭 Style Loss:</b></td><td>{style_score.item():.2e} ({style_diff:+.2e})</td></tr>\n",
    "                    <tr><td><b>🧠 Content Loss:</b></td><td>{content_score.item():.4f} ({content_diff:+.4f})</td></tr>\n",
    "                    <tr><td><b>⏱️ Verstrichen:</b></td><td>{time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))}</td></tr>\n",
    "                    <tr><td><b>📅 ETA:</b></td><td>{time.strftime(\"%H:%M:%S\", time.gmtime(eta))}</td></tr>\n",
    "                </table>\n",
    "                \"\"\"\n",
    "\n",
    "                info_box.value = f\"<b>🔁 Optimierung läuft – Iteration {run}</b>\"\n",
    "\n",
    "                # Log-Verlauf aktualisieren\n",
    "                with log_output:\n",
    "                    display(\n",
    "                        HTML(f\"\"\"\n",
    "                        <div style='font-family:monospace; border-bottom:1px solid #ddd; padding:4px 0;'>\n",
    "                            <b>Iteration {run:>4}:</b> \n",
    "                            Style Loss = {style_score.item():.2e} ({style_diff:+.2e}), \n",
    "                            Content Loss = {content_score.item():.4f} ({content_diff:+.4f}), \n",
    "                            ⏱️ {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))}\n",
    "                        </div>\n",
    "                    \"\"\")\n",
    "                    )\n",
    "\n",
    "                print(\n",
    "                    f\"Iteration {run}: Style Loss: {style_score.item():.2e}, Content Loss: {content_score.item():.4f}\"\n",
    "                )\n",
    "\n",
    "                img_clone = input_img.clone()\n",
    "                imshow(img_clone, title=f\"Iteration {run}\")\n",
    "\n",
    "    progress_bar.bar_style = \"success\"\n",
    "    progress_bar.description = \"100%\"\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    info_box.value = \"<b>✅ Style Transfer abgeschlossen!</b>\"\n",
    "    metrics_box.value += f\"<br><b>⏳ Gesamtdauer:</b> {time.strftime('%H:%M:%S', time.gmtime(total_time))}\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_img.clamp_(0, 1)\n",
    "\n",
    "    def last_part(path):\n",
    "        return os.path.splitext(os.path.basename(path))[0] if path else \"unknown\"\n",
    "\n",
    "    content_name = last_part(content_path)\n",
    "    style_name = last_part(style_path)\n",
    "    gif_path = f\"style_transfer_content-{content_name}_style-{style_name}.gif\"\n",
    "\n",
    "    if frames:\n",
    "        frames[0].save(\n",
    "            gif_path,\n",
    "            save_all=True,\n",
    "            append_images=frames[1:],\n",
    "            duration=200,\n",
    "            loop=0,\n",
    "        )\n",
    "        print(f\"GIF gespeichert unter {gif_path}\")\n",
    "\n",
    "    return input_img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausführung\n",
    "\n",
    "In diesem letzten Schritt wird der gesamte Style-Transfer-Prozess gestartet und das Ergebnis visualisiert.\n",
    "\n",
    "\n",
    "### Erklärung der Schritte:\n",
    "\n",
    "| Codezeile                                      | Bedeutung                                                                 |\n",
    "|------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| `input_img = content_img.clone()`              | Erzeugt eine Kopie des Content-Bildes als Startpunkt für die Optimierung. Das Eingabebild wird im Laufe des Style Transfers angepasst. |\n",
    "| `output = run_style_transfer(...)`             | Startet den Style Transfer mit allen zuvor definierten Parametern. Das Ergebnis ist das optimierte Bild, das Stil und Inhalt vereint. |\n",
    "| `plt.figure()`                                 | Öffnet eine neue Matplotlib-Figur für die Bildanzeige.                   |\n",
    "| `imshow(output, title=\"Output Image\")`         | Zeigt das Ergebnisbild im Plot mit dem Titel „Output Image“.             |\n",
    "| `plt.ioff()` / `plt.show()`                    | Deaktiviert interaktive Anzeige und zeigt das Bildfenster.               |\n",
    "\n",
    "\n",
    "### Zweck:\n",
    "Das Bild `output`, das durch den Style Transfer entstanden ist, wird abschließend dargestellt. Es kombiniert den Inhalt des Content-Bildes mit dem Stil des Style-Bildes und stellt somit das Endergebnis des gesamten Prozesses dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Ausführung\n",
    "# -----------------------------------------------\n",
    "plt.figure()\n",
    "imshow(content_img, title=\"Content-Bild (Inhalt)\")\n",
    "\n",
    "plt.figure()\n",
    "imshow(style_img, title=\"Style-Bild (Stil)\")\n",
    "input_img = content_img.clone()\n",
    "\n",
    "output = run_style_transfer(\n",
    "    cnn,\n",
    "    cnn_normalization_mean,\n",
    "    cnn_normalization_std,\n",
    "    content_img,\n",
    "    style_img,\n",
    "    input_img,\n",
    "    show_every=100,   # Bildanzeige alle 100 Iterationen\n",
    "    gif_every=20,     # GIF-Frame alle 20 Iterationen\n",
    "    content_path=selected_content_path,\n",
    "    style_path=selected_style_path,\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "imshow(output, title=\"Output Image\")\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die obige Abbildung zeigt das Ergebnis. Was dieses Verfahren leistet, ist im Grunde genommen lediglich eine Form der Übertragung von Texturen eines Bilds auf ein anderes. \n",
    "\n",
    "- Am besten funktioniert es, wenn die Referenzstilbilder stark strukturiert und hochgradig selbstähnlich sind und die Zielinhaltbilder keine feinen Details enthalten müssen, um noch erkennbar zu sein. \n",
    "- Ziemlich abstrakte Merkmale, wie etwa der Stil eines Porträts, lassen sich für gewöhnlich nicht auf andere Bilder übertragen. \n",
    "- Der **Algorithmus gehört eher zur klassischen Signalverarbeitung als zur KI**, Sie dürfen hier also keine Wunder erwarten.\n",
    "- Darüber hinaus arbeitet der Algorithmus zur Stilübertragung langsam. Die hier ausgeführte Transformation ist aber so einfach, dass sie auch von einem kleinen, schnellen Feedforward-Netz erlernt werden kann – sofern Ihnen geeignete Trainingsdaten zur Verfügung stehen. Schnelle Stilübertragungen können also erzielt werden, indem man zunächst einmal den Rechenaufwand investiert, um bei unverändertem Referenzstilbild eine Reihe von Ein-/Ausgabe-Trainingsbeispielen zu erzeugen, mit denen das einfache CNN trainiert wird, damit es diese stilspezifische Transformation erlernt. Sobald das erledigt ist, erfolgt die Stilübertragung auf ein Bild augenblicklich, denn es ist ja nur noch eine Zustandsweitergabe in\n",
    "dem kleinen CNN nötig.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take aways\n",
    "\n",
    "Bei der Stilübertragung wird ein neues Bild erzeugt, das den Inhalt eines Zielinhaltbilds\n",
    "erhält und den Stil eines Referenzstilbilds übernimmt.\n",
    "- Inhalte können von den höher liegenden Aktivierungen eines CNNs erfasst werden.\n",
    "- Der Stil kann durch interne Korrelationen der Aktivierungen verschiedener Layer eines CNNs erfasst werden.\n",
    "- Deep Learning ermöglicht es, eine Stilübertragung als Optimierungsaufgabe für ein vortrainiertes CNN mit Verlustfunktion zu formulieren.\n",
    "- Von diesem grundlegenden Konzept sind viele Varianten und Verfeinerungen möglich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where's the intelligence?\n",
    "\n",
    "- Auch wenn Style-Transfer eher zur klassischen Signalverarbeitung gehört, gelicngt diese Übertragung von Mustern auf ein anderes Bild erst durch Deep Learning. Welche Eigenschaften von CNNs sind hier gefragt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
