{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> ¬© Christoph W√ºrsch, Fran√ßois Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN06/6.1-CNN_auf_kleinen_Datens√§tzen_pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.2 Ein CNN von Grund auf mit einer kleinen Datenmenge trainieren\n",
    "\n",
    "This notebook contains the code sample found in Chapter 5, Section 2 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff).\n",
    "\n",
    "Wenn Sie in einem beruflichen Umfeld maschinelles Sehen betreiben, werden Sie\n",
    "in der Praxis h√§ufig mit der Situation konfrontiert, dass f√ºr das Training eines\n",
    "Bildklassifizierungsmodells nur wenige Daten zur Verf√ºgung stehen. ¬ªWenige¬´\n",
    "kann hier einige Hundert, aber auch mehreren Zehntausend Bilder bedeuten. \n",
    "\n",
    "Als praktisches Beispiel betrachten wir die Klassifizierung von Bildern als Hunde oder\n",
    "Katzen. Die Datenmenge besteht aus 4.000 Bildern (2.000 Hunde und 2.000 Katzen).\n",
    "Wir verwenden 2.000 Bilder f√ºr das Training und jeweils 1.000 f√ºr die Validierung\n",
    "bzw. das Testen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In diesem Abschnitt werden wir eine grundlegende Strategie dazu er√∂rtern, wie\n",
    "wir die Aufgabe in Angriff nehmen k√∂nnen, ein neues Modell von Grund auf mit\n",
    "den wenigen vorhandenen Daten zu trainieren. \n",
    "\n",
    "- Zun√§chst einmal werden wir ein kleines naives CNN **ohne Regularisierung** mit den 2.000 Trainingssamples trainieren, um einsch√§tzen zu k√∂nnen, was sich erreichen l√§sst. Die Korrektklassifizierungsrate dieses Modells betr√§gt 71%. Hier ist die **√úberanpassung** das entscheidende Problem. \n",
    "\n",
    "\n",
    "- Anschliessend wird die **Datenaugmentation** vorgestellt, ein leistungsf√§higes Verfahren zur Abschw√§chung der √úberanpassung beim maschinellen Sehen. Mit diesem Verfahren werden wir die Korrektklassifizierungsrate des NNs auf 82% verbessern k√∂nnen.\n",
    "\n",
    "\n",
    "- Im darauffolgenden Abschnitt werden wir uns mit zwei weiteren elementaren Verfahren zur Anwendung des Deep Learnings auf kleine Datenmengen befassen: **Merkmalsextraktion** mit einem vortrainierten NN (damit werden wir eine Korrektklassifizierungsrate von 90 bis 96% erzielen) und **Feinabstimmung eines vortrainierten NNs** (hiermit werden wir schlie√ülich eine Korrektklassifizierungsrate von 97% erreichen). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Zusammengenommen bilden diese **drei Strategien**\n",
    "- (a) **Training** eines kleinen Modells von Grund auf\n",
    "- (b) **Merkmalsextraktion** mit einem vortrainierten Modell und \n",
    "- (c) **Feinabstimmung** eines vortrainierten Modells\n",
    "die Grundlage f√ºr Ihren zuk√ºnftigen Werkzeugkasten f√ºr die Bildklassifizierung, wenn nur wenige Daten verf√ºgbar sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Die Bedeutung des Deep Learnings f√ºr Aufgaben mit kleinen Datenmengen\n",
    "\n",
    "Mitunter heisst es, dass Deep Learning nur funktioniert, wenn sehr viele Daten\n",
    "verf√ºgbar sind. \n",
    "- Das stimmt zum Teil: Zu den grundlegenden Eigenschaften des *Deep Learnings* geh√∂rt die F√§higkeit, eigenst√§ndig interessante Merkmale in den Trainingsdaten aufzusp√ºren, ohne dass eine manuelle Merkmalserstellung erforderlich w√§re.\n",
    "- Das l√§sst sich jedoch nur erreichen, wenn viele Trainingssamples verf√ºgbar sind. \n",
    "- Das trifft insbesondere auf Aufgaben zu, bei denen die Eingabedaten  sehr viele Dimensionen besitzen, wie es bei Bildern der Fall ist.\n",
    "\n",
    "Was aber ¬ªviele Samples¬´ konkret bedeutet, ist relativ und h√§ngt von der Gr√∂sse und Tiefe des zu trainierenden NNs ab. So ist es beispielsweise nicht m√∂glich, ein CNN mit nur einigen Dutzend Samples darauf zu trainieren, komplexe Aufgaben\n",
    "zu l√∂sen, allerdings k√∂nnen ein paar Hundert schon durchaus ausreichen, wenn\n",
    "das Modell klein, gut regularisiert und die Aufgabe einfach ist. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Da CNNs **lokale translationsinvariante Merkmale** erlernen, sind sie insbesondere f√ºr Aufgaben der\n",
    "Sinneswahrnehmung sehr effizient. Ein CNN von Grund auf mit einer sehr kleinen\n",
    "Datenmenge zu trainieren, wird trotz des relativen Mangels an Daten zu halbwegs\n",
    "vern√ºnftigen Ergebnissen f√ºhren, ohne dass eine Merkmalserstellung von\n",
    "Hand erforderlich w√§re."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dar√ºber hinaus sind Deep-Learning-Modelle naturgem√§ss gut wiederverwendbar:\n",
    "- Sie k√∂nnen beispielsweise ein mit einer grossen Datenmenge trainiertes Bildklassifizierungs- oder Spracherkennungsmodell zur L√∂sung deutlich anderer Aufgaben verwenden, ohne gr√∂ssere √Ñnderungen vornehmen zu m√ºssen. \n",
    "- F√ºr das maschinelle Sehen stehen inzwischen viele vortrainierte Modelle (die f√ºr gew√∂hnlich mit der ImageNet-Datensammlung trainiert wurden) √∂ffentlich zum Herunterladen bereit und k√∂nnen genutzt werden, um mit sehr wenigen Daten leistungsf√§hige Modelle f√ºr das maschinelle Sehen zu entwickeln. \n",
    "\n",
    "Genau das werden wir im n√§chsten Abschnitt tun. Aber zun√§chst einmal werden die Daten ben√∂tigt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Daten herunterladen\n",
    "\n",
    "Der Datensatz, den wir hier verwenden werden, ist kein Bestandteil von Keras.\n",
    "Er wurde im Rahmen eines Wettbewerbs f√ºr maschinelles Sehen Ende 2013 von\n",
    "Kaggle zur Verf√ºgung gestellt. Damals waren CCNs noch nicht in aller Munde.\n",
    "Sie k√∂nnen die Datenmenge unter http://www.kaggle.com/c/dogs-vs-cats/\n",
    "data herunterladen. (Dazu m√ºssen Sie ein Konto einrichten, falls Sie noch keins besitzen ‚Äì aber keine Sorge, der Vorgang ist ganz einfach.)\n",
    "\n",
    "Bei den Bildern handelt es sich um farbige JPEG-Dateien mittlerer Aufl√∂sung.\n",
    "Die folgende Abbildung zeigt einige Beispiele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/cats_vs_dogs_samples.jpg\" width=\"640\"  align=\"center\"/>\n",
    "\n",
    "*Abbbildung 5.9 Beispiele der Bilder von Hunden und Katzen. Die Gr√∂sse der Bilder wurde nicht\n",
    "ge√§ndert: Die Samples sind von unterschiedlicher Gr√∂sse, liegen in verschiedenen\n",
    "Formaten vor usw.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kaggle\n",
    "\n",
    "Dass der Kaggle-Wettbewerb zur Unterscheidung von Hunden und Katzen 2013\n",
    "von Teilnehmern gewonnen wurde, die CNNs nutzten, war keine √úberraschung.\n",
    "Die besten Teilnehmer erreichten eine Korrektklassifizierungsrate von bis zu\n",
    "95%. Bei diesem Beispiel werden wir (im n√§chsten Abschnitt) eine √§hnlich gute\n",
    "Korrektklassifizierungsrate erzielen, obwohl wir zum Trainieren der Modelle nur\n",
    "10% der Daten verwenden werden, die den Wettbewerbsteilnehmern zur Verf√ºgung\n",
    "standen. Diese Datenmenge enth√§lt 25.000 Bilder von Hunden und Katzen\n",
    "(jeweils 12.500 von jeder Klasse) und ist 543 MB gross (komprimiert). Nach dem\n",
    "Herunterladen und Entpacken k√∂nnen Sie eine neue aus drei Teilmengen bestehende\n",
    "Datenmenge erzeugen: eine Trainingsdatenmenge mit jeweils 1.000\n",
    "Samples beider Klassen sowie eine Validierungs- und eine Testdatenmenge mit\n",
    "jeweils 500 Samples beider Klassen.\n",
    "\n",
    "Nachfolgend der dazu erforderliche Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os, shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDir(train_dir):\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.mkdir(train_dir)\n",
    "    else:\n",
    "        print('already existing: %s' % train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = 'E:/teaching/ANN/datasets/kaggle_original_data/train'\n",
    "\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = 'E:/teaching/ANN/datasets/cats_and_dogs_small'\n",
    "\n",
    "createDir(base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Trainings Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "createDir(train_dir)\n",
    "\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "createDir(validation_dir)\n",
    "\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "createDir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "createDir(train_cats_dir)\n",
    "    \n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "createDir(train_dogs_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Validation Datensatz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "createDir(validation_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "createDir(validation_dogs_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Test Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Directory with our test cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "createDir(test_cats_dir)\n",
    "\n",
    "# Directory with our test dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "createDir(test_dogs_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_dataset_dir)\n",
    "print(train_cats_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Zur √úberpr√ºfung der Vollst√§ndigkeit der Daten ermitteln wir die Anzahl der Bilder\n",
    "in den Teilmengen (Trainings-, Validierungs- und Testdatenmenge):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('total test cat images:', len(os.listdir(test_cats_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Es sind also tats√§chlich 2.000 Trainingsbilder, 1.000 Validierungsbilder und 1.000\n",
    "Testbilder vorhanden. Jede Teilmenge enth√§lt dieselbe Anzahl Samples beider Klassen:\n",
    "Hierbei handelt es sich um eine *ausgewogene Bin√§rklassifizierungsaufgabe* (engl. *balanced classes*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architektur des NN erzeugen\n",
    "\n",
    "Im letzten Beispiel haben wir f√ºr die Klassifizierung der MNIST-Ziffern ein kleines\n",
    "CNN erstellt. NNs dieser Art sollten Ihnen also bereits vertraut sein. Wir verwenden\n",
    "hier die gleiche allgemeine Struktur: \n",
    "- Das CNN besteht aus einem Stapel, in dem sich `Conv2D`- (mit `relu`-Aktivierung) und `MaxPooling2D`-Layer abwechseln.\n",
    "- Da wir es hier jedoch mit gr√∂sseren Bildern und einer komplexeren Aufgabenstellung zu tun haben, verwenden wir ein dementsprechend gr√∂sseres NN: \n",
    "- Es besitzt eine zus√§tzliche aus einem `Conv2D`- und einem `MaxPooling2D`-Layer bestehende Stufe. Diese soll zum einen die Kapazit√§t des CNNs erh√∂hen und zum anderen die Gr√∂sse der Feature-Map weiter reduzieren, damit sie nicht allzu gross ist, wenn der `Flatten`-Layer erreicht wird. \n",
    "- Da hier eingangs Eingaben der Gr√∂sse 150 √ó 150 verwendet werden (eine mehr oder weniger willk√ºrliche Wahl), ergeben sich unmittelbar vor dem `Flatten`-Layer Feature-Maps der Gr√∂sse 7 √ó 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "class CNNModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv4 = torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc1 = torch.nn.Linear(128 * 7 * 7, 512)  # Adjusted for input size\n",
    "        self.fc2 = torch.nn.Linear(512, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y = y.unsqueeze(1).float()  # Ensure correct shape\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5) == y).float().mean()\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y = y.unsqueeze(1).float()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5) == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32*(3*3*3+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32*64*(3*3)+64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64*128*(3*3)+128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die Anzahl der Kan√§le der Feature-Maps nimmt beim Durchlaufen des NNs\n",
    "nach und nach zu (sie w√§chst von 32 auf 128), w√§hrend die Gr√∂√üe allm√§hlich\n",
    "sinkt (von 150 √ó 150 auf 7 √ó 7). Dieses Verhalten werden Sie bei fast allen CNNs\n",
    "beobachten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "\n",
    "model = CNNModel()\n",
    "print(summarize(model, max_depth=3))  # max_depth controls how much detail to show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die gegebene Eingangsgr√∂√üe f√ºr die Daten ist:\n",
    "\n",
    "- **Bilder**: `[batch_size, channels, height, width]` ‚Üí `[20, 3, 150, 150]`\n",
    "- **Labels**: `[batch_size]` ‚Üí `[20]`\n",
    "\n",
    "Das aktuelle CNN-Architektur-Design ber√ºcksichtigt jedoch nicht die tats√§chliche Gr√∂√üe der Feature Maps, die nach den Faltungs- und Pooling-Schichten entstehen. Dies f√ºhrt zu einer falschen Anzahl von Eing√§ngen in die Fully Connected (FC) Layer.\n",
    "\n",
    "Die Architektur besteht aus mehreren **Faltungs- und Max-Pooling-Schichten**, die die r√§umlichen Dimensionen der Bilder ver√§ndern. Die Berechnung erfolgt wie folgt:\n",
    "\n",
    "1. **Conv2D (3 ‚Üí 32, Kernel: 3x3, kein Padding) ‚Üí MaxPool(2x2)**\n",
    "\n",
    "   - Eingangsgr√∂√üe: `(150, 150, 3)`\n",
    "   - Nach der Faltung: `(148, 148, 32)` (weil 3x3-Filter ohne Padding genutzt wird)\n",
    "   - Nach MaxPool(2x2): `(74, 74, 32)`\n",
    "\n",
    "2. **Conv2D (32 ‚Üí 64, Kernel: 3x3) ‚Üí MaxPool(2x2)**\n",
    "\n",
    "   - Nach der Faltung: `(72, 72, 64)`\n",
    "   - Nach MaxPool(2x2): `(36, 36, 64)`\n",
    "\n",
    "3. **Conv2D (64 ‚Üí 128, Kernel: 3x3) ‚Üí MaxPool(2x2)**\n",
    "\n",
    "   - Nach der Faltung: `(34, 34, 128)`\n",
    "   - Nach MaxPool(2x2): `(17, 17, 128)`\n",
    "\n",
    "4. **Conv2D (128 ‚Üí 128, Kernel: 3x3) ‚Üí MaxPool(2x2)**\n",
    "\n",
    "   - Nach der Faltung: `(15, 15, 128)`\n",
    "   - Nach MaxPool(2x2): `(7, 7, 128)`\n",
    "\n",
    "## Notwendige Anpassung der Fully Connected Layer\n",
    "\n",
    "Die  Berechnung ergibt eine **Feature-Map-Gr√∂√üe von 7 √ó 7 √ó 128 = 6272**. Daher muss die Fully Connected Layer angepasst werden:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(20, 3, 150, 150))  # Batch size of 1, 3 color channels, 150x150 image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Da es sich hier um eine **Bin√§rklassifizierungsaufgabe** handelt, endet das NN mit\n",
    "einer einzelnen Einheit (einem Dense-Layer der Gr√∂sse 1) und einer *sigmoid-Aktivierung*.\n",
    "Diese Einheit gibt die Wahrscheinlichkeit daf√ºr an, dass das NN die eine\n",
    "oder die andere der beiden Klasse erkannt hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Optimierer und Kostenfunktion (loss)\n",
    "\n",
    "Bei der Kompilierung kommt wie √ºblich der `Adam`-Optimierer zum Einsatz. Da\n",
    "das NN mit einer sigmoiden Einheit endet, wird als Kosten- oder Verlustfunktion die **bin√§re\n",
    "Kreuzentropie** verwendet.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def configure_optimizers(self):\n",
    "    return torch.optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datenvorverarbeitung (data preprocessing)\n",
    "\n",
    "Wie Sie inzwischen wissen, sollten die Daten bei der Vorverarbeitung in Fliesskommazahltensoren\n",
    "umgewandelt werden, bevor sie in das NN eingespeist werden.\n",
    "Die Daten sind in Form von JPEG-Dateien auf der Festplatte gespeichert,\n",
    "daher sind f√ºr die Verarbeitung durch das NN die folgenden Schritte erforderlich:\n",
    "1. Lesen Sie die Bilddateien ein.\n",
    "2. **Wandeln** Sie die JPEG-Dateien in **RGB**-Pixelwerte um.\n",
    "3. **Konvertieren** Sie die RGB-Pixelwerte in **Fliesskommazahl**-Tensoren (`float`).\n",
    "4. **Skalieren** Sie die Pixelwerte aus dem Bereich von 0 bis 255 neu, sodass sie im Intervall `[0, 1]` liegen. (Wie Sie wissen, sind standardisierte Eingabewerte f√ºr NNs besser geeignet.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Das mag m√ºhsam erscheinen, aber gl√ºcklicherweise bietet Keras verschiedene\n",
    "Hilfsprogramme, die diese Aufgaben automatisch erledigen k√∂nnen. Zu diesem\n",
    "Zweck gibt es ein Modul mit Hilfsprogrammen zur Bildverarbeitung, das Sie\n",
    "unter keras.preprocessing.image finden. Es enth√§lt insbesondere die Klasse\n",
    "ImageDataGenerator, die es erm√∂glicht, auf der Festplatte gespeicherte Bilddateien\n",
    "automatisch in einen Stapel von Tensoren der vorverarbeiteten Bilder umzuwandeln.\n",
    "Diese Klasse wird hier verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_dir)\n",
    "print(train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations similar to ImageDataGenerator\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),  # Resize images to 150x150\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize (similar to rescaling 1./255)\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "validation_dataset = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=20, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python-Generatoren \n",
    "\n",
    "- Ein Python-Generator ist ein Objekt, das als **Iterator** fungiert. \n",
    "- Sie k√∂nnen ein solches Objekt zusammen mit dem Operator `for ... in` verwenden. \n",
    "- Zum Erzeugen eines Generators dient der `yield`-Operator.\n",
    "\n",
    "Hier ist ein Beispiel f√ºr einen Generator, der Integer liefert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        yield i\n",
    "\n",
    "for item in generator():\n",
    "    print(item)\n",
    "    if item > 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Betrachten Sie die Ausgabe eines dieser Generatoren: \n",
    "- Er liefert Stapel von 150 √ó 150 Pixel grossen RGB-Bildern (Shape (20, 150, 150, 3)) und bin√§ren Klassenbezeichnungen (Shape (20,)). \n",
    "- Jeder dieser Stapel enth√§lt 20 Samples.\n",
    "- Beachten Sie hier, dass der Generator die Bilder im Zielverzeichnis in einer Endlosschleife durchl√§uft. \n",
    "- Deshalb m√ºssen Sie die Schleife irgendwann mit der break-Anweisung anhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through the DataLoader for one batch\n",
    "for data_batch, labels_batch in train_loader:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nun passen wir das Modell mithilfe des **Generators** an die Daten an. \n",
    "\n",
    "- Dazu verwenden wir die Methode`fit()`, welche auch f√ºr Datengeneratoren wie diesen unterst√ºtzt.\n",
    "- Die Methode erwartet als erstes Argument einen Python-Generator, der auf unbestimmte Zeit Stapel von Eingaben und Zielwerten liefert, wie es hier der Fall ist. \n",
    "- Da die Daten auf unbestimmte Zeit erzeugt werden, muss das Keras-Modell wissen, wie viele Samples durch den Generator abgerufen werden sollen, um eine Epoche abzuschliessen. \n",
    "- Dazu dient das Argument `steps_per_epoch`: Nach dem Abruf der `steps_per_epoch` Stapel durch den Generator ‚Äì also nachdem die `steps_per_epoch` Gradientenabstiegsschritte erledigt wurden ‚Äì, f√§hrt die `fit()`-Methode mit der n√§chsten Epoche fort. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Beim Aufruf von `fit()` k√∂nnen Sie das Argument `validation_data` √ºbergeben. \n",
    "- An dieser Stelle ist es wichtig, darauf zu achten, dass dieses Argument zwar ein Datengenerator sein darf, es k√∂nnte jedoch auch ein Tupel von `Numpy`-Arrays sein. \n",
    "- Wenn Sie als `validation_data` einen Generator √ºbergeben, sollte er auf unbestimmte Zeit Stapel von Validierungsdaten liefern. \n",
    "- In diesem Fall sollten Sie zudem dem Argument `validation_steps` einen Wert zuweisen, der festlegt, wie viele Stapel der Generator f√ºr die Bewertung abrufen soll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Im vorliegenden Fall bestehen die Stapel aus 20 Samples, daher sind 100 Stapel n√∂tig, um die 2.000 Samples zu verarbeiten.\n",
    "Wir trainieren f√ºr 30 Epochen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "logger = CSVLogger(\"logs\", name=\"cnn_model\")\n",
    "model = CNNModel()\n",
    "trainer = pl.Trainer(max_epochs=10, accelerator=\"auto\", logger=logger)\n",
    "trainer.fit(model, train_loader, validation_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es empfiehlt sich, das Modell nach dem Training stets abzuspeichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"cats_and_dogs_small_1.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.state_dict()` speichert nur die Gewichte des Modells (empfohlen f√ºr PyTorch-Modelle).\n",
    "Sie k√∂nnen das Modell sp√§ter laden mit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()  # Initialize the model\n",
    "model.load_state_dict(torch.load(\"cats_and_dogs_small_1.pth\"))\n",
    "model.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Sie die gesamte Modellarchitektur und die Gewichte zusammen speichern m√∂chten, verwenden Sie:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "torch.save(model, \"cats_and_dogs_small_1_full.pth\")\n",
    "model = torch.load(\"cats_and_dogs_small_1_full.pth\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nun geben wir die Diagramme der Korrektklassifizierungsrate und der Verlustfunktion\n",
    "beim Training aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load training logs and plot\n",
    "log_path = \"logs/cnn_model/version_3/metrics.csv\"\n",
    "df = pd.read_csv(log_path)\n",
    "df.head()\n",
    "df=df.groupby('epoch').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(df):   \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(df[\"step\"], df[\"train_loss\"],'.-', label=\"Train Loss\")\n",
    "    plt.plot(df[\"step\"], df[\"val_loss\"],'.-.', label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(df[\"step\"], df[\"train_acc\"], '.-',label=\"Train Accuracy\")\n",
    "    plt.plot(df[\"step\"], df[\"val_acc\"], '.-.',label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die beiden Diagramme sind f√ºr eine **√úberanpassung (overfitting)** charakteristisch. \n",
    "- Die Korrektklassifizierungsrate nimmt beim Training im Laufe der Zeit ann√§hernd linear zu, bis sie schlie√ülich fast 100% erreicht, w√§hrend sie bei der Validierung nicht √ºber 70% bis 72% hinauskommt. \n",
    "- Die Verlustfunktion erreicht bei der Validierung schon nach der f√ºnften Epoche ihr Minimum und √§ndert sich w√§hrend der folgenden Epochen kaum. \n",
    "- Die Verlustfunktion beim Training hingegen nimmt ann√§hernd linear ab, bis sie fast null erreicht.\n",
    "\n",
    "Da hier nur relativ wenige Trainingssamples vorliegen (2.000), werden Sie der √úberanpassung die gr√∂√üte Beachtung schenken m√ºssen. Sie kennen inzwischen ja schon einige der Methoden zum Abschw√§chen der √úberanpassung, wie das\n",
    "**Dropout-Verfahren** oder die **L2-Regularisierung**. \n",
    "\n",
    "Nun werden wir ein weiteres f√ºr das maschinelle Sehen geeignetes Verfahren einsetzen, das fast √ºberall bei der\n",
    "Bildverarbeitung mit Deep-Learning-Modellen verwendet wird: **die Datenaugmentation (data augmentation)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "- Die √úberanpassung wird dadurch verursacht, dass zu wenige Samples verf√ºgbar sind, sodass es nicht m√∂glich ist, ein Modell zu trainieren, das sich gut f√ºr neue Daten verallgemeinern l√§sst. \n",
    "- W√§ren unbegrenzt Daten verf√ºgbar, w√ºrde Ihr Modell s√§mtliche Aspekte der vorliegenden Datenverteilung kennen: Es w√ºrde nie zu einer √úberanpassung kommen. \n",
    "- Die **Datenaugmentation** verfolgt den Ansatz, anhand der vorhandenen Trainingssamples zus√§tzliche Daten zu erstellen, indem die Datensammlung durch neue Samples erweitert wird, die durch eine Reihe **zuf√§lliger Transformationen** erzeugt werden, die glaubw√ºrdig aussehende Bilder liefern. \n",
    "- Das Ziel ist hier, dass Ihr Modell beim Training niemals zwei v√∂llig identische Bilder zu sehen bekommt. Das erleichtert es dem Modell, weitere Aspekte der Daten zu erkennen, und verbessert die Verallgemeinerungsf√§higkeit.\n",
    "\n",
    "In Keras kann dieses Ziel durch die Konfiguration verschiedener zuf√§lliger Transformationen erreicht werden, die auf die von der `ImageDataGenerator`-Instanz eingelesenen Bilder angewendet werden. Betrachten wir zun√§chst einmal ein Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations with data augmentation for training set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),  # Resize images to 150x150\n",
    "    transforms.RandomRotation(40),  # Random rotation within 40 degrees\n",
    "    transforms.RandomAffine(degrees=0, shear=0.2),  # Shear transformation\n",
    "    transforms.RandomResizedCrop(150, scale=(0.8, 1.2)),  # Zoom effect\n",
    "    transforms.RandomHorizontalFlip(),  # Horizontal flipping\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),  # Width & height shift\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize\n",
    "])\n",
    "\n",
    "# Transformations for validation set (no augmentation)\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "validation_dataset = datasets.ImageFolder(root=validation_dir, transform=validation_transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Erkl√§rung der Augmentationen**\n",
    "\n",
    "| Augmentation | Beschreibung |\n",
    "|-------------|-------------|\n",
    "| `transforms.RandomRotation(40)` | Dreht das Bild zuf√§llig um bis zu ¬±40 Grad. |\n",
    "| `transforms.RandomAffine(degrees=0, shear=0.2)` | Wendet eine Schertransformation an. |\n",
    "| `transforms.RandomResizedCrop(150, scale=(0.8, 1.2))` | Wendet einen Zoom-Effekt an. |\n",
    "| `transforms.RandomHorizontalFlip()` | Spiegelt das Bild zuf√§llig horizontal. |\n",
    "| `transforms.RandomAffine(degrees=0, translate=(0.2, 0.2))` | Verschiebt das Bild zuf√§llig um 20 % der Bildgr√∂√üe. |\n",
    "| `transforms.ToTensor()` | Konvertiert Bilder in Tensoren. |\n",
    "| `transforms.Normalize((0.5,), (0.5,))` | Normalisiert die Pixelwerte. |\n",
    "\n",
    "Diese Konfiguration ahmt `ImageDataGenerator` in PyTorch nach und stellt eine robuste Datenaugmentation f√ºr das Training sicher. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the same data augmentation transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomAffine(degrees=0, shear=0.2),\n",
    "    transforms.RandomResizedCrop(150, scale=(0.8, 1.2)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Get a sample image path\n",
    "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
    "img_path = fnames[1]  # Select one image\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img = img.resize((150, 150))\n",
    "\n",
    "# Convert image to tensor\n",
    "img_tensor = transforms.ToTensor()(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)  # Reshape to (1, C, H, W)\n",
    "\n",
    "# Generate augmented images\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "i = 0\n",
    "for _ in range(4):\n",
    "    augmented_img = transform(img)\n",
    "    axes[i].imshow(transforms.ToPILImage()(augmented_img))\n",
    "    axes[i].axis(\"off\")\n",
    "    i += 1\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wenn Sie ein neues NN mit dieser Konfiguration der Datenaugmentation trainieren,\n",
    "wird es niemals zwei identische Bilder zu sehen bekommen. \n",
    "- Die Bilder, die es sieht, sind dessen ungeachtet eng miteinander verwoben, denn sie wurden ja anhand nur weniger urspr√ºnglicher Bilder erstellt. Sie k√∂nnen auf diese Weise keine neuen Informationen erzeugen, sondern lediglich die vorhandenen Informationen anders miteinander verkn√ºpfen. \n",
    "- Daher reicht dieses Verfahren wom√∂glich nicht aus, um die √úberanpassung komplett loszuwerden. \n",
    "- Um die √úberanpassung weiter abzuschw√§chen, f√ºgen wir dem Modell unmittelbar vor dem vollst√§ndig verbundenen Klassifizierer einen **`Dropout`-Layer** hinzu.\n",
    "\n",
    "**NB: Die Validierungsdaten sollen nicht transformiert werden!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "logger = CSVLogger(\"logs\", name=\"cnn_model2\")\n",
    "model2 = CNNModel()\n",
    "trainer = pl.Trainer(max_epochs=30, accelerator=\"auto\", logger=logger)\n",
    "trainer.fit(model2, train_loader, validation_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Und nun trainieren wird das CNN mit Datenaugmentation und Dropout-Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Speichern Sie das Modell ‚Äì Sie werden es in Abschnitt 5.4 erneut verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"cats_and_dogs_small_2.pth\")\n",
    "#model2 = CNNModel()  # Initialize the model\n",
    "#model2.load_state_dict(torch.load(\"cats_and_dogs_small_1.pth\"))\n",
    "#model2.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training logs and plot\n",
    "log_path = \"logs/cnn_model2/version_1/metrics.csv\"\n",
    "dg = pd.read_csv(log_path)\n",
    "dg.head()\n",
    "dg=dg.groupby('epoch').mean()\n",
    "plot_history(dg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wir geben wieder die Diagramme aus (siehe Abbildung 5.12 und Abbildung 5.13).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dank *Datenaugmentation und Dropout-Verfahren* kommt es nicht mehr zu einer *√úberanpassung*: Die Kurven beim Training und bei der Validierung verlaufen sehr √§hnlich. Sie erzielen jetzt eine Korrektklassifizierungsrate von 82%, eine (relative)\n",
    "Verbesserung um 15% im Vergleich zum Modell ohne Regularisierung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Durch den Einsatz weiterer *Regularisierungsverfahren* und die Abstimmung der Parameter des NNs (etwa die Anzahl der Filter pro Layer oder die Anzahl der Layer im NN) kann sogar eine noch h√∂here Korrektklassifizierungsrate von bis zu 87% erreicht werden. \n",
    "- Es d√ºrfte sich jedoch als schwierig erweisen, allein durch das Training des CNNs von Grund auf eine h√∂here Korrektklassifizierungsrate zu erzielen, weil nur so wenige Daten verf√ºgbar sind. \n",
    "- Der n√§chste Schritt zur Verbesserung der Korrektklassifizierungsrate bei dieser Aufgabe ist der Einsatz eines vortrainierten Modells, auf das sich die beiden folgenden Abschnitte konzentrieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where's the intelligence?\n",
    "\n",
    "- Was emuliert die Data Augmentation?\n",
    "- Diskutieren Sie mit Ihren Peers, welche Regularisierungsverfahren f√ºr CNN angewendet werden k√∂nnen?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "ger",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ger",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
