{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\"  align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN06/6.1-CNN_auf_kleinen_Datensätzen_pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.2 Ein CNN von Grund auf mit einer kleinen Datenmenge trainieren\n",
    "\n",
    "This notebook contains the code sample found in Chapter 5, Section 2 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff).\n",
    "\n",
    "Wenn Sie in einem beruflichen Umfeld maschinelles Sehen betreiben, werden Sie\n",
    "in der Praxis häufig mit der Situation konfrontiert, dass für das Training eines\n",
    "Bildklassifizierungsmodells nur wenige Daten zur Verfügung stehen. »Wenige«\n",
    "kann hier einige Hundert, aber auch mehreren Zehntausend Bilder bedeuten. \n",
    "\n",
    "Als praktisches Beispiel betrachten wir die Klassifizierung von Bildern als Hunde oder\n",
    "Katzen. Die Datenmenge besteht aus 4.000 Bildern (2.000 Hunde und 2.000 Katzen).\n",
    "Wir verwenden 2.000 Bilder für das Training und jeweils 1.000 für die Validierung\n",
    "bzw. das Testen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In diesem Abschnitt werden wir eine grundlegende Strategie dazu erörtern, wie\n",
    "wir die Aufgabe in Angriff nehmen können, ein neues Modell von Grund auf mit\n",
    "den wenigen vorhandenen Daten zu trainieren. \n",
    "\n",
    "- Zunächst einmal werden wir ein kleines naives CNN **ohne Regularisierung** mit den 2.000 Trainingssamples trainieren, um einschätzen zu können, was sich erreichen lässt. Die Korrektklassifizierungsrate dieses Modells beträgt 71%. Hier ist die **Überanpassung** das entscheidende Problem. \n",
    "\n",
    "\n",
    "- Anschliessend wird die **Datenaugmentation** vorgestellt, ein leistungsfähiges Verfahren zur Abschwächung der Überanpassung beim maschinellen Sehen. Mit diesem Verfahren werden wir die Korrektklassifizierungsrate des NNs auf 82% verbessern können.\n",
    "\n",
    "\n",
    "- Im darauffolgenden Abschnitt werden wir uns mit zwei weiteren elementaren Verfahren zur Anwendung des Deep Learnings auf kleine Datenmengen befassen: **Merkmalsextraktion** mit einem vortrainierten NN (damit werden wir eine Korrektklassifizierungsrate von 90 bis 96% erzielen) und **Feinabstimmung eines vortrainierten NNs** (hiermit werden wir schließlich eine Korrektklassifizierungsrate von 97% erreichen). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Zusammengenommen bilden diese **drei Strategien**\n",
    "- (a) **Training** eines kleinen Modells von Grund auf\n",
    "- (b) **Merkmalsextraktion** mit einem vortrainierten Modell und \n",
    "- (c) **Feinabstimmung** eines vortrainierten Modells\n",
    "die Grundlage für Ihren zukünftigen Werkzeugkasten für die Bildklassifizierung, wenn nur wenige Daten verfügbar sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Die Bedeutung des Deep Learnings für Aufgaben mit kleinen Datenmengen\n",
    "\n",
    "Mitunter heisst es, dass Deep Learning nur funktioniert, wenn sehr viele Daten\n",
    "verfügbar sind. \n",
    "- Das stimmt zum Teil: Zu den grundlegenden Eigenschaften des *Deep Learnings* gehört die Fähigkeit, eigenständig interessante Merkmale in den Trainingsdaten aufzuspüren, ohne dass eine manuelle Merkmalserstellung erforderlich wäre.\n",
    "- Das lässt sich jedoch nur erreichen, wenn viele Trainingssamples verfügbar sind. \n",
    "- Das trifft insbesondere auf Aufgaben zu, bei denen die Eingabedaten  sehr viele Dimensionen besitzen, wie es bei Bildern der Fall ist.\n",
    "\n",
    "Was aber »viele Samples« konkret bedeutet, ist relativ und hängt von der Grösse und Tiefe des zu trainierenden NNs ab. So ist es beispielsweise nicht möglich, ein CNN mit nur einigen Dutzend Samples darauf zu trainieren, komplexe Aufgaben\n",
    "zu lösen, allerdings können ein paar Hundert schon durchaus ausreichen, wenn\n",
    "das Modell klein, gut regularisiert und die Aufgabe einfach ist. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Da CNNs **lokale translationsinvariante Merkmale** erlernen, sind sie insbesondere für Aufgaben der\n",
    "Sinneswahrnehmung sehr effizient. Ein CNN von Grund auf mit einer sehr kleinen\n",
    "Datenmenge zu trainieren, wird trotz des relativen Mangels an Daten zu halbwegs\n",
    "vernünftigen Ergebnissen führen, ohne dass eine Merkmalserstellung von\n",
    "Hand erforderlich wäre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Darüber hinaus sind Deep-Learning-Modelle naturgemäss gut wiederverwendbar:\n",
    "- Sie können beispielsweise ein mit einer grossen Datenmenge trainiertes Bildklassifizierungs- oder Spracherkennungsmodell zur Lösung deutlich anderer Aufgaben verwenden, ohne grössere Änderungen vornehmen zu müssen. \n",
    "- Für das maschinelle Sehen stehen inzwischen viele vortrainierte Modelle (die für gewöhnlich mit der ImageNet-Datensammlung trainiert wurden) öffentlich zum Herunterladen bereit und können genutzt werden, um mit sehr wenigen Daten leistungsfähige Modelle für das maschinelle Sehen zu entwickeln. \n",
    "\n",
    "Genau das werden wir im nächsten Abschnitt tun. Aber zunächst einmal werden die Daten benötigt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Daten herunterladen\n",
    "\n",
    "Der Datensatz, den wir hier verwenden werden, ist kein Bestandteil von Keras.\n",
    "Er wurde im Rahmen eines Wettbewerbs für maschinelles Sehen Ende 2013 von\n",
    "Kaggle zur Verfügung gestellt. Damals waren CCNs noch nicht in aller Munde.\n",
    "Sie können die Datenmenge unter http://www.kaggle.com/c/dogs-vs-cats/\n",
    "data herunterladen. (Dazu müssen Sie ein Konto einrichten, falls Sie noch keins besitzen – aber keine Sorge, der Vorgang ist ganz einfach.)\n",
    "\n",
    "Bei den Bildern handelt es sich um farbige JPEG-Dateien mittlerer Auflösung.\n",
    "Die folgende Abbildung zeigt einige Beispiele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/cats_vs_dogs_samples.jpg\" width=\"640\"  align=\"center\"/>\n",
    "\n",
    "*Abbbildung 5.9 Beispiele der Bilder von Hunden und Katzen. Die Grösse der Bilder wurde nicht\n",
    "geändert: Die Samples sind von unterschiedlicher Grösse, liegen in verschiedenen\n",
    "Formaten vor usw.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kaggle\n",
    "\n",
    "Dass der Kaggle-Wettbewerb zur Unterscheidung von Hunden und Katzen 2013\n",
    "von Teilnehmern gewonnen wurde, die CNNs nutzten, war keine Überraschung.\n",
    "Die besten Teilnehmer erreichten eine Korrektklassifizierungsrate von bis zu\n",
    "95%. Bei diesem Beispiel werden wir (im nächsten Abschnitt) eine ähnlich gute\n",
    "Korrektklassifizierungsrate erzielen, obwohl wir zum Trainieren der Modelle nur\n",
    "10% der Daten verwenden werden, die den Wettbewerbsteilnehmern zur Verfügung\n",
    "standen. Diese Datenmenge enthält 25.000 Bilder von Hunden und Katzen\n",
    "(jeweils 12.500 von jeder Klasse) und ist 543 MB gross (komprimiert). Nach dem\n",
    "Herunterladen und Entpacken können Sie eine neue aus drei Teilmengen bestehende\n",
    "Datenmenge erzeugen: eine Trainingsdatenmenge mit jeweils 1.000\n",
    "Samples beider Klassen sowie eine Validierungs- und eine Testdatenmenge mit\n",
    "jeweils 500 Samples beider Klassen.\n",
    "\n",
    "Nachfolgend der dazu erforderliche Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os, shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDir(train_dir):\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.mkdir(train_dir)\n",
    "    else:\n",
    "        print('already existing: %s' % train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = 'E:/teaching/ANN/datasets/kaggle_original_data/train'\n",
    "\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = 'E:/teaching/ANN/datasets/cats_and_dogs_small'\n",
    "\n",
    "createDir(base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Trainings Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "createDir(train_dir)\n",
    "\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "createDir(validation_dir)\n",
    "\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "createDir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "createDir(train_cats_dir)\n",
    "    \n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "createDir(train_dogs_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Validation Datensatz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "createDir(validation_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "createDir(validation_dogs_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Test Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Directory with our test cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "createDir(test_cats_dir)\n",
    "\n",
    "# Directory with our test dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "createDir(test_dogs_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_dataset_dir)\n",
    "print(train_cats_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Zur Überprüfung der Vollständigkeit der Daten ermitteln wir die Anzahl der Bilder\n",
    "in den Teilmengen (Trainings-, Validierungs- und Testdatenmenge):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('total test cat images:', len(os.listdir(test_cats_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Es sind also tatsächlich 2.000 Trainingsbilder, 1.000 Validierungsbilder und 1.000\n",
    "Testbilder vorhanden. Jede Teilmenge enthält dieselbe Anzahl Samples beider Klassen:\n",
    "Hierbei handelt es sich um eine *ausgewogene Binärklassifizierungsaufgabe* (engl. *balanced classes*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architektur des NN erzeugen\n",
    "\n",
    "Im letzten Beispiel haben wir für die Klassifizierung der MNIST-Ziffern ein kleines\n",
    "CNN erstellt. NNs dieser Art sollten Ihnen also bereits vertraut sein. Wir verwenden\n",
    "hier die gleiche allgemeine Struktur: \n",
    "- Das CNN besteht aus einem Stapel, in dem sich `Conv2D`- (mit `relu`-Aktivierung) und `MaxPooling2D`-Layer abwechseln.\n",
    "- Da wir es hier jedoch mit grösseren Bildern und einer komplexeren Aufgabenstellung zu tun haben, verwenden wir ein dementsprechend grösseres NN: \n",
    "- Es besitzt eine zusätzliche aus einem `Conv2D`- und einem `MaxPooling2D`-Layer bestehende Stufe. Diese soll zum einen die Kapazität des CNNs erhöhen und zum anderen die Grösse der Feature-Map weiter reduzieren, damit sie nicht allzu gross ist, wenn der `Flatten`-Layer erreicht wird. \n",
    "- Da hier eingangs Eingaben der Grösse 150 × 150 verwendet werden (eine mehr oder weniger willkürliche Wahl), ergeben sich unmittelbar vor dem `Flatten`-Layer Feature-Maps der Grösse 7 × 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "class CNNModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv4 = torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc1 = torch.nn.Linear(128 * 7 * 7, 512)  # Adjusted for input size\n",
    "        self.fc2 = torch.nn.Linear(512, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y = y.unsqueeze(1).float()  # Ensure correct shape\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5) == y).float().mean()\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y = y.unsqueeze(1).float()\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = ((y_hat > 0.5) == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32*(3*3*3+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32*64*(3*3)+64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64*128*(3*3)+128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die Anzahl der Kanäle der Feature-Maps nimmt beim Durchlaufen des NNs\n",
    "nach und nach zu (sie wächst von 32 auf 128), während die Größe allmählich\n",
    "sinkt (von 150 × 150 auf 7 × 7). Dieses Verhalten werden Sie bei fast allen CNNs\n",
    "beobachten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "\n",
    "model = CNNModel()\n",
    "print(summarize(model, max_depth=3))  # max_depth controls how much detail to show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die gegebene Eingangsgröße für die Daten ist:\n",
    "\n",
    "- **Bilder**: `[batch_size, channels, height, width]` → `[20, 3, 150, 150]`\n",
    "- **Labels**: `[batch_size]` → `[20]`\n",
    "\n",
    "Das aktuelle CNN-Architektur-Design berücksichtigt jedoch nicht die tatsächliche Größe der Feature Maps, die nach den Faltungs- und Pooling-Schichten entstehen. Dies führt zu einer falschen Anzahl von Eingängen in die Fully Connected (FC) Layer.\n",
    "\n",
    "Die Architektur besteht aus mehreren **Faltungs- und Max-Pooling-Schichten**, die die räumlichen Dimensionen der Bilder verändern. Die Berechnung erfolgt wie folgt:\n",
    "\n",
    "1. **Conv2D (3 → 32, Kernel: 3x3, kein Padding) → MaxPool(2x2)**\n",
    "\n",
    "   - Eingangsgröße: `(150, 150, 3)`\n",
    "   - Nach der Faltung: `(148, 148, 32)` (weil 3x3-Filter ohne Padding genutzt wird)\n",
    "   - Nach MaxPool(2x2): `(74, 74, 32)`\n",
    "\n",
    "2. **Conv2D (32 → 64, Kernel: 3x3) → MaxPool(2x2)**\n",
    "\n",
    "   - Nach der Faltung: `(72, 72, 64)`\n",
    "   - Nach MaxPool(2x2): `(36, 36, 64)`\n",
    "\n",
    "3. **Conv2D (64 → 128, Kernel: 3x3) → MaxPool(2x2)**\n",
    "\n",
    "   - Nach der Faltung: `(34, 34, 128)`\n",
    "   - Nach MaxPool(2x2): `(17, 17, 128)`\n",
    "\n",
    "4. **Conv2D (128 → 128, Kernel: 3x3) → MaxPool(2x2)**\n",
    "\n",
    "   - Nach der Faltung: `(15, 15, 128)`\n",
    "   - Nach MaxPool(2x2): `(7, 7, 128)`\n",
    "\n",
    "## Notwendige Anpassung der Fully Connected Layer\n",
    "\n",
    "Die  Berechnung ergibt eine **Feature-Map-Größe von 7 × 7 × 128 = 6272**. Daher muss die Fully Connected Layer angepasst werden:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(20, 3, 150, 150))  # Batch size of 1, 3 color channels, 150x150 image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Da es sich hier um eine **Binärklassifizierungsaufgabe** handelt, endet das NN mit\n",
    "einer einzelnen Einheit (einem Dense-Layer der Grösse 1) und einer *sigmoid-Aktivierung*.\n",
    "Diese Einheit gibt die Wahrscheinlichkeit dafür an, dass das NN die eine\n",
    "oder die andere der beiden Klasse erkannt hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Optimierer und Kostenfunktion (loss)\n",
    "\n",
    "Bei der Kompilierung kommt wie üblich der `Adam`-Optimierer zum Einsatz. Da\n",
    "das NN mit einer sigmoiden Einheit endet, wird als Kosten- oder Verlustfunktion die **binäre\n",
    "Kreuzentropie** verwendet.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def configure_optimizers(self):\n",
    "    return torch.optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datenvorverarbeitung (data preprocessing)\n",
    "\n",
    "Wie Sie inzwischen wissen, sollten die Daten bei der Vorverarbeitung in Fliesskommazahltensoren\n",
    "umgewandelt werden, bevor sie in das NN eingespeist werden.\n",
    "Die Daten sind in Form von JPEG-Dateien auf der Festplatte gespeichert,\n",
    "daher sind für die Verarbeitung durch das NN die folgenden Schritte erforderlich:\n",
    "1. Lesen Sie die Bilddateien ein.\n",
    "2. **Wandeln** Sie die JPEG-Dateien in **RGB**-Pixelwerte um.\n",
    "3. **Konvertieren** Sie die RGB-Pixelwerte in **Fliesskommazahl**-Tensoren (`float`).\n",
    "4. **Skalieren** Sie die Pixelwerte aus dem Bereich von 0 bis 255 neu, sodass sie im Intervall `[0, 1]` liegen. (Wie Sie wissen, sind standardisierte Eingabewerte für NNs besser geeignet.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Das mag mühsam erscheinen, aber glücklicherweise bietet Keras verschiedene\n",
    "Hilfsprogramme, die diese Aufgaben automatisch erledigen können. Zu diesem\n",
    "Zweck gibt es ein Modul mit Hilfsprogrammen zur Bildverarbeitung, das Sie\n",
    "unter keras.preprocessing.image finden. Es enthält insbesondere die Klasse\n",
    "ImageDataGenerator, die es ermöglicht, auf der Festplatte gespeicherte Bilddateien\n",
    "automatisch in einen Stapel von Tensoren der vorverarbeiteten Bilder umzuwandeln.\n",
    "Diese Klasse wird hier verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_dir)\n",
    "print(train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations similar to ImageDataGenerator\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),  # Resize images to 150x150\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize (similar to rescaling 1./255)\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "validation_dataset = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=20, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python-Generatoren \n",
    "\n",
    "- Ein Python-Generator ist ein Objekt, das als **Iterator** fungiert. \n",
    "- Sie können ein solches Objekt zusammen mit dem Operator `for ... in` verwenden. \n",
    "- Zum Erzeugen eines Generators dient der `yield`-Operator.\n",
    "\n",
    "Hier ist ein Beispiel für einen Generator, der Integer liefert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        yield i\n",
    "\n",
    "for item in generator():\n",
    "    print(item)\n",
    "    if item > 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Betrachten Sie die Ausgabe eines dieser Generatoren: \n",
    "- Er liefert Stapel von 150 × 150 Pixel grossen RGB-Bildern (Shape (20, 150, 150, 3)) und binären Klassenbezeichnungen (Shape (20,)). \n",
    "- Jeder dieser Stapel enthält 20 Samples.\n",
    "- Beachten Sie hier, dass der Generator die Bilder im Zielverzeichnis in einer Endlosschleife durchläuft. \n",
    "- Deshalb müssen Sie die Schleife irgendwann mit der break-Anweisung anhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through the DataLoader for one batch\n",
    "for data_batch, labels_batch in train_loader:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nun passen wir das Modell mithilfe des **Generators** an die Daten an. \n",
    "\n",
    "- Dazu verwenden wir die Methode`fit()`, welche auch für Datengeneratoren wie diesen unterstützt.\n",
    "- Die Methode erwartet als erstes Argument einen Python-Generator, der auf unbestimmte Zeit Stapel von Eingaben und Zielwerten liefert, wie es hier der Fall ist. \n",
    "- Da die Daten auf unbestimmte Zeit erzeugt werden, muss das Keras-Modell wissen, wie viele Samples durch den Generator abgerufen werden sollen, um eine Epoche abzuschliessen. \n",
    "- Dazu dient das Argument `steps_per_epoch`: Nach dem Abruf der `steps_per_epoch` Stapel durch den Generator – also nachdem die `steps_per_epoch` Gradientenabstiegsschritte erledigt wurden –, fährt die `fit()`-Methode mit der nächsten Epoche fort. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Beim Aufruf von `fit()` können Sie das Argument `validation_data` übergeben. \n",
    "- An dieser Stelle ist es wichtig, darauf zu achten, dass dieses Argument zwar ein Datengenerator sein darf, es könnte jedoch auch ein Tupel von `Numpy`-Arrays sein. \n",
    "- Wenn Sie als `validation_data` einen Generator übergeben, sollte er auf unbestimmte Zeit Stapel von Validierungsdaten liefern. \n",
    "- In diesem Fall sollten Sie zudem dem Argument `validation_steps` einen Wert zuweisen, der festlegt, wie viele Stapel der Generator für die Bewertung abrufen soll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Im vorliegenden Fall bestehen die Stapel aus 20 Samples, daher sind 100 Stapel nötig, um die 2.000 Samples zu verarbeiten.\n",
    "Wir trainieren für 30 Epochen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "logger = CSVLogger(\"logs\", name=\"cnn_model\")\n",
    "model = CNNModel()\n",
    "trainer = pl.Trainer(max_epochs=10, accelerator=\"auto\", logger=logger)\n",
    "trainer.fit(model, train_loader, validation_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es empfiehlt sich, das Modell nach dem Training stets abzuspeichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"cats_and_dogs_small_1.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.state_dict()` speichert nur die Gewichte des Modells (empfohlen für PyTorch-Modelle).\n",
    "Sie können das Modell später laden mit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()  # Initialize the model\n",
    "model.load_state_dict(torch.load(\"cats_and_dogs_small_1.pth\"))\n",
    "model.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Sie die gesamte Modellarchitektur und die Gewichte zusammen speichern möchten, verwenden Sie:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "torch.save(model, \"cats_and_dogs_small_1_full.pth\")\n",
    "model = torch.load(\"cats_and_dogs_small_1_full.pth\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nun geben wir die Diagramme der Korrektklassifizierungsrate und der Verlustfunktion\n",
    "beim Training aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load training logs and plot\n",
    "log_path = \"logs/cnn_model/version_3/metrics.csv\"\n",
    "df = pd.read_csv(log_path)\n",
    "df.head()\n",
    "df=df.groupby('epoch').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(df):   \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(df[\"step\"], df[\"train_loss\"],'.-', label=\"Train Loss\")\n",
    "    plt.plot(df[\"step\"], df[\"val_loss\"],'.-.', label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(df[\"step\"], df[\"train_acc\"], '.-',label=\"Train Accuracy\")\n",
    "    plt.plot(df[\"step\"], df[\"val_acc\"], '.-.',label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die beiden Diagramme sind für eine **Überanpassung (overfitting)** charakteristisch. \n",
    "- Die Korrektklassifizierungsrate nimmt beim Training im Laufe der Zeit annähernd linear zu, bis sie schließlich fast 100% erreicht, während sie bei der Validierung nicht über 70% bis 72% hinauskommt. \n",
    "- Die Verlustfunktion erreicht bei der Validierung schon nach der fünften Epoche ihr Minimum und ändert sich während der folgenden Epochen kaum. \n",
    "- Die Verlustfunktion beim Training hingegen nimmt annähernd linear ab, bis sie fast null erreicht.\n",
    "\n",
    "Da hier nur relativ wenige Trainingssamples vorliegen (2.000), werden Sie der Überanpassung die größte Beachtung schenken müssen. Sie kennen inzwischen ja schon einige der Methoden zum Abschwächen der Überanpassung, wie das\n",
    "**Dropout-Verfahren** oder die **L2-Regularisierung**. \n",
    "\n",
    "Nun werden wir ein weiteres für das maschinelle Sehen geeignetes Verfahren einsetzen, das fast überall bei der\n",
    "Bildverarbeitung mit Deep-Learning-Modellen verwendet wird: **die Datenaugmentation (data augmentation)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "- Die Überanpassung wird dadurch verursacht, dass zu wenige Samples verfügbar sind, sodass es nicht möglich ist, ein Modell zu trainieren, das sich gut für neue Daten verallgemeinern lässt. \n",
    "- Wären unbegrenzt Daten verfügbar, würde Ihr Modell sämtliche Aspekte der vorliegenden Datenverteilung kennen: Es würde nie zu einer Überanpassung kommen. \n",
    "- Die **Datenaugmentation** verfolgt den Ansatz, anhand der vorhandenen Trainingssamples zusätzliche Daten zu erstellen, indem die Datensammlung durch neue Samples erweitert wird, die durch eine Reihe **zufälliger Transformationen** erzeugt werden, die glaubwürdig aussehende Bilder liefern. \n",
    "- Das Ziel ist hier, dass Ihr Modell beim Training niemals zwei völlig identische Bilder zu sehen bekommt. Das erleichtert es dem Modell, weitere Aspekte der Daten zu erkennen, und verbessert die Verallgemeinerungsfähigkeit.\n",
    "\n",
    "In Keras kann dieses Ziel durch die Konfiguration verschiedener zufälliger Transformationen erreicht werden, die auf die von der `ImageDataGenerator`-Instanz eingelesenen Bilder angewendet werden. Betrachten wir zunächst einmal ein Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations with data augmentation for training set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),  # Resize images to 150x150\n",
    "    transforms.RandomRotation(40),  # Random rotation within 40 degrees\n",
    "    transforms.RandomAffine(degrees=0, shear=0.2),  # Shear transformation\n",
    "    transforms.RandomResizedCrop(150, scale=(0.8, 1.2)),  # Zoom effect\n",
    "    transforms.RandomHorizontalFlip(),  # Horizontal flipping\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),  # Width & height shift\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize\n",
    "])\n",
    "\n",
    "# Transformations for validation set (no augmentation)\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "validation_dataset = datasets.ImageFolder(root=validation_dir, transform=validation_transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Erklärung der Augmentationen**\n",
    "\n",
    "| Augmentation | Beschreibung |\n",
    "|-------------|-------------|\n",
    "| `transforms.RandomRotation(40)` | Dreht das Bild zufällig um bis zu ±40 Grad. |\n",
    "| `transforms.RandomAffine(degrees=0, shear=0.2)` | Wendet eine Schertransformation an. |\n",
    "| `transforms.RandomResizedCrop(150, scale=(0.8, 1.2))` | Wendet einen Zoom-Effekt an. |\n",
    "| `transforms.RandomHorizontalFlip()` | Spiegelt das Bild zufällig horizontal. |\n",
    "| `transforms.RandomAffine(degrees=0, translate=(0.2, 0.2))` | Verschiebt das Bild zufällig um 20 % der Bildgröße. |\n",
    "| `transforms.ToTensor()` | Konvertiert Bilder in Tensoren. |\n",
    "| `transforms.Normalize((0.5,), (0.5,))` | Normalisiert die Pixelwerte. |\n",
    "\n",
    "Diese Konfiguration ahmt `ImageDataGenerator` in PyTorch nach und stellt eine robuste Datenaugmentation für das Training sicher. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the same data augmentation transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomAffine(degrees=0, shear=0.2),\n",
    "    transforms.RandomResizedCrop(150, scale=(0.8, 1.2)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Get a sample image path\n",
    "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
    "img_path = fnames[1]  # Select one image\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img = img.resize((150, 150))\n",
    "\n",
    "# Convert image to tensor\n",
    "img_tensor = transforms.ToTensor()(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)  # Reshape to (1, C, H, W)\n",
    "\n",
    "# Generate augmented images\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "i = 0\n",
    "for _ in range(4):\n",
    "    augmented_img = transform(img)\n",
    "    axes[i].imshow(transforms.ToPILImage()(augmented_img))\n",
    "    axes[i].axis(\"off\")\n",
    "    i += 1\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wenn Sie ein neues NN mit dieser Konfiguration der Datenaugmentation trainieren,\n",
    "wird es niemals zwei identische Bilder zu sehen bekommen. \n",
    "- Die Bilder, die es sieht, sind dessen ungeachtet eng miteinander verwoben, denn sie wurden ja anhand nur weniger ursprünglicher Bilder erstellt. Sie können auf diese Weise keine neuen Informationen erzeugen, sondern lediglich die vorhandenen Informationen anders miteinander verknüpfen. \n",
    "- Daher reicht dieses Verfahren womöglich nicht aus, um die Überanpassung komplett loszuwerden. \n",
    "- Um die Überanpassung weiter abzuschwächen, fügen wir dem Modell unmittelbar vor dem vollständig verbundenen Klassifizierer einen **`Dropout`-Layer** hinzu.\n",
    "\n",
    "**NB: Die Validierungsdaten sollen nicht transformiert werden!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "logger = CSVLogger(\"logs\", name=\"cnn_model2\")\n",
    "model2 = CNNModel()\n",
    "trainer = pl.Trainer(max_epochs=30, accelerator=\"auto\", logger=logger)\n",
    "trainer.fit(model2, train_loader, validation_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Und nun trainieren wird das CNN mit Datenaugmentation und Dropout-Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Speichern Sie das Modell – Sie werden es in Abschnitt 5.4 erneut verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"cats_and_dogs_small_2.pth\")\n",
    "#model2 = CNNModel()  # Initialize the model\n",
    "#model2.load_state_dict(torch.load(\"cats_and_dogs_small_1.pth\"))\n",
    "#model2.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training logs and plot\n",
    "log_path = \"logs/cnn_model2/version_1/metrics.csv\"\n",
    "dg = pd.read_csv(log_path)\n",
    "dg.head()\n",
    "dg=dg.groupby('epoch').mean()\n",
    "plot_history(dg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wir geben wieder die Diagramme aus (siehe Abbildung 5.12 und Abbildung 5.13).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dank *Datenaugmentation und Dropout-Verfahren* kommt es nicht mehr zu einer *Überanpassung*: Die Kurven beim Training und bei der Validierung verlaufen sehr ähnlich. Sie erzielen jetzt eine Korrektklassifizierungsrate von 82%, eine (relative)\n",
    "Verbesserung um 15% im Vergleich zum Modell ohne Regularisierung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Durch den Einsatz weiterer *Regularisierungsverfahren* und die Abstimmung der Parameter des NNs (etwa die Anzahl der Filter pro Layer oder die Anzahl der Layer im NN) kann sogar eine noch höhere Korrektklassifizierungsrate von bis zu 87% erreicht werden. \n",
    "- Es dürfte sich jedoch als schwierig erweisen, allein durch das Training des CNNs von Grund auf eine höhere Korrektklassifizierungsrate zu erzielen, weil nur so wenige Daten verfügbar sind. \n",
    "- Der nächste Schritt zur Verbesserung der Korrektklassifizierungsrate bei dieser Aufgabe ist der Einsatz eines vortrainierten Modells, auf das sich die beiden folgenden Abschnitte konzentrieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where's the intelligence?\n",
    "\n",
    "- Was emuliert die Data Augmentation?\n",
    "- Diskutieren Sie mit Ihren Peers, welche Regularisierungsverfahren für CNN angewendet werden können?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "ger",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ger",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
