{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> ¬© Christoph W√ºrsch </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f√ºr Ausf√ºhrung auf Google Colab auskommentieren und installieren\n",
    "%pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter-Tuning mit PyTorch Lightning und Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Import der ben√∂tigten Bibliotheken\n",
    "In diesem Schritt bereiten wir unser Arbeitsumfeld vor, indem wir die ben√∂tigten Bibliotheken importieren und den **FashionMNIST-Datensatz** laden. \n",
    "FashionMNIST ist ein Standard-Bildklassifizierungsdatensatz mit Graustufenbildern von Modeartikeln (Schuhe, Hosen, T-Shirts etc.), die in **10 verschiedene Klassen** unterteilt sind. \n",
    "Die **LightningDataModule**-Struktur hilft uns sp√§ter, die Daten einfacher zu verwalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "#!pip install optuna\n",
    "#!pip install optuna-integration[pytorch_lightning]\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allgemeine Einstellungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENT_VALID_EXAMPLES = (\n",
    "    0.1  # 10 % der Trainingsdaten werden f√ºr die Validierung genutzt\n",
    ")\n",
    "BATCHSIZE = 128  # Anzahl der Samples pro Batch\n",
    "CLASSES = 10  # 10 Klassen f√ºr FashionMNIST\n",
    "EPOCHS = 10  # Maximale Anzahl der Trainings-Epochen\n",
    "TRIALS = 10  # Anzahl der Versuche f√ºr Optuna\n",
    "DIR = os.getcwd()  # Arbeitsverzeichnis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Definition des neuronalen Netzwerks\n",
    "Hier definieren wir unser **neuronales Netzwerk (MLP-Modell)**. Es besteht aus:\n",
    "- Einer **Flatten-Schicht**, die das 28√ó28-Bild in einen Vektor umwandelt.\n",
    "- Einer **voll verbundenen (Dense) Schicht** mit einer variablen Anzahl an Neuronen.\n",
    "- Einer **ReLU-Aktivierungsfunktion**, um Nichtlinearit√§t hinzuzuf√ºgen.\n",
    "- Einer **Dropout-Schicht**, um √úberanpassung (Overfitting) zu reduzieren.\n",
    "- Einer **voll verbundenen Ausgabe-Schicht**, die das Bild in eine der **10 Klassen** klassifiziert.\n",
    "\n",
    "Die Anzahl der **Neuronen** in der ersten Schicht und die **Dropout-Rate** sind Hyperparameter, die sp√§ter optimiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout: float, hidden_units: int) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()  # Wandelt das 28x28 Bild in einen Vektor um\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_units)  # Erste vollverbundene Schicht\n",
    "        self.relu = nn.ReLU()  # Aktivierungsfunktion\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout-Schicht f√ºr Regularisierung\n",
    "        self.fc2 = nn.Linear(hidden_units, CLASSES)  # Ausgabe-Schicht mit 10 Klassen\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(data)  # Bild in Vektor umwandeln\n",
    "        x = self.fc1(x)  # Durch die erste vollverbundene Schicht\n",
    "        x = self.relu(x)  # Aktivierungsfunktion anwenden\n",
    "        x = self.dropout(x)  # Dropout f√ºr Regularisierung\n",
    "        x = self.fc2(x)  # Durch die Ausgabe-Schicht\n",
    "        return F.log_softmax(x, dim=1)  # Log-Softmax f√ºr Klassifikation anwenden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Definition des Lightning-Moduls\n",
    "Ein **LightningModule** kapselt das Training, die Validierung und das Testen eines Modells. \n",
    "- `training_step()` berechnet den Verlust w√§hrend des Trainings.\n",
    "- `validation_step()` berechnet die Genauigkeit auf den Validierungsdaten.\n",
    "- `configure_optimizers()` gibt den Optimierer (Adam) zur√ºck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningNet(pl.LightningModule):\n",
    "    def __init__(self, dropout: float, hidden_units: int, learning_rate: float) -> None:\n",
    "        super().__init__()\n",
    "        self.model = Net(\n",
    "            dropout, hidden_units\n",
    "        )  # Initialisierung des neuronalen Netzwerks\n",
    "        self.learning_rate = learning_rate  # Lernrate speichern\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(data)  # Vorw√§rtsdurchlauf des Modells\n",
    "\n",
    "    def training_step(self, batch: List[torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        data, target = batch  # Eingabedaten und Zielwerte aus dem Batch extrahieren\n",
    "        output = self(data)  # Modellvorhersage\n",
    "        loss = F.nll_loss(output, target)  # Berechnung des Verlusts\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)  # Verlust im Trainingslog speichern\n",
    "        return loss  # Verlust zur√ºckgeben\n",
    "\n",
    "    def validation_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n",
    "        data, target = batch  # Eingabedaten und Zielwerte aus dem Batch extrahieren\n",
    "        output = self(data)  # Modellvorhersage\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # Vorhersagen in Klassen umwandeln\n",
    "        accuracy = pred.eq(target.view_as(pred)).float().mean()  # Genauigkeit berechnen\n",
    "        self.log(\n",
    "            \"val_acc\", accuracy, prog_bar=True\n",
    "        )  # Genauigkeit im Validierungslog speichern\n",
    "        self.log(\n",
    "            \"hp_metric\", accuracy, on_step=False, on_epoch=True, prog_bar=True\n",
    "        )  # Genauigkeit als Hyperparameter-Metrik speichern\n",
    "\n",
    "    def configure_optimizers(self) -> optim.Optimizer:\n",
    "        return optim.Adam(\n",
    "            self.model.parameters(), lr=self.learning_rate\n",
    "        )  # Optimierer konfigurieren\n",
    "\n",
    "    def test_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n",
    "        data, target = batch  # Eingabedaten und Zielwerte aus dem Batch extrahieren\n",
    "        output = self(data)  # Modellvorhersage\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # Vorhersagen in Klassen umwandeln\n",
    "        accuracy = pred.eq(target.view_as(pred)).float().mean()  # Genauigkeit berechnen\n",
    "        self.log(\n",
    "            \"test_acc\", accuracy, prog_bar=True\n",
    "        )  # Genauigkeit im Testlog speichern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) DataLoader f√ºr FashionMNIST\n",
    "Hier implementieren wir eine Klasse, um den FashionMNIST-Datensatz effizient zu verwalten. \n",
    "- **Trainings-, Validierungs- und Test-Sets** werden erstellt.\n",
    "- **Dataloader** versorgen das Modell mit den richtigen Batch-Gr√∂ssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir  # Verzeichnis f√ºr die Daten\n",
    "        self.batch_size = batch_size  # Batch-Gr√∂sse\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        # Laden des Test-Datensatzes\n",
    "        self.mnist_test = datasets.FashionMNIST(\n",
    "            self.data_dir, train=False, download=True, transform=transforms.ToTensor()\n",
    "        )\n",
    "        # Laden des vollst√§ndigen Trainings-Datensatzes\n",
    "        mnist_full = datasets.FashionMNIST(\n",
    "            self.data_dir, train=True, download=True, transform=transforms.ToTensor()\n",
    "        )\n",
    "        # Aufteilen des Trainings-Datensatzes in Trainings- und Validierungs-Datensatz\n",
    "        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        # DataLoader f√ºr den Trainings-Datensatz\n",
    "        return DataLoader(\n",
    "            self.mnist_train, batch_size=self.batch_size, shuffle=True, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        # DataLoader f√ºr den Validierungs-Datensatz\n",
    "        return DataLoader(\n",
    "            self.mnist_val, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        # DataLoader f√ºr den Test-Datensatz\n",
    "        return DataLoader(\n",
    "            self.mnist_test, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Definition der Optuna Optimierung\n",
    "Hier definieren wir eine `objective`-Funktion f√ºr Optuna, die verschiedene Kombinationen von Hyperparametern testet, um das Modell zu optimieren. \n",
    "Optuna variiert **Dropout, Anzahl der Neuronen und die Lernrate**, um die beste Leistung zu finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    # Vorschlagen der Anzahl der Neuronen in der versteckten Schicht\n",
    "    hidden_units = trial.suggest_int(\"units\", 32, 128, step=32)\n",
    "    # Vorschlagen der Dropout-Rate\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "    # Vorschlagen der Lernrate\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    # Initialisieren des Modells mit den vorgeschlagenen Hyperparametern\n",
    "    model = LightningNet(dropout, hidden_units, learning_rate)\n",
    "    # Initialisieren des DataModules\n",
    "    datamodule = FashionMNISTDataModule(data_dir=DIR, batch_size=BATCHSIZE)\n",
    "\n",
    "    # Initialisieren des Trainers\n",
    "    trainer = pl.Trainer(\n",
    "        logger=True,  # Logger aktivieren\n",
    "        limit_val_batches=PERCENT_VALID_EXAMPLES,  # Begrenzen der Validierungs-Batches\n",
    "        enable_checkpointing=False,  # Checkpointing deaktivieren\n",
    "        max_epochs=EPOCHS,  # Maximale Anzahl der Epochen\n",
    "        accelerator=\"auto\",  # Automatische Auswahl der Hardware (CPU/GPU)\n",
    "        devices=[0],  # Verwenden des ersten Ger√§ts (z.B. GPU 0)\n",
    "        callbacks=[\n",
    "            # Callback f√ºr das Pruning (fr√ºhes Beenden) basierend auf der Validierungsgenauigkeit\n",
    "            PyTorchLightningPruningCallback(trial, monitor=\"val_acc\"),\n",
    "            # Callback f√ºr das fr√ºhe Stoppen basierend auf der Validierungsgenauigkeit\n",
    "            pl.callbacks.EarlyStopping(monitor=\"val_acc\", patience=5),\n",
    "        ],\n",
    "    )\n",
    "    # Training des Modells\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "    # R√ºckgabe der besten Validierungsgenauigkeit\n",
    "    return trainer.callback_metrics.get(\"val_acc\", torch.tensor(0)).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Hyperparametersuche ausf√ºhren\n",
    "Jetzt starten wir die Hyperparameter-Optimierung mit **Optuna**. Optuna testet mehrere Kombinationen von Hyperparametern und bestimmt automatisch die beste Konfiguration f√ºr unser Modell.\n",
    "- `study = optuna.create_study(direction=\"maximize\")`: Erstellt eine Optuna-Studie zur Optimierung der **Validierungsgenauigkeit**.\n",
    "- `study.optimize(objective, n_trials=TRIALS)`: F√ºhrt `TRIALS` viele Experimente durch, um die beste Hyperparameter-Kombination zu finden.\n",
    "- `study.best_params`: Gibt die besten gefundenen Werte f√ºr die Hyperparameter aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen eines Optuna-Studiums zur Hyperparameter-Optimierung\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "# Durchf√ºhren der Versuche und Ausgabe des aktuellen Versuchs\n",
    "for i, trial in enumerate(study.trials, start=1):\n",
    "    print(f\"Running trial {i}/{TRIALS}\")\n",
    "# Optimieren des Modells mit der definierten Zielsetzung\n",
    "study.optimize(objective, n_trials=TRIALS, timeout=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) Training mit den optimalen Hyperparametern\n",
    "Nachdem Optuna die besten Hyperparameter gefunden hat, trainieren wir das Modell erneut mit diesen Werten.\n",
    "- Die besten Hyperparameter werden aus `study.best_trial.params` extrahiert.\n",
    "- Das Modell wird mit diesen optimalen Parametern initialisiert.\n",
    "- Ein neuer `Trainer` trainiert das Modell erneut mit den besten Hyperparametern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrufen der besten Hyperparameter\n",
    "best_hps = study.best_trial.params\n",
    "print(\"Best hyperparameters:\")\n",
    "# Ausgabe der besten Hyperparameter\n",
    "for key, value in best_hps.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Initialisieren des Modells mit den besten Hyperparametern\n",
    "best_model = LightningNet(\n",
    "    dropout=best_hps[\"dropout\"],\n",
    "    hidden_units=best_hps[\"units\"],\n",
    "    learning_rate=best_hps[\"learning_rate\"],\n",
    ")\n",
    "# Initialisieren des DataModules\n",
    "datamodule = FashionMNISTDataModule(data_dir=DIR, batch_size=BATCHSIZE)\n",
    "# Initialisieren des Trainers\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, accelerator=\"auto\", devices=[0])\n",
    "# Training des Modells mit den besten Hyperparametern\n",
    "trainer.fit(best_model, datamodule=datamodule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h) Evaluieren Sie das Modell auf den Testdaten\n",
    "Der letzte Schritt besteht darin, unser trainiertes Modell auf den Testdaten zu evaluieren, um zu √ºberpr√ºfen, wie gut es generalisiert.\n",
    "- `trainer.test()` wertet das Modell auf dem Testdatensatz aus.\n",
    "- `print(\"Test results:\", test_results)` gibt die Testergebnisse aus, einschlie√ülich der finalen Genauigkeit des Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe des besten Modells\n",
    "print(best_model)\n",
    "# Testen des Modells und Ausgabe der Testergebnisse\n",
    "test_results = trainer.test(best_model, datamodule=datamodule)\n",
    "print(\"Test results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "Mit dieser √úbung haben Sie gelernt, wie man **PyTorch Lightning** und **Optuna** kombiniert, um ein neuronales Netz f√ºr Bildklassifikation zu optimieren. Durch **automatisches Hyperparameter-Tuning** k√∂nnen Sie die Leistung Ihres Modells erheblich verbessern. üéØüöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
