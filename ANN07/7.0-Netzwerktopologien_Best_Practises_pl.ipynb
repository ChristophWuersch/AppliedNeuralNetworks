{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN07/7.0-Netzwerktopologien_Best_Practises_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Ausführung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bewährte Verfahren des Deep Learnings (PyTorch Lightning Edition)\n",
    "\n",
    "Diese Notebooks zeigen die in Kapitel 6, Abschnitt 3 von [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff) gezeigten Konzepte, angepasst für PyTorch Lightning.\n",
    "# \n",
    "Die Themen:\n",
    "- Die **funktionale Lightning-API** (via `nn.Module` und `LightningModule`)\n",
    "- **Callbacks** und Lightning's `Trainer`\n",
    "- TensorBoard für Visualisierung\n",
    "- Praktiken wie *Batch Normalization*, *Residual Connections*, *Hyperparameter Tuning*, *Ensembles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Lightning Version: {L.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Lektion lernen wir eine Reihe leistungsfähiger Tools kennen, die es uns erleichtern, schwierige Aufgaben Modelle nach dem aktuellen Stand der Technik zu entwickeln. Mit der funktionalen Lightning-API können wir graphenähnliche Modelle entwickeln, einem Layer verschiedene Eingaben übergeben und Lightning-Modelle wie Python-Funktionen verwenden.\n",
    "- Lightning-Callbacks und das browserbasierte Visualisierungstool TensorBoard ermöglichen es, Modelle während des Trainings zu überwachen.\n",
    "- Darüber hinaus kommen verschiedene andere bewährte Verfahren zur Sprache, wie die Normierung von Stapeln (*batch normalization*), residuale Verbindungen (*residual connections*), Hyperparameteroptimierung (*hyperparameter tuning*) und Ensemblemodelle (*ensemble models*).\n",
    "### 7.1 Jenseits des Sequential-Modells: die funktionale Lightning-API\n",
    "Bislang wurden die meisten vorgestellten Modelle mithilfe des Sequential-Modells implementiert. Diesem Modell liegen die Annahmen zugrunde, dass das NN genau eine Eingabe erhält, genau eine Ausgabe liefert und aus einem linearen Stapel von Layern besteht.\n",
    "\n",
    "Diese Annahmen haben sich schon häufig als richtig erwiesen. Der Aufbau ist tatsächlich so gebräuchlich, dass wir in der Lage waren, eine Vielzahl verschiedener Themen und praktischer Anwendungen zu erörtern, und dabei bisher ausschließlich die Sequential-Klasse verwendet haben. Allerdings sind diese Annahmen in manchen Fällen nicht flexibel genug. Bei einigen NNs sind mehrere Eingaben erforderlich, andere benötigen mehrere Ausgaben, und wieder andere besitzen interne Verzweigungen zwischen den Layern und sehen deshalb nicht mehr wie ein linearer Stapel, sondern wie ein Graph von Layern aus.\n",
    "### Modelle mit multimodalen Eingängen\n",
    "\n",
    "Bei manchen Aufgaben sind beispielsweise multimodale Eingaben erforderlich: Daten aus mehreren Eingabequellen werden zusammengeführt, wobei unterschiedliche Layer die verschiedenen Datentypen verarbeiten. \n",
    "\n",
    "Stellen Sie sich ein Deep-Learning-Modell vor, das versucht, den wahrscheinlichsten Preis eines Kleidungsstücks aus zweiter Hand vorherzusagen, und zu diesem Zweck die folgenden Eingaben verwendet: \n",
    "- vom Benutzer bereitgestellte *Metadaten* (wie Marke, Alter, Grösse usw.), \n",
    "- eine vom Benutzer bereitgestellte *Beschreibung in Textform* sowie \n",
    "- ein *Foto* des Kleidungsstücks.\n",
    "\n",
    "Wenn nur die Metadaten verfügbar wären, könnten Sie die *One-hot-Codierung* verwenden und mit einem Fully-connected NN den Preis vorhersagen. Wenn nur die Beschreibung in Textform vorläge, könnten Sie ein RNN oder ein 1-D-CNN verwenden. Und wenn Sie nur das Foto hätten, könnten Sie ein 2-D-CNN verwenden. \n",
    "\n",
    "Aber wie kann man alles gleichzeitig nutzen?\n",
    "Ein naiver Ansatz wäre es, drei verschiedene Modelle zu trainieren und anschliessend einen gewichteten Mittelwert der Vorhersagen zu berechnen. Diese Vorgehensweise könnte jedoch suboptimal sein, weil die von den Modellen extrahierten Informationen womöglich *redundant* sind. Besser ist es, dem Modell alle drei Eingaben gleichzeitig bereitzustellen, sodass durch das *Zusammenwirken der verschiedenen Eingaben* ein präziseres Modell der Daten erlernt werden kann. Auf diese Weise entsteht ein Modell mit drei Eingabezweigen.\n",
    "\n",
    "<img src=\"Bilder/MultiInputModell.png\" width=\"640\"  align=\"center\"/>\n",
    "\n",
    "In einigen Fällen kann es auch erforderlich sein, mehrere **Zielattribute** anhand der Eingabedaten vorherzusagen. \n",
    "- Vielleicht möchten Sie den Text eines Romans oder einer Kurzgeschichte automatisch nach Genre (wie z.B. Romanze oder Thriller) klassifizieren, aber gleichzeitig auch den ungefähren Zeitpunkt vorhersagen, zu  dem der Text geschrieben wurde. Natürlich könnten Sie zwei verschiedene Modelle trainieren: eins für das Genre und ein zweites für den Zeitpunkt, aber da diese beiden Attribute statistisch nicht voneinander unabhängig sind, können Sie ein besseres Modell entwickeln, das erlernt, Genre und Zeitpunkt gleichzeitig vorherzusagen.\n",
    "\n",
    "Ein solches Modell würde **zwei Ausgaben** liefern. Aufgrund der Korrelationen zwischen Genre und Zeitpunkt würde die Kenntnis des Zeitpunkts der Entstehung eines Romans dem Modell dabei helfen, ergiebige und genaue Repräsentationen im Raum der Romangenres zu erlernen. Umgekehrt wäre auch die Kenntnis des Genres zur Vorhersage des Zeitpunkts nützlich.\n",
    "\n",
    "<img src=\"Bilder/MultiOutputModell.png\" width=\"640\"  align=\"center\"/>\n",
    "\n",
    "### Nichtlineare Netztopologien (DAG)\n",
    "\n",
    "Darüber hinaus benötigen viele der in jüngster Zeit entwickelten Architekturen **nicht-lineare Netztopologien**: \n",
    "- Netze, die die Form eines gerichteten *azyklischen Graphen* besitzen. \n",
    "- Die von *Szegedy et al.*[1] bei Google entwickelte Inceptions-Familie neuronaler Netze beispielsweise beruht auf Inception-Modulen, bei denen Eingaben von mehreren parallel verlaufenden Faltungszweigen verarbeitet werden, deren Ausgaben wieder zu einem einzelnen Tensor zusammengeführt werden.\n",
    "\n",
    "<img src=\"Bilder/InceptionModule.png\" width=\"640\"  align=\"center\"/>\n",
    "\n",
    "[1] [Christian Szegedy et al., Going Deeper with Convolutions, Conference on Computer Vision and\n",
    "Pattern Recognition (2014)](https://arxiv.org/abs/1409.4842).\n",
    "\n",
    "### Residual Connections\n",
    "\n",
    "Und dann gibt es noch den Trend, Modellen **residuale Verbindungen** hinzuzufügen, der mit der von *He et al.*[2] bei Microsoft entwickelten **ResNet-Familie** neuronaler Netze einsetzte. Bei einer residualen Verbindung werden frühere Repräsentationen dem nachfolgenden Datenstrom wieder hinzugefügt, indem ein älterer Ausgabetensor zu einem jüngeren Ausgabetensor hinzuaddiert wird. Dadurch lässt sich ein Informationsverlust während des Ablaufs der Datenverarbeitung verhindern. Für diese graphenähnlichen Netze gibt es viele weitere Beispiele.\n",
    "\n",
    "<img src=\"Bilder/ResidualModule.png\" width=\"440\"  align=\"center\"/>\n",
    "\n",
    "\n",
    "[2] [Kaiming He et al., Deep Residual Learning for Image Recognition, Conference on Computer\n",
    "Vision and Pattern Recognition (2015)](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "### Lightning functional API\n",
    "Diese drei wichtigen Anwendungsfälle (Modelle mit mehreren Eingaben, Modelle mit mehreren Ausgaben sowie graphenähnliche Modelle) sind in Lightning nicht realisierbar, wenn nur die Sequential-Klasse verwendet wird.\n",
    "\n",
    "Es gibt jedoch eine weitere, sehr viel allgemeinere und flexiblere Möglichkeit, Lightning zu nutzen: die funktionale API. Der folgende Abschnitt erklärt, was genau das ist, was sie leistet und wie man sie verwendet.\n",
    "\n",
    "Mit der funktionalen API können Sie Tensoren direkt bearbeiten und Layer wie Funktionen verwenden, die Tensoren entgegennehmen und zurückliefern (daher auch die Bezeichnung funktionale API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Der Input ist Tensor\n",
    "input_tensor = torch.randn(1, 32)\n",
    "# Ein Layer ist eine Funktion.\n",
    "dense = nn.Linear(32, 32)\n",
    "# Ein Layer kann mit einem Tensor aufgerufen werden und gibt einen Tensor zurück\n",
    "output_tensor = torch.relu(dense(input_tensor))\n",
    "\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir definieren ein einfaches Modell mit drei linearen Schichten als ``LightningModule`` und verschaffen uns mit torchsummary einen Überblick über die Architektur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "from torchsummary import summary\n",
    "\n",
    "# Definiere ein einfaches Modell als LightningModule\n",
    "class SimpleModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        # Definiere die Layer des Modells\n",
    "        self.layer1 = nn.Linear(64, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, 10)\n",
    "        # Listen zur Speicherung der Trainingsverluste\n",
    "        self.train_losses = []\n",
    "        self._epoch_losses = []  # Temporäre Speicherung pro Batch\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Definiere den Vorwärtsdurchlauf\n",
    "        x = F.relu(self.layer1(x))  # Aktivierungsfunktion ReLU nach dem ersten Layer\n",
    "        x = F.relu(self.layer2(x))  # Aktivierungsfunktion ReLU nach dem zweiten Layer\n",
    "        x = self.layer3(x)  # Ausgabe des dritten Layers\n",
    "        return x\n",
    "\n",
    "# Überprüfe, ob eine GPU verfügbar ist, und verwende sie, falls möglich\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialisiere das Modell und verschiebe es auf das entsprechende Gerät (CPU oder GPU)\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Zeige eine Zusammenfassung des Modells an, wobei die Eingabegröße (64,) ist\n",
    "summary(model, (64,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Einzige, was an dieser Stelle ein wenig wie Zauberei wirkt, ist die **Instanziierung eines Model-Objekts allein durch Übergabe eines Eingabe- und eines Ausgabetensors**.\n",
    "Hinter den Kulissen ruft Lightning sämtliche Layer ab, die daran beteiligt\n",
    "sind, aus dem Eingabetensor `input_tensor` den Ausgabetensor `output_tensor`\n",
    "zu berechnen, und fasst sie in einer graphenähnlichen Datenstruktur zusammen – einem Modell. \n",
    "\n",
    "Das Ganze funktioniert natürlich nur, weil `output_tensor` durch wiederholte Transformation von `input_tensor` entstanden ist. Wenn wir versuchen, ein Modell mit Ein- und Ausgaben zu erzeugen, die nicht zusammengehören, löst das einen Laufzeitfehler aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BadModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(BadModel, self).__init__()\n",
    "        # Definiere die Layer des Modells\n",
    "        self.layer1 = nn.Linear(64, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Definiere den Vorwärtsdurchlauf mit ReLU-Aktivierungen\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# Initialisiere das Modell\n",
    "bad_model = BadModel()\n",
    "\n",
    "# Definieren Sie den unrelated_input Tensor\n",
    "# Dieser Tensor hat eine falsche Eingabegröße, um einen Fehler zu verursachen\n",
    "unrelated_input = torch.randn(1, 32)\n",
    "\n",
    "# Versuch, ein Modell mit nicht zusammengehörenden Eingabe- und Ausgabetensoren zu erstellen\n",
    "try:\n",
    "    # Dies wird einen Laufzeitfehler verursachen, da die Eingabegröße nicht mit der erwarteten Größe übereinstimmt\n",
    "    output = bad_model(unrelated_input)\n",
    "except RuntimeError as e:\n",
    "    # Fange den Laufzeitfehler ab und drucke die Fehlermeldung\n",
    "    print(f\"Laufzeitfehler: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Fehlermeldung weist darauf hin, dass es PyTorch Lightning nicht gelungen ist, den Ausgabetensor des Modells korrekt auf den erwarteten Eingabetensor input_1 abzubilden. Dies tritt typischerweise auf, wenn die Shapes oder die Datenformate der Ein- und Ausgaben nicht zueinander passen oder nicht korrekt transformiert wurden.\n",
    "\n",
    "In diesem Fall verwenden wir ein benutzerdefiniertes Modell, das auf der LightningModule-Klasse basiert. Dabei definieren wir explizit den Vorwärtsdurchlauf, die Verlustberechnung und die Optimierung. Es liegt also in unserer Verantwortung, sicherzustellen, dass die Ein- und Ausgabetensoren korrekt verarbeitet und miteinander kompatibel sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definiere die Trainingsschritt-Funktion\n",
    "def training_step(self, batch, batch_idx):\n",
    "    x, y = batch  # Entpacke den Batch in Eingaben (x) und Zielwerte (y)\n",
    "    y_hat = self(x)  # Berechne die Vorhersagen des Modells\n",
    "    loss = F.cross_entropy(y_hat, y)  # Berechne den Cross-Entropy-Verlust\n",
    "    self.log(\"train_loss\", loss, prog_bar=True)  # Logge den Verlust für die Fortschrittsanzeige\n",
    "    self._epoch_losses.append(loss.item())  # Füge den Verlust zur Liste der Verluste pro Epoche hinzu\n",
    "    return loss  # Gib den Verlust zurück\n",
    "\n",
    "# Definiere die Funktion, die am Ende jeder Epoche aufgerufen wird\n",
    "def on_train_epoch_end(self):\n",
    "    if self._epoch_losses:  # Überprüfe, ob die Liste der Verluste pro Epoche nicht leer ist\n",
    "        avg_loss = sum(self._epoch_losses) / len(self._epoch_losses)  # Berechne den durchschnittlichen Verlust\n",
    "        self.train_losses.append(avg_loss)  # Füge den durchschnittlichen Verlust zur Liste der Trainingsverluste hinzu\n",
    "        self._epoch_losses.clear()  # Leere die Liste der Verluste pro Epoche für die nächste Epoche\n",
    "\n",
    "# Definiere die Optimierer-Konfigurationsfunktion\n",
    "def configure_optimizers(self):\n",
    "    return torch.optim.RMSprop(self.parameters(), lr=0.01)  # Verwende RMSprop als Optimierer mit einer Lernrate von 0.01\n",
    "\n",
    "# Füge die definierten Methoden zur SimpleModel-Klasse hinzu\n",
    "SimpleModel.training_step = training_step\n",
    "SimpleModel.on_train_epoch_end = on_train_epoch_end\n",
    "SimpleModel.configure_optimizers = configure_optimizers\n",
    "\n",
    "# Dummy-Daten generieren und normalisieren\n",
    "x_train = np.random.random((1000, 64)).astype(np.float32)  # Generiere zufällige Trainingsdaten\n",
    "y_train = np.random.randint(0, 10, (1000,)).astype(np.int64)  # Generiere zufällige Zielwerte\n",
    "x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)  # Normalisiere die Trainingsdaten\n",
    "\n",
    "# In Tensoren umwandeln\n",
    "x_train_tensor = torch.tensor(x_train)  # Konvertiere die Trainingsdaten in einen Tensor\n",
    "y_train_tensor = torch.tensor(y_train)  # Konvertiere die Zielwerte in einen Tensor\n",
    "\n",
    "# DataLoader erstellen\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)  # Erstelle ein Dataset aus den Tensoren\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)  # Erstelle einen DataLoader mit einer Batch-Größe von 32\n",
    "\n",
    "# Modell und Trainer initialisieren\n",
    "model = SimpleModel()  # Initialisiere das Modell\n",
    "trainer = L.Trainer(max_epochs=30, logger=False, enable_checkpointing=False)  # Initialisiere den Trainer mit 30 Epochen\n",
    "trainer.fit(model, train_loader)  # Trainiere das Modell mit dem DataLoader\n",
    "\n",
    "# Lernkurve plotten\n",
    "plt.plot(model.train_losses, marker=\"o\")  # Plotte die Trainingsverluste\n",
    "plt.xlabel(\"Epoche\")  # Beschrifte die x-Achse\n",
    "plt.ylabel(\"Cross Entropy Loss\")  # Beschrifte die y-Achse\n",
    "plt.title(\"Lernkurve\")  # Setze den Titel des Plots\n",
    "plt.grid(True)  # Zeige ein Gitter im Plot an\n",
    "plt.show()  # Zeige den Plot an\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Modelle mit mehreren Eingaben\n",
    "\n",
    "Mit der funktionalen API können Modelle mit mehreren Eingaben erstellt werden. Typischerweise führen Modelle dieser Art die verschiedenen Eingabezweige mit einem Layer zusammen, der mehrere Tensoren miteinander kombiniert: durch **Addition, durch Verkettung usw**.\n",
    "\n",
    "Für gewöhnlich wird diese Aufgabe mittels PyTorch-Operationen wie `torch.add` und `torch.cat` erledigt. Betrachten wir ein besonders einfaches Beispiel für ein Modell mit mehreren Eingaben: ein Modell zum Beantworten von Fragen.\n",
    "\n",
    "Ein solches Modell besitzt typischerweise zwei Eingaben: eine in natürlicher Sprache formulierte Frage und einen Textabschnitt (z.B. einen Zeitungsartikel), der die Informationen zum Beantworten der Frage bereitstellt. Das Modell muss nun eine Antwort geben. Im einfachsten Fall besteht die Antwort aus einem einzelnen Wort, das via softmax-Funktion anhand eines vorgegebenen Vokabulars ermittelt wird.\n",
    "\n",
    "<img src=\"Bilder/ChatBot.png\" width=\"440\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das folgende Beispiel zeigt, wie Sie mit der funktionalen API ein solches Modell\n",
    "erstellen können. Sie richten zwei voneinander unabhängige Zweige ein, codieren\n",
    "den Text der möglichen Antworten und die Frage als Repräsentationsvektoren, verketten\n",
    "diese Vektoren und fügen den verketteten Repräsentationen schließlich\n",
    "einen softmax-Klassifizierer hinzu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Dummy data configuration\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500\n",
    "\n",
    "# Dummy data generation\n",
    "text = torch.randint(1, text_vocabulary_size, (num_samples, max_length))\n",
    "question = torch.randint(1, question_vocabulary_size, (num_samples, max_length))\n",
    "answers = torch.randint(0, answer_vocabulary_size, (num_samples,))\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = TensorDataset(text, question, answers)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Define the model class\n",
    "class TextQuestionModel(L.LightningModule):\n",
    "    def __init__(self, text_vocab_size, question_vocab_size, answer_vocab_size):\n",
    "        super(TextQuestionModel, self).__init__()\n",
    "        # Embedding layers for text and question inputs\n",
    "        self.text_embedding = nn.Embedding(text_vocab_size, 64)\n",
    "        self.text_lstm = nn.LSTM(64, 32, batch_first=True)\n",
    "        self.question_embedding = nn.Embedding(question_vocab_size, 32)\n",
    "        self.question_lstm = nn.LSTM(32, 16, batch_first=True)\n",
    "        # Fully connected layer to combine the outputs of the LSTMs\n",
    "        self.fc = nn.Linear(32 + 16, answer_vocab_size)\n",
    "        # Softmax activation for the output\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, text, question):\n",
    "        # Forward pass for text input\n",
    "        embedded_text = self.text_embedding(text)\n",
    "        _, (encoded_text, _) = self.text_lstm(embedded_text)\n",
    "        # Forward pass for question input\n",
    "        embedded_question = self.question_embedding(question)\n",
    "        _, (encoded_question, _) = self.question_lstm(embedded_question)\n",
    "        # Concatenate the last hidden states of the LSTMs from the two different inputs\n",
    "        concatenated = torch.cat((encoded_text[-1], encoded_question[-1]), dim=-1)\n",
    "        # Pass the concatenated tensor through the fully connected layer\n",
    "        logits = self.fc(concatenated)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Unpack the batch\n",
    "        text, question, answers = batch\n",
    "        # Forward pass\n",
    "        logits = self.forward(text, question)\n",
    "        # Compute the loss\n",
    "        loss = self.criterion(logits, answers)\n",
    "        # Compute the accuracy\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = (preds == answers).float().mean()\n",
    "        # Log the loss and accuracy\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Use RMSprop optimizer\n",
    "        return optim.RMSprop(self.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize the model\n",
    "model = TextQuestionModel(\n",
    "    text_vocabulary_size, question_vocabulary_size, answer_vocabulary_size\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = L.Trainer(max_epochs=10)\n",
    "# Train the model\n",
    "trainer.fit(model, dataloader)\n",
    "\n",
    "# Visualize and save the model architecture\n",
    "sample_text = torch.randint(1, text_vocabulary_size, (1, max_length))\n",
    "sample_question = torch.randint(1, question_vocabulary_size, (1, max_length))\n",
    "logits = model(sample_text, sample_question)\n",
    "dot = make_dot(logits, params=dict(model.named_parameters()))\n",
    "dot.graph_attr.update(dpi=\"600\")\n",
    "dot.render(\"TextQuestionModel_architecture\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='TextQuestionModel_architecture.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um dieses Modell mit zwei Eingaben zu trainieren, gibt es zwei mögliche APIs:\n",
    "\n",
    "- Sie können dem Modell eine Liste von PyTorch-Tensoren oder ein Dictionary übergeben, das den Eingabebezeichnungen PyTorch-Tensoren zuordnet. Die letztere Option ist natürlich nur verfügbar, wenn Sie die Eingaben benannt haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Modelle mit mehreren Ausgaben\n",
    "\n",
    "Auch in PyTorch Lightning lässt sich auf einfache Weise ein Modell mit mehreren Ausgaben erstellen. Als Beispiel betrachten wir ein neuronales Netz, das mehrere Eigenschaften der Daten gleichzeitig vorhersagen soll. Man könnte sich etwa ein System vorstellen, das eine Reihe von Social-Media-Beiträgen als Eingabe erhält und daraus verschiedene Merkmale einer Person schätzt, z.B. Alter, Geschlecht oder Einkommen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Dummy-Datenkonfiguration\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "vocabulary_size = 50000\n",
    "num_income_groups = 10\n",
    "\n",
    "# Dummy-Daten generieren\n",
    "posts = torch.randint(1, vocabulary_size, (num_samples, max_length))\n",
    "ages = torch.randint(0, 100, (num_samples,))\n",
    "incomes = torch.randint(0, num_income_groups, (num_samples,))\n",
    "genders = torch.randint(0, 2, (num_samples,))\n",
    "\n",
    "# Dataset und DataLoader erstellen\n",
    "dataset = TensorDataset(posts, ages, incomes, genders)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Definiere das Modell mit mehreren Ausgaben\n",
    "class MultiOutputModel(L.LightningModule):\n",
    "    def __init__(self, vocabulary_size, num_income_groups):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, 256)\n",
    "        self.conv1 = nn.Conv1d(256, 128, 3)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(128, 256, 3)\n",
    "        self.conv3 = nn.Conv1d(256, 256, 3)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.conv4 = nn.Conv1d(256, 256, 3)\n",
    "        self.conv5 = nn.Conv1d(256, 256, 3)\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.age_output = nn.Linear(128, 1)\n",
    "        self.income_output = nn.Linear(128, num_income_groups)\n",
    "        self.gender_output = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(0, 2, 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        age = self.age_output(x)\n",
    "        income = self.income_output(x)\n",
    "        gender = torch.sigmoid(self.gender_output(x))\n",
    "        return age, income, gender\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        posts, ages, incomes, genders = batch\n",
    "        age_pred, income_pred, gender_pred = self(posts)\n",
    "        age_loss = F.mse_loss(age_pred.squeeze(), ages.float())\n",
    "        income_loss = F.cross_entropy(income_pred, incomes)\n",
    "        gender_loss = F.binary_cross_entropy(gender_pred.squeeze(), genders.float())\n",
    "        loss = age_loss + income_loss + gender_loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "# Modell und Trainer initialisieren\n",
    "model = MultiOutputModel(vocabulary_size, num_income_groups)\n",
    "trainer = L.Trainer(max_epochs=10)\n",
    "trainer.fit(model, dataloader)\n",
    "\n",
    "# Modellarchitektur visualisieren und speichern\n",
    "sample_input = torch.randint(1, vocabulary_size, (1, max_length))\n",
    "age, income, gender = model(sample_input)\n",
    "dot = make_dot((age, income, gender), params=dict(model.named_parameters()))\n",
    "dot.graph_attr.update(dpi=\"600\")\n",
    "dot.render(\"MultiOutputModel_architecture\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " An dieser Stelle ist es wichtig zu beachten, dass ein solches Modell die Moeglichkeit bieten muss, fuer die verschiedenen Ausgaben unterschiedliche Verlustfunktionen zu verwenden.\n",
    "\n",
    "- Die Vorhersage des Alters ist beispielsweise eine skalare Regression,\n",
    "- die Vorhersage des Geschlechts hingegen eine Binaerklassifikation, die eine andere Verlustfunktion erfordert.\n",
    "\n",
    "Da beim Gradientenabstiegsverfahren jedoch ein einzelner Skalar minimiert wird, muessen die verschiedenen Verlustfunktionen zu einem gemeinsamen Gesamtverlust kombiniert werden. Die einfachste Methode besteht darin, die Werte der einzelnen Verluste einfach zu summieren.\n",
    "\n",
    "In PyTorch Lightning erfolgt diese Kombination direkt in der Methode ``training_step``, wo man die jeweiligen Verluste berechnet und selbst addiert. Lightning erwartet, dass du am Ende einen Gesamtverlustwert zurueckgibst, der dann vom Framework optimiert wird.\n",
    "\n",
    "Die Funktion configure_optimizers dient in Lightning ausschliesslich dazu, Optimierer (und optional Lernraten-Scheduler) zu definieren – nicht aber zur Verwaltung oder Kombination von Verlustfunktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "# Replace 'example.png' with the path to your PNG file.\n",
    "display(Image(filename='MultiOutputModel_architecture.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Gewichtung\n",
    "Beachten Sie hier, dass sehr unausgewogene Beiträge zum Verlust dazu führen können, dass die Repräsentationen des Modells bevorzugt für die Aufgabe mit den größten Werten der Verlustfunktion optimiert werden – und zwar auf Kosten der übrigen Aufgaben.\n",
    "\n",
    "- Um das zu verhindern, können Sie die Beiträge gewichten, die die verschiedenen Werte der Verlustfunktion zum Gesamtverlust leisten.\n",
    "- Das ist besonders nützlich, wenn die Werte der unterschiedlichen Verlustfunktionen in verschiedenen Größenordnungen liegen.\n",
    "- Bei der Regression zur Altersvorhersage beispielsweise nimmt der mittlere quadratische Fehler (mse) typischerweise Werte zwischen ca. 3 und 5 an, während der Wert der für die Geschlechtsklassifikation verwendeten Kreuzentropie manchmal nur 0.1 beträgt.\n",
    "- Um ausgeglichenere Beiträge der verschiedenen Verluste zu erzielen, können Sie z.B. der Kreuzentropie eine Gewichtung von 10 und dem mittleren quadratischen Fehler eine Gewichtung von 0.25 zuweisen.\n",
    "\n",
    "In PyTorch Lightning erfolgt diese Gewichtung direkt im training_step, indem Sie die einzelnen Verluste mit entsprechenden Faktoren multiplizieren, bevor sie zum Gesamtverlust aufsummiert werden.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputModel(L.LightningModule):\n",
    "    def __init__(self, vocabulary_size, num_income_groups):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, 256)\n",
    "        self.conv1 = nn.Conv1d(256, 128, 3)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(128, 256, 3)\n",
    "        self.conv3 = nn.Conv1d(256, 256, 3)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.conv4 = nn.Conv1d(256, 256, 3)\n",
    "        self.conv5 = nn.Conv1d(256, 256, 3)\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.age_output = nn.Linear(128, 1)\n",
    "        self.income_output = nn.Linear(128, num_income_groups)\n",
    "        self.gender_output = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(0, 2, 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        age = self.age_output(x)\n",
    "        income = self.income_output(x)\n",
    "        gender = torch.sigmoid(self.gender_output(x))\n",
    "        return age, income, gender\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        posts, ages, incomes, genders = batch\n",
    "        age_pred, income_pred, gender_pred = self(posts)\n",
    "        age_loss = F.mse_loss(age_pred.squeeze(), ages.float())\n",
    "        income_loss = F.cross_entropy(income_pred, incomes)\n",
    "        gender_loss = F.binary_cross_entropy(gender_pred.squeeze(), genders.float())\n",
    "\n",
    "        # Gewichtung der Verluste\n",
    "        loss = 0.25 * age_loss + 1.0 * income_loss + 10.0 * gender_loss\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Initialisiere das Modell\n",
    "model = MultiOutputModel(vocabulary_size, num_income_groups)\n",
    "\n",
    "# Initialisiere den Trainer\n",
    "trainer = L.Trainer(max_epochs=10)\n",
    "\n",
    "# Trainiere das Modell mit dem DataLoader\n",
    "trainer.fit(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Inception Module: Gerichtete azyklische Graphen von Layern\n",
    "Mit der flexiblen Struktur von PyTorch und Lightning können Sie nicht nur Modelle mit mehreren Ein- und Ausgaben erstellen, sondern auch neuronale Netze mit komplexer interner Topologie implementieren. PyTorch erlaubt die Umsetzung von Netzen in Form von gerichteten azyklischen Graphen (DAGs).\n",
    "\n",
    "Azyklisch bedeutet, dass keine Schleifen im Datenfluss erlaubt sind – ein Tensor darf nie zur Eingabe eines Layers werden, der ihn selbst erzeugt hat. Die einzige erlaubte Form von Schleifen betrifft rekurrente Netze, bei denen spezialisierte rekurrente Layer verwendet werden.\n",
    "\n",
    "Einige bekannte Bausteine neuronaler Netze setzen auf solche Graphenstrukturen – besonders Inception-Module und residuale Verbindungen. Um zu verstehen, wie man solche flexiblen Strukturen in PyTorch Lightning modelliert, betrachten wir ein Beispiel für ein Inception-Modul.\n",
    "\n",
    "### Inception-Module: Aufbau und Idee\n",
    "Inception V3 ist eine bekannte Architektur für Convolutional Neural Networks, die 2013/2014 von Christian Szegedy und seinem Team bei Google entwickelt wurde, inspiriert durch das \"Network in Network\"-Konzept.\n",
    "\n",
    "Ein Inception-Modul besteht aus mehreren parallel verlaufenden Zweigen, die jeweils unterschiedliche Faltungsoperationen ausführen, und deren Ergebnisse anschließend verkettet werden.\n",
    "\n",
    "- Die einfachste Variante eines Inception-Moduls hat drei bis vier Zweige.\n",
    "- Typisch ist eine Kombination aus 1x1-Faltung, gefolgt von 3x3- oder 5x5-Faltungen.\n",
    "- Abschließend werden die Ergebnisse aller Zweige konkateniert.\n",
    "- So kann das Netzwerk räumliche und kanalbezogene Merkmale getrennt verarbeiten, was effizienter ist als beides gleichzeitig zu lernen.\n",
    "Komplexere Varianten integrieren zusätzlich Pooling-Schritte, verschiedene Filtergrößen (z.B. 5x5 statt 3x3), oder reine 1x1-Faltungen ohne räumliche Faltung.\n",
    "\n",
    "Die folgende Abbildung zeigt ein Beispiel für ein Inception-Modul aus Inception V3:\n",
    "\n",
    "<img src=\"Bilder/InceptionModule_V3.png\" width=\"540\"  align=\"center\"/>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## Die Aufgabe von 1x1-Faltungen in Lightning\n",
    "Sie wissen bereits, dass Faltungen räumliche Patches aus einem Eingabetensor extrahieren und auf jedes Patch dieselbe Transformation anwenden. Ein Sonderfall tritt bei 1x1-Faltungen auf: Hier besteht jedes Patch nur aus einem einzelnen Feld. In PyTorch bzw. Lightning definieren Sie solche Faltungen mit nn.Conv2d(..., kernel_size=1).\n",
    "\n",
    "Die Operation wirkt in diesem Fall wie ein Linear-Layer, der auf jedes Feld separat angewendet wird:\n",
    "\n",
    "- Es werden ausschließlich kanalbezogene Informationen verarbeitet, räumliche Informationen bleiben unberücksichtigt, da jeweils nur ein einzelnes Feld betrachtet wird.\n",
    "- In Inception-Modulen werden solche 1x1-Faltungen (auch punktweise Faltungen) gezielt eingesetzt, um das Lernen kanalbezogener Merkmale vom Lernen räumlicher Merkmale zu trennen.\n",
    "- Das ist besonders effizient, wenn man davon ausgeht, dass einzelne Kanäle räumlich stark autokorreliert sind, während zwischen verschiedenen Kanälen kaum Korrelationen bestehen.\n",
    "\n",
    "In PyTorch Lightning werden 1x1-Faltungen einfach mit nn.Conv2d realisiert und lassen sich flexibel mit anderen Faltungen oder Pooling-Layern zu Inception-Modulen kombinieren.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "\n",
    "# Definition des Inception-Moduls als LightningModule\n",
    "class InceptionModule(L.LightningModule):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Zweig A: 1x Conv2D\n",
    "        self.branch_a = nn.Conv2d(input_channels, 128, kernel_size=1, stride=2, padding=0)\n",
    "\n",
    "        # Zweig B: 2x Conv2D\n",
    "        self.branch_b1 = nn.Conv2d(input_channels, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.branch_b2 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Zweig C: AveragePooling + Conv2D\n",
    "        self.branch_c_pool = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.branch_c_conv = nn.Conv2d(input_channels, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Zweig D: 2x Conv2D\n",
    "        self.branch_d1 = nn.Conv2d(input_channels, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.branch_d2 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Vorwärtsdurchlauf durch Zweig A\n",
    "        branch_a = self.relu(self.branch_a(x))\n",
    "\n",
    "        # Vorwärtsdurchlauf durch Zweig B\n",
    "        branch_b = self.relu(self.branch_b1(x))\n",
    "        branch_b = self.relu(self.branch_b2(branch_b))\n",
    "\n",
    "        # Vorwärtsdurchlauf durch Zweig C\n",
    "        branch_c = self.branch_c_pool(x)\n",
    "        branch_c = self.relu(self.branch_c_conv(branch_c))\n",
    "\n",
    "        # Vorwärtsdurchlauf durch Zweig D\n",
    "        branch_d = self.relu(self.branch_d1(x))\n",
    "        branch_d = self.relu(self.branch_d2(branch_d))\n",
    "\n",
    "        # Konkatenieren entlang der Kanalachse (dim=1)\n",
    "        output = torch.cat([branch_a, branch_b, branch_c, branch_d], dim=1)\n",
    "        return output\n",
    "\n",
    "# Modell zusammenbauen und testen\n",
    "model = InceptionModule()\n",
    "sample_input = torch.randn(1, 3, 224, 224)  # Beispiel-Eingabe\n",
    "output = model(sample_input)\n",
    "print(f\"Output Shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speichere Modell in Bild\n",
    "from torchviz import make_dot\n",
    "\n",
    "sample_input = torch.randn(1, 3, 224, 224).to(\n",
    "    next(model.parameters()).device\n",
    ")  # Beispiel-Eingabe\n",
    "output = model(sample_input)\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.graph_attr.update(dpi=\"600\")\n",
    "dot.render(\"InceptionModule_architecture\", format=\"png\")\n",
    "print(\"Inception Module Architektur gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='InceptionModule_architecture.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚡ Inception-Architektur in PyTorch Lightning (L)\n",
    "- Die vollständige Inception-Architektur steht in PyTorch als ``torchvision.models.inception_v3`` zur Verfügung, inklusive der mit der ImageNet-Datenmenge vortrainierten Gewichtungen ``(weights=\"IMAGENET1K_V1\")``.\n",
    "- Ein eng verwandtes Modell ist Xception, das als erweiterte Version der Inception-Architektur gilt. In PyTorch ist Xception5 zwar nicht direkt unter ``torchvision.models`` verfügbar, jedoch kann es über externe Bibliotheken (z.B. timm) geladen werden.\n",
    "- ⚡ Xception steht für Extreme Inception und ist eine CNN-Architektur, die die Grundidee von Inception radikal weiterentwickelt. Der zentrale Gedanke: die Trennung von räumlichen und kanalbezogenen Merkmalen beim Erlernen von Repräsentationen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Residuale Verbindungen (ResNets) in Lightning (L)\n",
    "Residuale Verbindungen sind essenzielle Bausteine moderner, graphenartiger neuronaler Netzwerke und kommen in vielen Architekturen ab 2015 zum Einsatz – unter anderem in Xception, ResNet, EfficientNet und anderen. Die grundlegende Idee wurde erstmals von He et al. (2015) vorgestellt und gewann den ILSVRC-ImageNet-Wettbewerb.\n",
    "\n",
    "Zwei zentrale Probleme tiefer Netzwerke:\n",
    "- Vanishing Gradients – bei zunehmender Tiefe verschwindet der Gradient, was das Training erschwert.\n",
    "- Repräsentations-Engpässe – tiefe Netze können Schwierigkeiten haben, Informationen effizient weiterzugeben.\n",
    "\n",
    "Lösung mit Residualen Verbindungen (L-style):\n",
    "- Residuale Verbindungen schaffen eine direkte Abkürzung (Skip Connection) zwischen nicht-benachbarten Layers.\n",
    "- Statt nur der normalen Weiterverarbeitung wird die Eingabe direkt zur Ausgabe addiert.\n",
    "- Voraussetzung: Beide Tensoren (Input und Output) müssen die gleiche Shape haben.\n",
    "- Falls nicht: → Lineare Projektion, z. B. via nn.Conv2d(1x1) oder nn.Linear (ohne Aktivierung), um Formate anzugleichen.\n",
    "\n",
    "[He et al., Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "class ResidualBlock(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.relu(self.conv3(out))\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.residual_block = ResidualBlock()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.residual_block(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Beispiel für die Modellzusammenfassung\n",
    "model = Model()\n",
    "print(model)\n",
    "\n",
    "# speichere das modell in Bild\n",
    "from torchviz import make_dot\n",
    "\n",
    "sample_input = torch.randn(1, 128, 32, 32)  # Beispiel-Eingabe\n",
    "output = model(sample_input)\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.graph_attr.update(dpi=\"600\")\n",
    "dot.render(\"ResidualBlock_architecture_matching\", format=\"png\")\n",
    "print(\"Residual Block Architektur gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='ResidualBlock_architecture_matching.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv_residual = nn.Conv2d(256, 128, kernel_size=1, stride=2, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv_residual(x)\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.maxpool(out)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.residual_block = ResidualBlock()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.residual_block(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Beispiel für die Modellzusammenfassung\n",
    "model = Model()\n",
    "print(model)\n",
    "# speichere das modell in Bild\n",
    "from torchviz import make_dot\n",
    "\n",
    "sample_input = torch.randn(1, 256, 32, 32)  # Beispiel-Eingabe\n",
    "output = model(sample_input)\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.graph_attr.update(dpi=\"600\")\n",
    "dot.render(\"ResidualBlock_architecture_no_matching\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='ResidualBlock_architecture_matching.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusammenfassung:\n",
    "\n",
    "### Dimensionen:\n",
    "- Der erste Block reduziert die Kanalanzahl von 256 auf 128 und halbiert die räumliche Auflösung durch MaxPooling.\n",
    "- Der zweite Block behält Kanalanzahl und räumliche Auflösung bei (alles bleibt 128 x 32 x 32).\n",
    "\n",
    "### Residual-Pfad (Shortcut):\n",
    "- Der erste Block passt den Shortcut mit einer 1x1 Convolution (stride=2) an die kleinere Output-Größe an.\n",
    "- Der zweite Block verwendet den Shortcut als Identität, da die Dimensionen gleich bleiben.\n",
    "\n",
    "### Downsampling:\n",
    "- Der erste Block führt Downsampling im Hauptpfad und im Shortcut durch.\n",
    "- Der zweite Block hat kein Downsampling.\n",
    "\n",
    "### Komplexität:\n",
    "- Der erste Block ist kürzer (2 Convs + Pooling), aber verändert die Dimensionen.\n",
    "- Der zweite Block ist tiefer (3 Convs) und arbeitet auf gleicher Dimension.\n",
    "\n",
    "### Einsatz:\n",
    "- Der erste Block ist für Übergänge zwischen Netzwerkschichten gedacht.\n",
    "- Der zweite Block eignet sich für tiefe Verarbeitung innerhalb derselben Schicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Engpässe der Repräsentation beim Deep Learning\n",
    "\n",
    "Bei einem `Sequential`-Modell baut jeder Layer auf dem vorhergehenden Layer\n",
    "auf und hat deshalb nur Zugriff auf die in den Aktivierungen der vorhergehenden\n",
    "Layer enthalten Informationen. Sollte einer der Layer zu klein sein (beispielsweise\n",
    "weil er nur niedrigdimensionale Merkmale besitzt), ist das Modell\n",
    "auf die Menge der Informationen beschränkt, die in die Aktivierungen dieses\n",
    "Layers gepackt werden können.\n",
    "    \n",
    "Dieses Konzept lässt sich durch eine Analogie zu einer Signalverarbeitung besser\n",
    "verstehen. Stellen Sie sich eine Pipeline zur Verarbeitung von Audiodaten vor, bei\n",
    "der eine Reihe von Operationen jeweils die Ausgabe der vorhergehenden Operation\n",
    "als Eingabe erhält. Wenn nun eine der Operationen das Signal so beschneidet,\n",
    "dass nur noch niedrige Frequenzen (z.B. von 0 bis 15 kHz) verbleiben,\n",
    "können die nachfolgenden Operationen die entfernten Frequenzen nicht wiederherstellen.\n",
    "Der Informationsverlust ist dauerhaft. Residuale Verbindungen, die\n",
    "früher vorhandene Informationen nachfolgenden Operationen wieder zur Verfügung\n",
    "stellen, lösen dieses Problem von Deep-Learning-Modellen teilweise.\n",
    "    \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Vanishing Gradients beim Deep Learning\n",
    "\n",
    "Der wichtigste Algorithmus zum Trainieren von DNNs, die Backpropagation,\n",
    "funktioniert folgendermassen: Ein Feedback-Signal der Ausgabe wird an tiefer\n",
    "gelegene Layer weitergeleitet. Wenn dieses Signal einen großen Stapel von Layern\n",
    "durchläuft, kann es sehr schwach werden oder sogar ganz verloren gehen –\n",
    "das NN kann dann nicht trainiert werden. Man spricht hier vom Problem des verschwindenden\n",
    "Gradienten.\n",
    "    \n",
    "Dieses Problem tritt sowohl bei DNNs als auch bei RNNs mit sehr grossen\n",
    "Sequenzen auf. In beiden Fällen muss das Feedback-Signal eine lange Reihe von\n",
    "Operationen durchlaufen. Das vom LSTM-Layer genutzte Verfahren zur Behebung\n",
    "dieses Problems in RNNs ist Ihnen bereits bekannt: Er verwendet eine\n",
    "Carry-Spur, die Informationen parallel zur eigentlichen Verarbeitung der Daten\n",
    "weiterleitet. In Feedforward-Netzen funktionieren residuale Verbindungen auf\n",
    "ähnliche Weise. Sie sind sogar noch einfacher und nutzen eine Carry-Spur, die\n",
    "rein lineare Informationen parallel zur eigentlichen Verarbeitung der Daten weiterleitet,\n",
    "und ermöglichen es so, dass sich Gradienten durch beliebig tiefe Stapel\n",
    "von Layern ausbreiten können.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Geteilte Gewichte: Symmetrie nutzen!\n",
    "Eine wichtige Eigenschaft moderner Deep-Learning-Frameworks wie Lightning ist die Möglichkeit, eine Instanz eines Layers mehrfach zu verwenden, ohne dass neue Gewichte erzeugt werden. Das bedeutet: Ein Layer wird nur einmal instanziiert und bei wiederholtem Aufruf mit denselben Gewichten verwendet.\n",
    "\n",
    "Vorteile:\n",
    "- Man kann gemeinsam nutzbare Zweige eines Netzwerks bauen, die dieselben Repräsentationen erzeugen.\n",
    "- Die Gewichte werden geteilt und gleichzeitig anhand beider Eingaben gelernt.\n",
    "- Das Modell kann z. B. Ähnlichkeiten zwischen Eingaben erkennen, ohne Redundanz bei der Berechnung.\n",
    "\n",
    "Beispielanwendung:\n",
    "Man möchte beurteilen, wie ähnlich sich zwei Sätze inhaltlich sind (z. B. für Duplikaterkennung in Texten). Dafür werden zwei Eingaben (Satz A und Satz B) in ein gemeinsames LSTM gegeben, die Ausgaben kombiniert und ein Ähnlichkeitswert zwischen 0 und 1 berechnet.\n",
    "\n",
    "- Satz A und Satz B werden symmetrisch verarbeitet.\n",
    "- Ein gemeinsam genutztes LSTM verarbeitet beide Sätze.\n",
    "- Solche Modelle heißen Siamese LSTM oder Modelle mit geteilten Gewichten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class SiameseLSTM(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(SiameseLSTM, self).__init__()\n",
    "        # Ein LSTM-Layer wird einmal instanziiert und auf beide Eingaben angewendet\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, left_input, right_input):\n",
    "        # Gemeinsames LSTM für beide Eingaben\n",
    "        left_output, _ = self.lstm(left_input)\n",
    "        right_output, _ = self.lstm(right_input)\n",
    "\n",
    "        # Nur den letzten Zeitschritt extrahieren (für Klassifikation)\n",
    "        left_out_final = left_output[:, -1, :]\n",
    "        right_out_final = right_output[:, -1, :]\n",
    "\n",
    "        # Verkettung beider Repräsentationen\n",
    "        merged = torch.cat([left_out_final, right_out_final], dim=1)\n",
    "\n",
    "        # Vorhersage Ähnlichkeit (zwischen 0 und 1)\n",
    "        similarity = self.sigmoid(self.fc(merged))\n",
    "        return similarity\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        left_input, right_input, targets = batch\n",
    "        preds = self(left_input, right_input)\n",
    "        loss = nn.functional.binary_cross_entropy(preds, targets)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Modell initialisieren und Beispiel-Durchlauf\n",
    "model = SiameseLSTM()\n",
    "x_left = torch.randn(4, 10, 128)  # (Batch, SeqLen, InputDim)\n",
    "x_right = torch.randn(4, 10, 128)\n",
    "output = model(x_left, x_right)\n",
    "print(output.shape)  # Erwartet: [4, 1]\n",
    "\n",
    "# Visualisierung (ähnlich Keras plot_model)\n",
    "from torchviz import make_dot\n",
    "\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.graph_attr.update(dpi=\"600\")\n",
    "dot.render(\"SiameseLSTM_architecture\", format=\"png\")\n",
    "print(\"Architektur gespeichert als PNG.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='SiameseLSTM_architecture.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Modelle als Layer \n",
    "Ein wichtiger Punkt: Mit der Lightning-API bzw. PyTorch können Sie Modelle genauso wie Layer verwenden – tatsächlich lässt sich ein Modell als ein »grösserer Layer« betrachten.\n",
    "\n",
    "Dies gilt sowohl für ``nn.Sequential`` als auch für individuell definierte Klassen, die von nn.Module oder ``L.LightningModule`` erben. Sie können ein solches Modell mit einem Eingabetensor aufrufen und erhalten direkt einen Ausgabetensor zurück:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "# Einfaches Modell als Layer\n",
    "class SimpleModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Nutzung des Modells als Layer\n",
    "class ModelAsLayer(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SimpleModel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Check if GPU is available and use it if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_layer = ModelAsLayer().to(device)\n",
    "print(model_layer)\n",
    "# torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model_layer, (32,), device=str(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Modell, das die **Bilder einer Stereokamera als Eingabe** verwendet, ist ein einfaches\n",
    "praktisches Beispiel für die Wiederverwendung von Modellinstanzen: \n",
    "- zwei parallel ausgerichtete Kameras mit nur wenigen Zentimetern Abstand. \n",
    "- Ein solches Modell kann räumliche Tiefe wahrnehmen, was sich für viele Anwendungen als nützlich erweist. \n",
    "- Es sollte nicht notwendig sein, zwei unabhängige Modelle zum Extrahieren der visuellen Merkmale der linken und der rechten Kamera zu verwenden, bevor sie zusammengeführt werden. \n",
    "- Zur Verarbeitung auf dieser Ebene können beide Eingaben gemeinsam genutzt werden, und zwar durch Layer, die dieselben Gewichtungen verwenden und daher auch die Repräsentationen gemeinsam haben.\n",
    "\n",
    "Und so können Sie ein Modell mit gemeinsamer Faltungsbasis in Keras implementieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import lightning as L\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "# Feature Extractor (EfficientNet als Ersatz für Xception)\n",
    "class FeatureExtractor(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base_model = models.efficientnet_b0(pretrained=False)\n",
    "        self.features = nn.Sequential(\n",
    "            *list(base_model.children())[:-1]\n",
    "        )  # Remove classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)  # Output: [B, 1280, 8, 8] bei 250x250 Input\n",
    "\n",
    "\n",
    "# Stereo Camera Model\n",
    "class StereoCamModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.extractor = FeatureExtractor()\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        left_feat = self.extractor(left_img)  # [B, 1280, 8, 8]\n",
    "        right_feat = self.extractor(right_img)  # [B, 1280, 8, 8]\n",
    "        merged = torch.cat([left_feat, right_feat], dim=1)  # [B, 2560, 8, 8]\n",
    "        return merged\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = StereoCamModel()\n",
    "print(model)\n",
    "\n",
    "\n",
    "# torchsummary – benötigt die Module in nn.Module, daher die inneren Features zeigen\n",
    "# Trick: Dummy-Modul für torchsummary\n",
    "class StereoSummaryWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        return self.model(left_img, right_img)\n",
    "\n",
    "\n",
    "summary_model = StereoSummaryWrapper(model)\n",
    "\n",
    "# Zeige Summary\n",
    "summary(summary_model, [(3, 250, 250), (3, 250, 250)], device=\"cpu\")\n",
    "\n",
    "# Torchviz: Computational Graph anzeigen\n",
    "left = torch.randn(1, 3, 250, 250)\n",
    "right = torch.randn(1, 3, 250, 250)\n",
    "output = model(left, right)\n",
    "\n",
    "# Visualisiere den Graphen\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.format = \"png\"\n",
    "dot.render(\"stereo_cam_model_graph\", format=\"png\")  # Speichert stereo_cam_model_graph.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='stereo_cam_model_graph.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
