{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> © Christoph Würsch, François Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik/systemtechnik/ice-institut-fuer-computational-engineering/\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN07/7.1-Vortrainierte_CNN_pl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenvorbereitung mit einem LightningDataModule\n",
    "\n",
    "Wir setzen voraus, dass der Datensatz in einem Ordner vorliegt, der die Unterordner `train`, `validation` und `test` enthält.\n",
    "Die Trainingsbilder werden mithilfe von Data Augmentation (Rotation, zufälliger Crop, horizontales Spiegeln) vorbereitet, während für Validierung und Test nur eine Reskalierung und Normalisierung erfolgt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 7. Verwendung eines vortrainierten CNNs\n",
    "\n",
    "This notebook contains the code sample found in Chapter 5, Section 3 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). \n",
    "\n",
    "- Vortrainierte NNs sind ein gängiger und äusserst effektiver Ansatz, Deep Learning mit kleinen Bilddatenmengen zu betreiben.\n",
    "- Ein vortrainiertes NN ist ein gespeichertes CNN, das vorher mit einer grossen Datenmenge trainiert wurde, typischerweise für eine umfangreiche Bildklassifizierungsaufgabe. Wenn diese ursprüngliche Datenmenge gross und allgemein genug ist, kann die durch das vortrainierte NN erlernte räumliche Merkmalshierarchie als allgemeines Modell der visuellen Welt dienen.\n",
    "- Deshalb können sich die Merkmale für viele verschiedene Aufgaben des maschinellen Sehens als nützlich erweisen, obwohl für diese Aufgaben völlig andere Klassen von Bedeutung sind als für die ursprüngliche Aufgabe. \n",
    "- Sie könnten beispielsweise ein NN mit den ImageNet-Daten trainieren (deren Klassen grösstenteils Tiere und Alltagsgegenstände sind) und dieses vortrainierte NN auch für etwas ganz anderes wie z.B. die Erkennung von Möbelstücken wiederverwenden. \n",
    "- Diese **Übertragbarkeit der erlernten Merkmale auf andere Aufgaben** ist ein entscheidender Vorteil des Deep Learnings gegenüber vielen älteren Shallow-Learning-Ansätzen und sorgt dafür, dass Deep Learning sehr gut für Aufgaben mit kleinen Datenmengen geeignet ist.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7.1 VGG-Architektur\n",
    "\n",
    "Hier betrachten wir ein grosses, mit der ImageNet-Datensammlung (1.4 Millionen gekennzeichnete Bilder und 1'000 verschiedene Klassen) trainiertes CNN. Die Datenmenge enthält viele Tierklassen inklusive unterschiedlicher Hunde- und Katzenrassen, daher ist zu erwarten, dass sich das CNN gut für die Klassifizierung von Hunde- und Katzenbildern eignet. \n",
    "                                                                \n",
    "Wir werden die 2014 von *Karen Simonyan* und *Andrew Zisserman* entwickelte [VGG16-Architektur](https://arxiv.org/abs/1409.1556) verwenden. Dabei handelt es sich um eine einfache und weithin gebräuchliche CNN-Architektur für die ImageNet-Datensammlung [1]. Das Modell ist zwar schon älter, weit vom heutigen Stand der Technik entfernt und\n",
    "zudem etwas schwerfälliger als viele jüngere Modelle, dennoch habe ich es ausgewählt, weil die Architektur dem sehr ähnlich ist, was Sie bereits kennen. Sie ist gut verständlich, ohne dass es erforderlich wäre, neue Konzepte einzuführen. \n",
    "\n",
    "<img src=\"Bilder/VGG16.png\" width=\"840\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Vielleicht hören Sie zum ersten Mal von einem dieser Modelle mit den eigentümlichen\n",
    "Namen *VGG, ResNet, Inception, Inception-ResNet, Xception* [2] und wie sie alle heissen. Sie werden sich daran gewöhnen, denn sie werden Ihnen häufig begegnen, wenn Sie sich mit Deep Learning und maschinellem Sehen befassen.\n",
    "\n",
    "[1] Karen Simonyan und Andrew Zisserman, *Very Deep Convolutional Networks for Large-Scale Image Recognition*, arXiv (2014), https://arxiv.org/abs/1409.1556.\n",
    "\n",
    "[2] CNN Architectures: VGG, Resnet, InceptionNet, XceptionNet UseCases : Image Feature Extraction + Transfer Learning https://www.kaggle.com/shivamb/cnn-architectures-vgg-resnet-inception-tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Merkmalsextraktion und Feinabstimmung\n",
    "\n",
    "Es gibt zwei Möglichkeiten, ein vortrainiertes NN zu verwenden: \n",
    "- **Merkmalsextraktion (feature extraction)** und **Feinabstimmung (fine tuning)**. \n",
    "- Wir werden beide betrachten und werfen zunächst einmal einen Blick auf die Merkmalsextraktion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.2 Feature extraction (Merkmalsextraktion)\n",
    "\n",
    "Bei der Merkmalsextraktion werden die von einem vorangegangenen NN erlernten Repräsentationen dazu verwendet, neuen Samples interessante Merkmale zu entnehmen. Diese Merkmale werden anschliessend in einen von Grund auf neu trainierten Klassifizierer eingespeist.\n",
    "\n",
    "Wie Sie bereits wissen, besitzen CNNs zur Bildklassifizierung zwei Bestandteile: \n",
    "- Am Anfang steht eine Reihe von Pooling- und Convolutional Layern, und sie enden mit einem vollständig verbundenen Klassifizierer. Den ersten Teil könnte man als die Faltungsbasis (engl. Convolutional Base) des Modells bezeichnen. \n",
    "- Bei CNNs werden die neuen Daten der Faltungsbasis eines bereits trainierten NNs übergeben. Anschliessend wird ein neuer Klassifizierer mit deren Ausgabe trainiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/swapping_fc_classifier.png\" width=\"640\" height=\"440\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aber warum nur die Faltungsbasis wiederverwenden? \n",
    "\n",
    "Könnte man nicht auch den vollständig verbundenen Klassifizierer wiederverwenden? \n",
    "\n",
    "Das sollte im Allgemeinen vermieden werden, weil die von der Faltungsbasis erlernten Repräsentationen\n",
    "wahrscheinlich allgemeiner und daher besser wiederverwendbar sind: \n",
    "\n",
    "- **Die Feature-Maps eines CNNs beschreiben die Vorkommen allgemeiner Konzepte in einem Bild und sind wahrscheinlich unabhängig von der vorliegenden Aufgabe des maschinellen Sehens nützlich.**\n",
    "- Die vom Klassifizierer erlernten Repräsentationen hingegen beziehen sich notwendigerweise auf die Klassen, mit denen das Modell trainiert wurde. Diese Repräsentationen enthalten lediglich Informationen über die Wahrscheinlichkeit des Vorkommens dieser oder jener Klasse im gesamten Bild. \n",
    "- Darüber hinaus enthalten die in den Fully-connected Layern vorhandenen Repräsentationen keine Informationen darüber, wo sich Objekte in den Eingabebildern befinden: Die Layer bewahren keine räumlichen Informationen, die Feature-Maps hingegen enthalten nach wie vor die Positionen der Objekte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wenn die Positionen der Objekte für die Lösung einer Aufgabe eine Rolle spielen, sind die vollständig verbundenen Merkmale weitgehend unbrauchbar. Beachten Sie, dass die Verallgemeinerungsfähigkeit (und damit die Wiederverwendbarkeit) der Repräsentationen von der Position eines Layers im Modell abhängt. \n",
    "- Die früher auftretenden Layer erzeugen lokale, sehr allgemeine Feature-Maps (die z.B. Ränder, Farben und Texturen enthalten), während die später auftretenden Layer abstraktere Konzepte extrahieren (wie etwa »Katzenohr« oder »Hundeauge«). \n",
    "- Sollte sich Ihre neue Datenmenge also sehr von der Datenmenge unterscheiden, mit dem das ursprüngliche Modell trainiert wurde, ist es womöglich besser, wenn nur die ersten paar Layer für die Merkmalsextraktion verantwortlich sind, anstatt die gesamte Faltungsbasis zu nutzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir verwenden also die Faltungsbasis des mit der ImageNet-Datensammlung trainierten VGG16-Modells, um interessante Merkmale der Hunde- und Katzenbilder zu extrahieren, und trainieren damit anschliessend einen Hunde/Katzen-Klassifizierer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einleitung\n",
    "\n",
    "In diesem Unterrichtsmaterial zeigen wir, wie Du ein vortrainiertes Convolutional Neural Network (CNN) – konkret die VGG16-Architektur – mit PyTorch Lightning verwenden kannst, um Aufgaben der Bildklassifikation (hier: Hunde vs. Katzen) zu lösen.  \n",
    "Dabei demonstrieren wir zwei Ansätze:\n",
    "\n",
    "1. **Schnelle Merkmalsextraktion:**  \n",
    "   Die Konvolutionsbasis wird einmalig genutzt, um Features für alle Bilder zu berechnen. Diese vorverarbeiteten Features werden dann in einem einfachen, vollständig verbundenen Klassifizierer weiterverwendet.  \n",
    "   *Vorteil:* Sehr schnelle Trainingszeiten, da die rechenintensive Vorverarbeitung nur einmal erfolgt.  \n",
    "   *Nachteil:* Datenaugmentation ist in diesem Ansatz nicht möglich.\n",
    "\n",
    "2. **Merkmalsextraktion mit Datenaugmentation und Feinabstimmung:**  \n",
    "   Hier erweitern wir die vortrainierte Konvolutionsbasis um einen zusätzlichen Klassifizierer, der direkt an die Eingabedaten angeschlossen wird. So können wir Datenaugmentation im Training nutzen.  \n",
    "   Nach einer ersten Trainingsphase (mit eingefrorener Konvolutionsbasis) werden einige der oberen Layer \"aufgetaut\" und gemeinsam mit dem Klassifizierer feinabgestimmt.  \n",
    "   *Vorteil:* Datenaugmentation reduziert Überanpassung; durch Feinabstimmung kann das Modell weiter verbessert werden.\n",
    "\n",
    "Im Folgenden findest Du den vollständigen Code, der beide Ansätze mit PyTorch Lightning umsetzt.\n",
    "\n",
    "## Vorbereitung – Imports und Hilfsfunktionen\n",
    "\n",
    "Zuerst importieren wir die notwendigen Module und definieren eine Hilfsfunktion zum Glätten von Kurven (nützlich für die Darstellung von Trainings- und Validierungsmetriken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "\n",
    "class MetricsLogger(Callback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accs = []\n",
    "        self.val_accs = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        self.train_losses.append(trainer.callback_metrics[\"train_loss\"].item())\n",
    "        self.train_accs.append(trainer.callback_metrics[\"train_acc\"].item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        self.val_losses.append(trainer.callback_metrics[\"val_loss\"].item())\n",
    "        self.val_accs.append(trainer.callback_metrics[\"val_acc\"].item())\n",
    "\n",
    "\n",
    "def plot_metrics(logger):\n",
    "    epochs = range(1, min(len(logger.train_losses), len(logger.val_losses)) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, logger.train_losses[: len(epochs)], \"bo\", label=\"Training loss\")\n",
    "    plt.plot(epochs, logger.val_losses[: len(epochs)], \"b\", label=\"Validation loss\")\n",
    "    plt.title(\"Training and validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, logger.train_accs[: len(epochs)], \"bo\", label=\"Training accuracy\")\n",
    "    plt.plot(epochs, logger.val_accs[: len(epochs)], \"b\", label=\"Validation accuracy\")\n",
    "    plt.title(\"Training and validation accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightningDataModule für den Cats vs. Dogs-Datensatz\n",
    "\n",
    "Wir gehen davon aus, dass Dein Datensatz in einem Ordner vorliegt, der die Unterordner `train`, `validation` und `test` enthält.\n",
    "\n",
    "- Für den Trainingsdatensatz definieren wir zusätzlich zur Reskalierung auch Datenaugmentation (Rotation, zufällige Verschiebung, horizontales Spiegeln etc.).\n",
    "- Für Validierung und Test erfolgt nur die Reskalierung.\n",
    "\n",
    "Hinweis: Bei vortrainierten Modellen (wie VGG16) ist es oft sinnvoll, auch die ImageNet-Normalisierung zu verwenden – hier nutzen wir jedoch eine vereinfachte Variante (Skalierung auf [0,1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatsDogsDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=20, img_size=150):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Transformationen für Training (inklusive Datenaugmentation)\n",
    "        self.train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((self.img_size, self.img_size)),\n",
    "                transforms.RandomRotation(40),\n",
    "                transforms.RandomResizedCrop(self.img_size, scale=(0.8, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),  # skaliert automatisch auf [0,1]\n",
    "            ]\n",
    "        )\n",
    "        # Für Validierung und Test: Nur Resizing und ToTensor\n",
    "        self.test_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((self.img_size, self.img_size)),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        self.train_dataset = datasets.ImageFolder(\n",
    "            os.path.join(self.data_dir, \"train\"), transform=self.train_transforms\n",
    "        )\n",
    "        self.val_dataset = datasets.ImageFolder(\n",
    "            os.path.join(self.data_dir, \"validation\"), transform=self.test_transforms\n",
    "        )\n",
    "        self.test_dataset = datasets.ImageFolder(\n",
    "            os.path.join(self.data_dir, \"test\"), transform=self.test_transforms\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
    "        )\n",
    "\n",
    "\n",
    "# Beispielpfad zum Datensatz (anpassen!)\n",
    "base_dir = \"cats_dogs\"\n",
    "data_module = CatsDogsDataModule(data_dir=base_dir, batch_size=20, img_size=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil (a): Schnelle Merkmalsextraktion mittels Pre-Computing der Features\n",
    "\n",
    "Hier berechnen wir einmalig die Features aller Bilder mit der vortrainierten VGG16-Konvolutionsbasis (ohne Klassifizierer) und speichern diese in Numpy-Arrays. Anschließend trainieren wir einen einfachen vollständig verbundenen Klassifizierer auf diesen vorverarbeiteten Features.\n",
    "\n",
    "Die VGG16-Konvolutionsbasis wird aus torchvision geladen.  \n",
    "Die Ausgabe der Konvolutionsbasis hat bei Eingaben von 150×150 in der Regel die Form (Batch, 512, 4, 4), was zu einem Featurevektor der Länge 8192 führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader, sample_count):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, labs = batch\n",
    "            inputs = inputs.to(next(model.parameters()).device)\n",
    "            outputs = model(inputs)\n",
    "            features.append(outputs.cpu())\n",
    "            labels.append(labs)\n",
    "            if len(torch.cat(labels)) >= sample_count:\n",
    "                break\n",
    "    features = torch.cat(features, dim=0)[:sample_count]\n",
    "    labels = torch.cat(labels, dim=0)[:sample_count]\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird ein vortrainiertes, definiertes VGG16-Modell geladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des vortrainierten VGG16-Modells und Extraktion der Konvolutionsbasis\n",
    "vgg16_model = torchvision.models.vgg16(pretrained=True)\n",
    "conv_base = vgg16_model.features  # nur die Faltungsbasis\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "conv_base.to(device)\n",
    "conv_base.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen eines DataLoaders ohne Datenaugmentation (nur Resizing und ToTensor)\n",
    "test_dataset_for_features = datasets.ImageFolder(\n",
    "    os.path.join(base_dir, \"train\"),\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((150, 150)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "train_loader_for_features = DataLoader(\n",
    "    test_dataset_for_features, batch_size=20, shuffle=False, num_workers=4\n",
    ")\n",
    "\n",
    "# Festlegen der Sample-Anzahlen (anpassen, falls erforderlich)\n",
    "train_sample_count = 2000\n",
    "val_sample_count = 1000\n",
    "test_sample_count = 1000\n",
    "\n",
    "train_features, train_labels = extract_features(\n",
    "    conv_base, train_loader_for_features, train_sample_count\n",
    ")\n",
    "\n",
    "# Für Validierung und Test ähnlich:\n",
    "val_dataset_for_features = datasets.ImageFolder(\n",
    "    os.path.join(base_dir, \"validation\"),\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((150, 150)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "val_loader_for_features = DataLoader(\n",
    "    val_dataset_for_features, batch_size=20, shuffle=False, num_workers=4\n",
    ")\n",
    "val_features, val_labels = extract_features(\n",
    "    conv_base, val_loader_for_features, val_sample_count\n",
    ")\n",
    "\n",
    "test_dataset_for_features = datasets.ImageFolder(\n",
    "    os.path.join(base_dir, \"test\"),\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((150, 150)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "test_loader_for_features = DataLoader(\n",
    "    test_dataset_for_features, batch_size=20, shuffle=False, num_workers=4\n",
    ")\n",
    "test_features, test_labels = extract_features(\n",
    "    conv_base, test_loader_for_features, test_sample_count\n",
    ")\n",
    "\n",
    "# Reshape: von (samples, 512, 4, 4) zu (samples, 8192)\n",
    "train_features = train_features.view(train_features.size(0), -1)\n",
    "val_features = val_features.view(val_features.size(0), -1)\n",
    "test_features = test_features.view(test_features.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningModule für den Klassifizierer (Feature Extraction)\n",
    "\n",
    "Der Klassifizierer besteht aus:\n",
    "- Einer Dense-Schicht mit 256 Neuronen und ReLU-Aktivierung\n",
    "- Dropout (50%) zur Regularisierung\n",
    "- Einer finalen Dense-Schicht mit 1 Neuron (binäre Klassifikation)\n",
    "\n",
    "Als Verlustfunktion verwenden wir Binary Cross Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractionClassifier(L.LightningModule):\n",
    "    def __init__(self, input_dim=8192):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(), nn.Dropout(0.5), nn.Linear(256, 1)\n",
    "        )\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.float().unsqueeze(1)\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = (preds.float() == y).float().mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.float().unsqueeze(1)\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = (preds.float() == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.float().unsqueeze(1)\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = (preds.float() == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.RMSprop(self.parameters(), lr=2e-5)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training des Klassifizierers auf den vorverarbeiteten Features\n",
    "\n",
    "Wir erzeugen TensorDatasets für Training, Validierung und Test und trainieren den Klassifizierer über 30 Epochen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_feat = TensorDataset(train_features, train_labels)\n",
    "val_dataset_feat = TensorDataset(val_features, val_labels)\n",
    "test_dataset_feat = TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader_feat = DataLoader(train_dataset_feat, batch_size=20, shuffle=True)\n",
    "val_loader_feat = DataLoader(val_dataset_feat, batch_size=20, shuffle=False)\n",
    "test_loader_feat = DataLoader(test_dataset_feat, batch_size=20, shuffle=False)\n",
    "\n",
    "clf = FeatureExtractionClassifier(input_dim=8192)\n",
    "logger = MetricsLogger()\n",
    "trainer = L.Trainer(max_epochs=30, accelerator=\"auto\", devices=1, callbacks=[logger])\n",
    "trainer.fit(clf, train_loader_feat, val_loader_feat)\n",
    "trainer.test(clf, test_loader_feat)\n",
    "\n",
    "plot_metrics(logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil (b): Merkmalsextraktion mit Datenaugmentation und Feinabstimmung\n",
    "\n",
    "Hier erweitern wir das Modell um einen zusätzlichen Klassifizierer, der direkt an die (eingefrorene) Konvolutionsbasis von VGG16 angeschlossen wird.  \n",
    "Zuerst trainieren wir das Modell mit eingefrorener Konvolutionsbasis. Anschließend \"tauten\" wir die oberen Layer auf und führen eine Feinabstimmung durch.\n",
    "\n",
    "### Modellaufbau\n",
    "\n",
    "Das Modell besteht aus:\n",
    "- Der VGG16-Konvolutionsbasis (vorgegeben durch `vgg16(pretrained=True).features`)\n",
    "- Einer Flatten-Schicht\n",
    "- Einer Dense-Schicht mit 256 Neuronen, ReLU-Aktivierung und Dropout (50%)\n",
    "- Einer finalen Dense-Schicht mit 1 Neuron (binäre Klassifikation)\n",
    "\n",
    "Wichtig: Zuerst wird die Konvolutionsbasis eingefroren, damit der neu initialisierte Klassifizierer stabil trainiert. Später werden einige Layer der Konvolutionsbasis wieder aktiviert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningModule(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Vortrainierte VGG16-Konvolutionsbasis laden\n",
    "        vgg16_model = torchvision.models.vgg16(pretrained=True)\n",
    "        self.conv_base = vgg16_model.features\n",
    "        # Alle Parameter der Konvolutionsbasis einfrieren\n",
    "        for param in self.conv_base.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Klassifizierer definieren\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_base(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.float().unsqueeze(1)\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = (preds.float() == y).float().mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.float().unsqueeze(1)\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = (preds.float() == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.RMSprop(self.parameters(), lr=2e-5)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsphase 1: Klassifizierer trainieren mit eingefrorener Konvolutionsbasis\n",
    "\n",
    "Wir nutzen hier das DataModule mit Datenaugmentation für den Trainingsdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup()\n",
    "ft_module = FineTuningModule()\n",
    "logger_ft = MetricsLogger()\n",
    "trainer_ft = L.Trainer(\n",
    "    max_epochs=10, accelerator=\"auto\", devices=1, callbacks=[logger_ft]\n",
    ")\n",
    "trainer_ft.fit(ft_module, data_module.train_dataloader(), data_module.val_dataloader())\n",
    "\n",
    "plot_metrics(logger_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation Beispiele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_augmented_images(dataset, num_images=6):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        image, label = dataset[i]\n",
    "        axes[i].imshow(image.permute(1, 2, 0))\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_augmented_images(data_module.train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsphase 2: Feinabstimmung – Auftauen der obersten Schichten\n",
    "\n",
    "Jetzt taue die oberen Convolutional Layer der Konvolutionsbasis (z.B. ab Index 24, was in VGG16 typischerweise Block 5 entspricht) auf, damit diese gemeinsam mit dem Klassifizierer trainiert werden können.  \n",
    "Wir definieren dazu ein neues LightningModule, in dem die Layer ab Index 24 wieder trainierbar gemacht werden und die Lernrate auf 1e-5 reduziert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningModule_Unfrozen(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg16_model = torchvision.models.vgg16(pretrained=True)\n",
    "        self.conv_base = vgg16_model.features\n",
    "        # Zunächst alle Parameter einfrieren\n",
    "        for param in self.conv_base.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Ab Index 24 (Block 5) wieder freigeben\n",
    "        for layer in self.conv_base[24:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_base(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.float().unsqueeze(1)\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = (preds.float() == y).float().mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.float().unsqueeze(1)\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = (preds.float() == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.float().unsqueeze(1)\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = (preds.float() == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.RMSprop(self.parameters(), lr=1e-5)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainiere nun das Modell mit den aufgetauten Schichten (Feinabstimmung) über z.B. 100 Epochen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_module_unfrozen = FineTuningModule_Unfrozen()\n",
    "logger_ft_fine = MetricsLogger()\n",
    "trainer_ft_fine = L.Trainer(\n",
    "    max_epochs=30, accelerator=\"auto\", devices=1, callbacks=[logger_ft_fine]\n",
    ")\n",
    "trainer_ft_fine.fit(\n",
    "    ft_module_unfrozen, data_module.train_dataloader(), data_module.val_dataloader()\n",
    ")\n",
    "plot_metrics(logger_ft_fine)\n",
    "# Optional: Modell abspeichern\n",
    "torch.save(ft_module_unfrozen.state_dict(), \"cats_and_dogs_small_finetuned.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation auf den Testdaten\n",
    "\n",
    "Nach Abschluss des Trainings evaluieren wir das finale Modell auf den Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft_fine.test(ft_module_unfrozen, data_module.test_dataloader())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model, dataloader, num_images=6):\n",
    "    model.eval()\n",
    "    images, labels = [], []\n",
    "    for batch in dataloader:\n",
    "        batch_images, batch_labels = batch\n",
    "        for img, lbl in zip(batch_images, batch_labels):\n",
    "            if len(images) < num_images // 2 and lbl == 0:\n",
    "                images.append(img)\n",
    "                labels.append(lbl)\n",
    "            elif len(images) < num_images and lbl == 1:\n",
    "                images.append(img)\n",
    "                labels.append(lbl)\n",
    "            if len(images) == num_images:\n",
    "                break\n",
    "        if len(images) == num_images:\n",
    "            break\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.tensor(labels)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images.to(next(model.parameters()).device))\n",
    "        preds = torch.sigmoid(outputs).cpu() > 0.5\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(images[i].permute(1, 2, 0))\n",
    "        axes[i].set_title(f\"Pred: {preds[i].item()}, Label: {labels[i].item()}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_predictions(ft_module_unfrozen, data_module.test_dataloader())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "- **CNNs** sind hervorragend für Aufgaben des maschinellen Sehens geeignet.\n",
    "- Bei kleinen Datenmengen kann Überanpassung ein Problem sein – Datenaugmentation ist hier eine effektive Lösung.\n",
    "- Mithilfe der **Merkmalsextraktion** können vortrainierte Modelle (wie VGG16) effizient wiederverwendet werden, indem nur der Klassifizierer neu trainiert wird.\n",
    "- Die anschließende **Feinabstimmung** erlaubt es, die spezialisierten Merkmale des vortrainierten Modells an die neue Aufgabe anzupassen und so die Leistung weiter zu verbessern.\n",
    "\n",
    "Dieses Material zeigt, wie Du mit PyTorch Lightning einen strukturierten Deep-Learning-Workflow (von der Datenvorbereitung über die schnelle Merkmalsextraktion bis hin zur Feinabstimmung) realisieren kannst.\n",
    "\n",
    "Viel Erfolg beim Experimentieren und Lernen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
