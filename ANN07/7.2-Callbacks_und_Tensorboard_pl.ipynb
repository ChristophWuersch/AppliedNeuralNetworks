{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Applied Neural Networks | FS 2025 </b><br>\n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> ¬© Christoph W√ºrsch, Fran√ßois Chollet </a> </div>\n",
    "<a href=\"https://www.ost.ch/de/forschung-und-dienstleistungen/technik-neu/systemtechnik/ice-institut-fuer-computational-engineering\"> Eastern Switzerland University of Applied Sciences OST | ICE </a>\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChristophWuersch/AppliedNeuralNetworks/blob/main/ANN07/7.2-Callbacks_und_Tensorboard_pl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f√ºr Ausf√ºhrung auf Google Colab auskommentieren und installieren\n",
    "!pip install -q -r https://raw.githubusercontent.com/ChristophWuersch/AppliedNeuralNetworks/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Callbacks und Tensorboard\n",
    "\n",
    "This notebook contains the code samples found in Chapter 7, Section 2 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). \n",
    "\n",
    "Die Themen in diesem Kapitel:\n",
    "- **PyTorch Lightning Callbacks** verwenden\n",
    "- Das Visualisierungstool **TensorBoard**\n",
    "- Bew√§hrte Verfahren f√ºr die Entwicklung von Modellen nach dem **aktuellen Stand der Technik**\n",
    "\n",
    "In dieser Lektion lernen wir eine Reihe leistungsf√§higer Tools kennen, die es uns erleichtern, schwierige Aufgaben Modelle nach dem aktuellen Stand der Technik zu entwickeln. \n",
    "- PyTorch Lightning Callbacks und das browserbasierte Visualisierungstool TensorBoard erm√∂glichen es, Modelle w√§hrend des Trainings zu √ºberwachen.\n",
    "- Dar√ºber hinaus kommen verschiedene andere bew√§hrte Verfahren zur Sprache, wie die Normierung von Stapeln (*batch normalization*), residuale Verbindungen (*residual connections*), Hyperparameteroptimierung (*hyperparameter tuning*) und Ensemblemodelle (*ensemble models*).\n",
    "\n",
    "### 7.1 Deep-Learning-Modelle mit PyTorch Lightning und TensorBoard untersuchen und √ºberwachen\n",
    "\n",
    "In diesem Abschnitt werden wir die M√∂glichkeiten untersuchen, mehr Kontrolle\n",
    "dar√ºber zu erlangen, was beim Training eines Modells geschieht. \n",
    "- Das Training eines Modells mit einer grossen Datenmenge √ºber Dutzende Epochen hinweg durch den Aufruf von `trainer.fit()` zu starten, hat eine gewisse √Ñhnlichkeit mit dem Werfen eines Papierflugzeugs: Nachdem Sie es losgelassen haben, k√∂nnen Sie keinen Einfluss mehr auf die Flugbahn oder den Ort der Landung nehmen. \n",
    "- Wenn Sie verhindern m√∂chten, dass die Fl√ºge ein schlimmes Ende nehmen (und die Papierflugzeuge zerst√∂rt werden), w√§re es besser, statt eines Papierflugzeugs eine Drohne zu verwenden, die ihre Umgebung wahrnimmt, Daten an den Drohnenpiloten √ºbermittelt und automatisch auf den aktuellen Zustand reagieren kann. \n",
    "- Der Aufruf von `trainer.fit()` macht mithilfe der hier vorgestellten Verfahren aus einem Papierflugzeug eine intelligente autonome Drohne, die sich selbst √ºberwacht und dynamisch auf Ereignisse reagieren kann.\n",
    "\n",
    "### Beeinflussung eines Modells w√§hrend des Trainings durch Callbacks\n",
    "\n",
    "Beim Trainieren eines Modells gibt es viele Dinge, die sich nicht vorhersagen lassen.\n",
    "Sie wissen insbesondere nicht, wie viele Epochen erforderlich sind, um bei\n",
    "der Validierung den optimalen Wert der Verlustfunktion zu erreichen. In den bisherigen\n",
    "Beispielen haben wir die Strategie verfolgt, das Modell so lange zu trainieren,\n",
    "bis eine √úberanpassung einsetzt. Anschlie√üend wurde das Modell von Grund\n",
    "auf neu mit der so ermittelten Anzahl von Epochen trainiert. Aber dieser Ansatz\n",
    "ist nat√ºrlich aufwendig und unwirtschaftlich. \n",
    "\n",
    "\n",
    "### Callbacks\n",
    "\n",
    "Besser w√§re es, das Training abzubrechen, wenn Sie feststellen, dass sich die\n",
    "Werte der Verlustfunktion nicht mehr verbessern, und genau das l√§sst sich mit\n",
    "PyTorch Lightning's `Callbacks` erreichen. \n",
    "- Ein `Callback` ist ein Objekt (eine Klasseninstanz, die bestimmte Methoden implementiert), das dem Trainer beim Aufruf von `fit()` √ºbergeben wird. \n",
    "\n",
    "- Anschlie√üend ruft der Trainer dieses Objekt zu verschiedenen Zeitpunkten w√§hrend des Trainings auf. \n",
    "\n",
    "- Das `Callback`-Objekt hat Zugriff auf alle verf√ºgbaren Daten √ºber den Zustand des Modells und dessen Leistung und kann Ma√ünahmen ergreifen: das Training unterbrechen, ein Modell speichern, andere Gewichtungen einlesen oder den Zustand des Modells auf andere Weise √§ndern.\n",
    "\n",
    "### Anwendungen von Callbacks:\n",
    "\n",
    "- Fixpunkterstellung (Model Checkpointing) ‚Äì Die aktuellen Gewichtungen des Modells werden zu bestimmten Zeitpunkten w√§hrend des Trainings automatisch gespeichert, z.‚ÄØB. das beste Modell basierend auf der Validierungsmetrik.\n",
    "‚Üí ``lightning.pytorch.callbacks.ModelCheckpoint``\n",
    "\n",
    "- early stopping (fr√ºher Abbruch) ‚Äì Wenn sich der Validierungsverlust oder eine andere Metrik √ºber mehrere Epochen nicht mehr verbessert, wird das Training abgebrochen. Das beste gefundene Modell bleibt erhalten.\n",
    "‚Üí ``lightning.pytorch.callbacks.EarlyStopping``\n",
    "\n",
    "- Dynamische Anpassung bestimmter Hyperparameter w√§hrend des Trainings ‚Äì typischerweise die Lernrate des Optimierers.\n",
    "‚Üí z.‚ÄØB. ``torch.optim.lr_scheduler.LambdaLR`` oder ``ReduceLROnPlateau`` √ºber ``configure_optimizers``\n",
    "\n",
    "- Protokollierung von Leistungskennzahlen des Trainings und der Validierung oder auch Visualisierung von Metriken und erlernten Repr√§sentationen w√§hrend des Trainings.\n",
    "‚Üí ``lightning.pytorch.loggers.CSVLogger``, TensorBoardLogger, etc.\n",
    "\n",
    "Der bekannte Fortschrittsbalken beim Training kommt auch in Lightning ‚Äì standardm√§ssig ‚Äì √ºber einen eingebauten Callback (ProgressBar).\n",
    "| Lightning Entsprechung                                     |\n",
    "|------------------------------------------------------------|\n",
    "| `lightning.pytorch.callbacks.ModelCheckpoint`              |\n",
    "| `lightning.pytorch.callbacks.EarlyStopping`                |\n",
    "| `torch.optim.lr_scheduler.LambdaLR` + `configure_optimizers` |\n",
    "| `torch.optim.lr_scheduler.ReduceLROnPlateau` + `configure_optimizers` |\n",
    "| `lightning.pytorch.loggers.CSVLogger`                      |\n",
    "\n",
    "\n",
    "Einige davon werden wir etwas n√§her betrachten, damit Sie eine Vorstellung\n",
    "davon bekommen, wie man sie verwendet, n√§mlich \n",
    "- `ModelCheckpoint` (Fixpunkterstellung),\n",
    "- `EarlyStopping` (fr√ºher Abbruch) und \n",
    "- `ReduceLROnPlateau` (Anpassung der Lernrate).\n",
    "## 7.2 Ein bekanntes Beispiel (CNN, MNIST-Datensatz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks und TensorBoard mit PyTorch Lightning\n",
    "\n",
    "In diesem Notebook lernst du, wie man mit **PyTorch Lightning** (importiert als `L`) das Training von Deep-Learning-Modellen √ºberwacht und steuert.\n",
    "\n",
    "**Zentrale Themen:**\n",
    "- Verwendung von **Callbacks** zur dynamischen Beeinflussung des Trainings\n",
    "- Visualisierung von Metriken mit **TensorBoard**\n",
    "- Best Practices: Early Stopping, Model Checkpointing, Lernratenanpassung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import lightning.pytorch as L\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation: Kontrolle beim Modelltraining\n",
    "\n",
    "Beim Training von Deep-Learning-Modellen laufen viele Prozesse automatisch ab. Ohne Kontrolle k√∂nnte das Training:\n",
    "- zu lange dauern\n",
    "- in √úberanpassung (Overfitting) enden\n",
    "- eine suboptimale Lernrate verwenden\n",
    "\n",
    "‚úî Durch den Einsatz von **Callbacks** k√∂nnen wir eingreifen:\n",
    "- **Training fr√ºhzeitig abbrechen** (Early Stopping)\n",
    "- **Bestes Modell speichern** (ModelCheckpoint)\n",
    "- **Lernrate dynamisch anpassen** (ReduceLROnPlateau)\n",
    "\n",
    "‚úî Mit **TensorBoard** sehen wir, wie sich Metriken entwickeln, und k√∂nnen fundierte Entscheidungen treffen.\n",
    "\n",
    "\n",
    "### PyTorch Lightning: Vorteile\n",
    "\n",
    "Lightning strukturiert deinen Code und entlastet dich von Boilerplate:\n",
    "- ‚ûî Trainings- und Validierungslogik √ºbersichtlich in `training_step`, `validation_step`\n",
    "- ‚ûî Einfaches Logging mit `self.log()`\n",
    "- ‚ûî Integration von Callbacks und TensorBoard ohne Mehraufwand\n",
    "\n",
    "\n",
    "### Daten laden (MNIST)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_ds, val_ds = random_split(mnist_train, [55000, 5000])\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=128)\n",
    "test_loader = DataLoader(mnist_test, batch_size=128)\n",
    "\n",
    "# Visualisierung einiger Bilder\n",
    "fig, axs = plt.subplots(5, 10, figsize=(12, 6))\n",
    "k = 0\n",
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "        axs[i, j].imshow(train_ds[k][0].squeeze(), cmap=\"gray\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "        k += 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell als LightningModule definieren\n",
    "\n",
    "Ein `LightningModule` definiert:\n",
    "- das Modell (Layers, Architektur)\n",
    "- die Loss-Funktion\n",
    "- die Schritte f√ºr Training und Validierung\n",
    "- Optimizer und Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64 * 5 * 5, 10),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        self.loss_fn = nn.NLLLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        self.log(\"train_acc\", acc, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"},\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks erkl√§rt\n",
    "\n",
    "### Was genau machen Callbacks im Detail?\n",
    "Was genau machen Callbacks im Detail?\n",
    "- **ModelCheckpoint**\n",
    "    - Speichert automatisch das Modell, wenn sich eine bestimmte Metrik verbessert (z.‚ÄØB. ``val_loss`` oder ``val_acc``).\n",
    "    - Vorteil: Du verlierst nie das beste Modell ‚Äì selbst wenn das Training sp√§ter schlechter wird.\n",
    "    - ‚ûî Beispiel: Bestes Ergebnis war bei Epoche 17 ‚Üí dieses Modell wird gespeichert.\n",
    "    - Pfad zur Datei: Standard oder via ``dirpath`` definierbar.\n",
    "\n",
    "- **EarlyStopping**\n",
    "    - Stoppt das Training automatisch, wenn sich eine Metrik √ºber mehrere Epochen nicht mehr verbessert.\n",
    "    - Verhindert Overfitting und spart Zeit.\n",
    "    - ‚ûî Beispiel: ``patience=3`` ‚ûî nach 3 Epochen ohne Verbesserung wird gestoppt.\n",
    "    - Das beste Modell bleibt gespeichert (in Kombi mit ModelCheckpoint).\n",
    "\n",
    "- **TensorBoard ‚Äì Visualisierung leicht gemacht**\n",
    "    - Ein browserbasiertes Tool zur √úberwachung des Trainingsverlaufs.\n",
    "    - Zeigt:\n",
    "        - üìä Verlust (``Loss``) und Genauigkeit (``Accuracy``) √ºber die Zeit\n",
    "        - üîç Modellgraph (Architektur)\n",
    "        - üìà Lernratenverlauf\n",
    "        - üìâ Gewichtshistogramme\n",
    "        - üìù Eigene Logs (Text, Bilder etc.)\n",
    "\n",
    "[noch mehr Lightning Callbacks hier!](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = L.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\", save_top_k=1, mode=\"min\", filename=\"mnist-best\"\n",
    ")\n",
    "\n",
    "earlystop_cb = L.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "\n",
    "logger = L.loggers.TensorBoardLogger(\"tb_logs\", name=\"mnist_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training starten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTModel()\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb],\n",
    "    logger=logger,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard starten in Kommandozeile\n",
    "\n",
    "```bash\n",
    "%tensorboard --logdir=Lektionen/ANN07/pytorchlightning/tb_logs\n",
    "```\n",
    "\n",
    "‚ûî Im Browser: `http://localhost:6006`\n",
    "\n",
    "Du siehst: Trainingskurven, Validierungsmetriken, Histogramme, Graphen. (eventuell Pfad anpassen!)\n",
    "\n",
    "### Benutzerdefinierter Callback: Aktivierungen speichern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLoggerCallback(L.Callback):\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        sample = next(iter(val_loader))[0][0:1].to(pl_module.device)\n",
    "        with torch.no_grad():\n",
    "            activations = pl_module.model(sample)\n",
    "        np.save(\n",
    "            f\"activations_epoch_{trainer.current_epoch}.npy\", activations.cpu().numpy()\n",
    "        )\n",
    "\n",
    "\n",
    "# Training mit eigenem Callback\n",
    "trainer_with_custom_cb = L.Trainer(\n",
    "    max_epochs=5, callbacks=[ActivationLoggerCallback()], logger=logger\n",
    ")\n",
    "trainer_with_custom_cb.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Fazit\n",
    "\n",
    "Mit **Callbacks** und **TensorBoard** kannst du das Modelltraining nicht nur besser verstehen, sondern auch effizienter und intelligenter steuern:\n",
    "- **Zeit sparen** durch fr√ºhzeitiges Stoppen\n",
    "- **Beste Modelle sichern**\n",
    "- **Lernrate adaptiv anpassen**\n",
    "- **Metriken visuell analysieren** f√ºr bessere Entscheidungen\n",
    "\n",
    "PyTorch Lightning macht das einfach ‚Äì und deine Experimente werden reproduzierbar und sauber dokumentiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
